{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# 이번장은 드래곤볼을 모으는 과정이라 생각하면 편하다.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#오차제곱 SSE\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "#2일 확률이 높다고 추정\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))\n",
    "\n",
    "#7일 확률이 높다고 추정\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))\n",
    "#윗 값이 손실함수 값이 낮은걸 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "#교차엔트로피 오차 CEE\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] #다시 세팅해주고\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "#정답이 0.6인데, 교차엔트로피의 값이 0.51이 나왔다. \n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "#이 사례에서는 정답(2)의 값이 0.1일 경우인데,\n",
    "#이 경우에서 오차엔트로피의 값이 무려 2.3이 나왔다.\n",
    "#즉, 오차값이 더 낮은 0.51의 케이스가 정답일 가능성이 높다고 판단하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train),(x_test, t_test) = load_mnist(normalize=True, one_hot_label= True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) #train_size (6만개)에서 배치사이즈 10개 를 무작위로 뽑아냄\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치용 교차 엔트로피 오차 구현하기\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size) #t : 정답 레이블\n",
    "        y = y.reshape(1, y.size) #y : 신경망의 출력\n",
    "        #reshape : 데이터 하나당 교체 엔트로피 오차를 구함\n",
    "        \n",
    "        batch_size = y.shape[0] \n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "#원핫에서는 t가 0이면 그 계산은 무시해도 좋다.\n",
    "\n",
    "#정답이 원핫인코딩이 아니라,숫자 레이블로 주어졌을 경우\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0] \n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "#원핫에서 np.log(y)였던 부분을 원핫이 아닌경우는 위처럼 상세하게 표현해줘야한다.\n",
    "\n",
    "#np.log(y[np.arange(batch_size), t])\n",
    "#np.arange(batch_size) : 0부터 batchsize -1 까지 배열생성\n",
    "#batch_size가 5면 0~4라는 넘파이 배열을 생성\n",
    "#고로 숫자 레이블이 담긴 t에 해당하는 신경망의 출력을 추출하게 된다.\n",
    "#y[0,2], y[1,7] ... 같은 넘파이 배열이 생성된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#해석미분\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50 \n",
    "    return (f(x+h) - f(x)) / h\n",
    "#정말 작은 크기는 해결하지 못한다\n",
    "\n",
    "#수치미분\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_1(x): #그냥 임의의 함수 구현\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1) # [0,20) 0.1간격의 배열 x를 만들자\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#편미분\n",
    "def function_2(x): #인수들의 제곱합을 계산하는 단순 식 \n",
    "    return x[0]**2 + x[1]**2 # 또는 return np,sum(x**2)\n",
    "\n",
    "#문1, x0 =3, x1=4일때 x0에 대한 편미분 df/dx0 를 구하라\n",
    "def function_tmp1(x0): \n",
    "    return x0 * x0 + 4.0 ** 2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문2, x0 =3, x1=4일때 x1에 대한 편미분 df/dx1 를 구하라\n",
    "def function_tmp2(x1):\n",
    "    return 3.0 ** 2.0 + x1 * x1\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기울기\n",
    "#위와 다르게 x0와 x1의 편미분을 동시에 계산하고 싶다면?\n",
    "#모든 변수의 편미분을 벡터로 정리 : 기울기\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같고 원소가 모드 0인 배열 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]   #f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h #f(x-h) 계산\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val #값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))\n",
    "#임의의 세 점에서의 기울기를 구해봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num = 100):\n",
    "    #init_x는 초기값, lr : learning rate 학습률 값, step_num : 경사법에 따른 반복 횟수 \n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x , lr=0.1, step_num = 100)\n",
    "#0에 가까운 결과값이 나왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "#lr값에 따라 값이 차이가 나는 것을 체감해보자\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x , lr=10.0, step_num = 100)\n",
    "\n",
    "print(gradient_descent(function_2, init_x = init_x , lr=10.0, step_num = 100))\n",
    "print(gradient_descent(function_2, init_x = init_x , lr=1e-10, step_num = 100)) \n",
    "#바로 이렇게 넘어가면 첫번째 프린트문 결과가 또 뜬다 왜 그런걸까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x = init_x , lr=10.0, step_num = 100)) #학습률이 너무 커 발산해버린 사례\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x = init_x , lr=1e-10, step_num = 100)) \n",
    "#이렇게 해줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#신경망에서의 기울기\n",
    "#가중치 매개변수에 대한 손실함수의 기울기\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #정규분포로 초기화\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x ,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.28863446 -0.66108606  0.57079283]\n",
      " [ 0.77733796  0.46054273  0.73756407]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) #실행할때마다 계속 바뀐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52642349 0.01783682 1.00628336]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6,0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) #최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6886491183404473\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "print(net.loss(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18649795  0.11214959 -0.29864754]\n",
      " [ 0.27974693  0.16822438 -0.44797131]]\n"
     ]
    }
   ],
   "source": [
    "# def f(W):\n",
    "#     return net.loss(x, t)\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망 학습 절차\n",
    "# 전제 : \n",
    "# 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다.\n",
    "\n",
    "# 1단계 - 미니배치:\n",
    "# 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하고, 그 미니배치의 손실 함수 값을 줄이는 것이 목표다.\n",
    "\n",
    "# 2단계 - 기울기 산출 :\n",
    "# 미니뱇의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "# 3단계 - 매개변수 갱신 :\n",
    "# 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "# 4단계 - 반복\n",
    "# 1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미니배치를 무작위로 선정하기에, 확률적 경사 하강법 SGD라 부른다.\n",
    "#cs231n에 해당 코드가 나온다 한다.\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #               입력층의 뉴런 수 , 은닉층, 출력층, 출력층\n",
    "        #가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #1번째 층의 가중치\n",
    "        self.params['b1'] = np.zeros(hidden_size)                                      # 1전째 층의 편향\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x): #예측 수행\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self,x,t): #x는 이미지의 데이터 , t는 정답레이블\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t): #정확도를 구한다\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,x, t): #가중치의 매개변수의 기울기를 구한다\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        \n",
    "        grads = {}    #기울기를 보관하는 딕셔너리 변수\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])  # 1번째 층의 가중치의 기울기\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1']) # 1번째 층의 편향의 기울기\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size=100, output_size=10)\n",
    "net.params['W1'].shape\n",
    "print(net.params['W1'].shape)\n",
    "net.params['b1'].shape\n",
    "print(net.params['b1'].shape)\n",
    "net.params['W2'].shape\n",
    "print(net.params['W2'].shape)\n",
    "net.params['b2'].shape\n",
    "print(net.params['b2'].shape)\n",
    "#params에는 신경망에 필요한 매개변수가 모두 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측처리가 순방향으로 진행\n",
    "x = np.random.rand(100,784)\n",
    "y = net.predict(x)\n",
    "#grads에는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) #더미 입력 데이터(100장 분량)\n",
    "t = np.random.rand(100,10)   #더미 정답 레이블(100장 분량)\n",
    "\n",
    "grads = net.numerical_gradient(x,t) #기울기 계산\n",
    "\n",
    "grads['W1'].shape\n",
    "print(grads['W1'].shape)\n",
    "grads['b1'].shape\n",
    "print(grads['b1'].shape)\n",
    "grads['W2'].shape\n",
    "print(grads['W2'].shape)\n",
    "grads['b2'].shape\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미니배치 학습\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "\n",
    "#하이퍼 파라미터\n",
    "iters_num = 10000 #반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 #미니배치 크기\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "for i in range(iters_num):#미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch) #성능개선 버전\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#윗 결과 수정\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "\n",
    "iters_num = 10000 \n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 \n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "#1에폭당 반복 수 \n",
    "iters_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #1 에폭당 정확도 계산 \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc |\" + str(train_acc)+ \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
