{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"knowledge_distilation_keras_tuto.ipynb","provenance":[],"authorship_tag":"ABX9TyOwmHgEkANKwk3cSVKfeaZb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"7XP2dHyNVyHs","executionInfo":{"status":"ok","timestamp":1627127982528,"user_tz":-540,"elapsed":2278,"user":{"displayName":"이서영","photoUrl":"","userId":"08706917529998634867"}}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5LcrmyPYYDj"},"source":["https://keras.io/examples/vision/knowledge_distillation/"]},{"cell_type":"code","metadata":{"id":"FJax9fTtYqMB"},"source":["class Distiller(keras.Model):\n","  def __init__(self, student, teacher):\n","    super(Distiller, self).__init__()\n","    self.teacher = teacher\n","    self.student = student\n","\n","  def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha = 0.1, temperature=3,):\n","    super(Distiller, self).compile(optimizer = optimizer, metrics = metrics)\n","    self.student_loss_fn = student_loss_fn\n","    self.distillation_loss_fn = distillation_loss_fn\n","    self.alpha = alpha\n","    self.temperature = temperature\n","\n","  def train_step(self, data):\n","    x, y = data\n","\n","    #Forward pass of teacher\n","    teacher_predictions = self.teacher(x, training = False)\n","\n","    with tf.GradientTape() as tape:\n","      #Forward pass of Students\n","      students_predictions = self.student(x, training = True)\n","\n","      #compute losses\n","      student_loss = self.student_loss_fn(y, students_predictions)\n","      distillation_loss = self.distillation_loss_fn(tf.nn.softmax(teacher_predictions / self.temperature, axis=1), tf.nn.softmax(students_predictions / self.temperature, axis=1),)\n","      \n","      loss = self.alpha * student_loss + ( 1 - self.alpha) * distillation_loss\n","\n","    #Compute gradients\n","    trainable_vars = self.student.trainable_variables # ???\n","    gradients = tape.gradient(loss, trainable_vars) #???\n","\n","    #update weights\n","    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","    #update the metrics configured in 'compile()'\n","    self.compiled_metrics.update_state(y, students_predictions)\n","\n","    #return a dict of performance\n","    results = {m.name: m.result() for m in self.metrics}\n","    results.update({\"student_loss\":student_loss, \"distillation_loss\":distillation_loss})\n","    return results\n","\n","  def test_step(self, data):\n","    x , y = data\n","\n","    y_prediction = self.student(x, training=False)\n","    student_loss = self.student_loss_fn(y, y_prediction)\n","    "],"execution_count":null,"outputs":[]}]}