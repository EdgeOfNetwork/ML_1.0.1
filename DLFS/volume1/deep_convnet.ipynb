{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_convnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4cV4Gs4cMVRLwQGbuHua7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdgeOfNetwork/ML_1.0.1/blob/master/deep_convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIYfQ2xcN9m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.input_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9U1AnjTOCMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzscp03COCE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from collections import OrderedDict\n",
        "\n",
        "class DeepConvNet:\n",
        "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
        "\n",
        "    네트워크 구성은 아래와 같음\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        affine - relu - dropout - affine - dropout - softmax\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
        "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 hidden_size=50, output_size=10):\n",
        "        # 가중치 초기화===========\n",
        "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
        "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
        "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
        "        \n",
        "        self.params = {}\n",
        "        pre_channel_num = input_dim[0]\n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
        "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
        "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
        "            pre_channel_num = conv_param['filter_num']\n",
        "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
        "        self.params['b7'] = np.zeros(hidden_size)\n",
        "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b8'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성===========\n",
        "        self.layers = []\n",
        "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
        "                           conv_param_1['stride'], conv_param_1['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
        "                           conv_param_2['stride'], conv_param_2['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
        "                           conv_param_3['stride'], conv_param_3['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
        "                           conv_param_4['stride'], conv_param_4['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
        "                           conv_param_5['stride'], conv_param_5['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
        "                           conv_param_6['stride'], conv_param_6['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        \n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x, train_flg=True)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx, train_flg=False)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        tmp_layers = self.layers.copy()\n",
        "        tmp_layers.reverse()\n",
        "        for layer in tmp_layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
        "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
        "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdQxKCkiQPAl",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "197ee977-e100-4522-e413-672ca97c716e"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ca164b3-8c28-4007-b231-b3a9b20ccbd9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ca164b3-8c28-4007-b231-b3a9b20ccbd9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving mnist.pkl to mnist (1).pkl\n",
            "Saving mnist.py to mnist (2).py\n",
            "User uploaded file \"mnist.pkl\" with length 54950333 bytes\n",
            "User uploaded file \"mnist.py\" with length 3655 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIuc0_pdQyFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "        \n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "        \n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "        \n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "        \n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "            \n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "                \n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeDIdnC_Q2P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGD:\n",
        "\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key] \n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():                                \n",
        "                self.v[key] = np.zeros_like(val)\n",
        "                \n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "    \n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
        "        \n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "            \n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "            \n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpV4WZEuRcsx",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "ff745345-03f8-4e71-b120-1cb9dea62adc"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-504455fd-8f2e-462a-a3eb-1ec1053c0143\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-504455fd-8f2e-462a-a3eb-1ec1053c0143\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deep_convnet_params.pkl to deep_convnet_params (1).pkl\n",
            "User uploaded file \"deep_convnet_params.pkl\" with length 989346 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY6TSYCRSIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db831a52-5732-4c88-8dc9-896c22f33596"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()  \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=20, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보관\n",
        "network.save_params(\"deep_convnet_params.pkl\")\n",
        "print(\"Saved Network Parameters!\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.7287780165979137\n",
            "train loss:0.8833219470488507\n",
            "train loss:0.9706411731859796\n",
            "train loss:0.8860068515058037\n",
            "train loss:0.8873321847307277\n",
            "train loss:0.97098691892847\n",
            "train loss:0.8009867475835278\n",
            "train loss:0.8838453887884741\n",
            "train loss:0.8448021576286304\n",
            "train loss:0.7301930427529792\n",
            "train loss:0.9095400062835194\n",
            "train loss:0.8841654768109549\n",
            "train loss:0.9587024331893449\n",
            "train loss:0.808716860549534\n",
            "train loss:0.8712601813351318\n",
            "train loss:0.9011942636844665\n",
            "train loss:0.949259350647577\n",
            "train loss:0.7151875283393377\n",
            "train loss:0.9098135553031088\n",
            "train loss:0.7979677290643571\n",
            "train loss:0.888360782241678\n",
            "train loss:0.9437867062329695\n",
            "train loss:0.9445381137080696\n",
            "train loss:0.7292798890913136\n",
            "train loss:0.8905379243181076\n",
            "train loss:0.7220754983370898\n",
            "train loss:0.8227165785002856\n",
            "train loss:0.8655469223284772\n",
            "train loss:0.7535970778659435\n",
            "train loss:0.8479026223903382\n",
            "train loss:0.8168435034355046\n",
            "train loss:1.0784988665686739\n",
            "train loss:1.0016964831726458\n",
            "train loss:0.9212798884583034\n",
            "train loss:0.8214588491472601\n",
            "train loss:0.8634063406905014\n",
            "train loss:0.9755376326240968\n",
            "train loss:0.9943655425788976\n",
            "train loss:0.879654438373117\n",
            "train loss:0.8517866753481251\n",
            "train loss:0.953323300981286\n",
            "train loss:0.7723826301098723\n",
            "train loss:0.9536250332051327\n",
            "train loss:0.9880220371963242\n",
            "train loss:0.8730721761000454\n",
            "train loss:0.7541580107277785\n",
            "train loss:0.7722619794092299\n",
            "train loss:0.6968458122726456\n",
            "train loss:0.9010692421641592\n",
            "train loss:0.717251750336618\n",
            "train loss:0.9053960321933716\n",
            "train loss:0.9122191880102338\n",
            "train loss:0.9326866843763031\n",
            "train loss:1.0591363841358221\n",
            "train loss:0.7599258667388024\n",
            "train loss:0.8717067920962599\n",
            "train loss:0.706553572233044\n",
            "train loss:0.8861028287320223\n",
            "train loss:1.179601933906154\n",
            "train loss:0.8548064107849096\n",
            "train loss:0.7451797301008384\n",
            "train loss:0.7885738442340616\n",
            "train loss:0.7614529694604161\n",
            "train loss:0.6966078693060891\n",
            "train loss:0.9377788202620708\n",
            "train loss:0.7806174474448739\n",
            "train loss:0.680431575327637\n",
            "train loss:0.9337333963980887\n",
            "train loss:0.8199045933604929\n",
            "train loss:0.9196013807611665\n",
            "train loss:0.8791677414519724\n",
            "train loss:0.8634916560804866\n",
            "train loss:1.0462095450204096\n",
            "train loss:0.7902762084891691\n",
            "train loss:0.8032041521211682\n",
            "train loss:0.9456129676497598\n",
            "train loss:0.9560964848928245\n",
            "train loss:0.8305032174654753\n",
            "train loss:0.8682244337543463\n",
            "train loss:0.8367649695405633\n",
            "train loss:0.9465589485828139\n",
            "train loss:0.9674226495277674\n",
            "train loss:0.8598554144577508\n",
            "train loss:0.8370380673803688\n",
            "train loss:0.8092542192737245\n",
            "train loss:0.8344906950170653\n",
            "train loss:0.9754188340738652\n",
            "train loss:0.8646153280176918\n",
            "train loss:0.6913734397682838\n",
            "train loss:0.7820897753904815\n",
            "train loss:0.9139892845725832\n",
            "train loss:0.7193730561602307\n",
            "train loss:0.9068928195927758\n",
            "train loss:0.8353958699474616\n",
            "train loss:0.9614863028954618\n",
            "train loss:0.7236068694960152\n",
            "train loss:0.7721479543237346\n",
            "train loss:0.9460794388463083\n",
            "train loss:0.9050067984313794\n",
            "train loss:0.872045060655201\n",
            "train loss:0.8164129355126089\n",
            "train loss:0.8114136319159332\n",
            "train loss:0.9016997440469212\n",
            "train loss:0.8446682537751447\n",
            "train loss:0.9226381010191398\n",
            "train loss:0.7496691028146079\n",
            "train loss:0.8714433290435233\n",
            "train loss:0.9035241699285463\n",
            "train loss:0.8980707416286796\n",
            "train loss:0.8469736756016581\n",
            "train loss:0.9485832505096796\n",
            "train loss:0.8754855087352964\n",
            "train loss:0.7514635057945211\n",
            "train loss:0.8158366127816121\n",
            "train loss:0.8857869537680187\n",
            "train loss:0.8278668333345118\n",
            "train loss:0.7992168663741392\n",
            "train loss:0.9079725801975376\n",
            "train loss:0.9308059595056499\n",
            "train loss:0.8215536686656414\n",
            "train loss:0.859947193691341\n",
            "train loss:0.9320609985842233\n",
            "train loss:0.8849203465820981\n",
            "train loss:1.0352941026895344\n",
            "train loss:0.9893724981500356\n",
            "train loss:0.8094029050540699\n",
            "train loss:0.9040512747591508\n",
            "train loss:0.7660258282265106\n",
            "train loss:0.8286523872728497\n",
            "train loss:0.9001944037548608\n",
            "train loss:0.8834734505241342\n",
            "train loss:1.0508879738350092\n",
            "train loss:0.9656852256531399\n",
            "train loss:0.8870133095060082\n",
            "train loss:0.7273857700575147\n",
            "train loss:0.7732315153206158\n",
            "train loss:0.8835081130376046\n",
            "train loss:0.8107228335747019\n",
            "train loss:0.7478637650203932\n",
            "train loss:0.8383739724551027\n",
            "train loss:0.8719131150114481\n",
            "train loss:0.985997532415232\n",
            "train loss:0.8026377784418774\n",
            "train loss:0.9010505463093966\n",
            "train loss:0.8550381650906992\n",
            "train loss:0.8840711865653023\n",
            "train loss:0.8667755506124819\n",
            "train loss:0.8994586992976277\n",
            "train loss:0.895491311335526\n",
            "train loss:0.8420847799093094\n",
            "train loss:0.7315408343380261\n",
            "train loss:0.6683419993319574\n",
            "train loss:0.9833279537278287\n",
            "train loss:0.9775774668513181\n",
            "train loss:0.8897455019635606\n",
            "train loss:0.8444877232123271\n",
            "train loss:0.8601281767586592\n",
            "train loss:0.8625468918105619\n",
            "train loss:1.0008196886809944\n",
            "train loss:0.7818876991975194\n",
            "train loss:0.9114657166251372\n",
            "train loss:0.9479390427906093\n",
            "train loss:0.7324931220283151\n",
            "train loss:0.947670373445576\n",
            "train loss:0.8717353703820446\n",
            "train loss:0.7169076735604016\n",
            "train loss:0.7469796245852229\n",
            "train loss:0.8437125964347498\n",
            "train loss:0.9854213081594416\n",
            "train loss:0.9181308724639453\n",
            "train loss:0.9672682444057004\n",
            "train loss:0.9294458669429275\n",
            "train loss:0.8178564645962855\n",
            "train loss:0.7429764797032204\n",
            "train loss:1.056178110263624\n",
            "train loss:0.8409558848148762\n",
            "train loss:0.8421074529356275\n",
            "train loss:0.9746880242604208\n",
            "train loss:0.8703480097171699\n",
            "train loss:0.7936807446537005\n",
            "train loss:0.7816496111518687\n",
            "train loss:0.7567003045115217\n",
            "train loss:0.945158758249597\n",
            "train loss:0.972994030017006\n",
            "train loss:0.843263420903807\n",
            "train loss:0.9687317760589517\n",
            "train loss:0.8314022978891664\n",
            "train loss:0.8352483569733855\n",
            "train loss:0.875849811736585\n",
            "=== epoch:13, train acc:0.995, test acc:0.987 ===\n",
            "train loss:1.0008260731381495\n",
            "train loss:0.8494704177967399\n",
            "train loss:0.9158477467894329\n",
            "train loss:0.8959849939349191\n",
            "train loss:0.9078802114706677\n",
            "train loss:0.9525501794289272\n",
            "train loss:0.7979549494386292\n",
            "train loss:0.9043490652347643\n",
            "train loss:0.9024584271311318\n",
            "train loss:0.7434979405007762\n",
            "train loss:0.7931580652430337\n",
            "train loss:0.8345178888470767\n",
            "train loss:0.8339718111673408\n",
            "train loss:0.9850567427513341\n",
            "train loss:0.9400119456035757\n",
            "train loss:0.8270417759446294\n",
            "train loss:0.8827191203660996\n",
            "train loss:0.7686392622882132\n",
            "train loss:0.9370787488926817\n",
            "train loss:0.878440849416387\n",
            "train loss:1.0084137698284565\n",
            "train loss:0.6143299144988281\n",
            "train loss:0.5725563229916986\n",
            "train loss:0.9026506206331295\n",
            "train loss:0.9032704873326933\n",
            "train loss:0.9268403679910882\n",
            "train loss:1.0249695669509906\n",
            "train loss:0.7830585092215392\n",
            "train loss:0.7546062999470741\n",
            "train loss:0.9491970081962162\n",
            "train loss:0.9062671182376364\n",
            "train loss:1.1010038523067827\n",
            "train loss:0.7886929381173359\n",
            "train loss:0.8542915963337474\n",
            "train loss:0.7256589684795243\n",
            "train loss:0.7781953629636571\n",
            "train loss:0.8233342009237821\n",
            "train loss:0.7267314739108807\n",
            "train loss:0.8031713181090308\n",
            "train loss:0.6420992355149862\n",
            "train loss:0.813375652067484\n",
            "train loss:0.97255469385058\n",
            "train loss:0.9334084956764039\n",
            "train loss:0.7578353774110894\n",
            "train loss:0.8655055883670251\n",
            "train loss:0.8548756114603924\n",
            "train loss:0.7231972235126986\n",
            "train loss:0.8985003662493787\n",
            "train loss:0.883040903982151\n",
            "train loss:0.879928346388057\n",
            "train loss:0.8149043553243245\n",
            "train loss:0.8125491338040648\n",
            "train loss:0.8944405822968697\n",
            "train loss:1.0019218867807964\n",
            "train loss:0.8935220647401486\n",
            "train loss:0.9083491754393971\n",
            "train loss:0.8243044217149301\n",
            "train loss:0.8283195568146087\n",
            "train loss:0.8480951337304588\n",
            "train loss:0.8301206293495027\n",
            "train loss:0.9593972268999016\n",
            "train loss:0.8187517858837968\n",
            "train loss:0.8910929962419425\n",
            "train loss:0.9118413838275987\n",
            "train loss:0.8847708597438271\n",
            "train loss:0.809858243684182\n",
            "train loss:0.8604321053371621\n",
            "train loss:0.8269659369504505\n",
            "train loss:0.9384138738799892\n",
            "train loss:0.8099753112763789\n",
            "train loss:0.8049825072427426\n",
            "train loss:0.8331439371485293\n",
            "train loss:0.7963136583436724\n",
            "train loss:0.9274322458134401\n",
            "train loss:0.694557560923319\n",
            "train loss:0.7428689268755078\n",
            "train loss:0.7621040929460693\n",
            "train loss:0.8783006653409086\n",
            "train loss:0.899066472262975\n",
            "train loss:0.8314669613111998\n",
            "train loss:0.9610372140171655\n",
            "train loss:0.9287067477554725\n",
            "train loss:0.7735444389624804\n",
            "train loss:0.7232511455622317\n",
            "train loss:0.7719120303856797\n",
            "train loss:0.988171882015815\n",
            "train loss:0.9867254572841115\n",
            "train loss:0.7141567550600024\n",
            "train loss:0.9755106879260628\n",
            "train loss:0.8556425495139999\n",
            "train loss:0.8016988037100821\n",
            "train loss:0.8645672984764609\n",
            "train loss:0.8403385451169779\n",
            "train loss:0.9211842648556743\n",
            "train loss:0.8490716751872904\n",
            "train loss:0.900168768226087\n",
            "train loss:0.8602089390435103\n",
            "train loss:0.8456044342270155\n",
            "train loss:0.9272644404435512\n",
            "train loss:0.9700806759509123\n",
            "train loss:0.9907284746919973\n",
            "train loss:0.9035519916894126\n",
            "train loss:0.8024647945798961\n",
            "train loss:0.8252684973536191\n",
            "train loss:0.8962926043407761\n",
            "train loss:0.87307421075315\n",
            "train loss:0.8696401061515876\n",
            "train loss:0.7881803240193278\n",
            "train loss:0.761404716094562\n",
            "train loss:0.7995921153459902\n",
            "train loss:0.696674866727974\n",
            "train loss:0.8341142029461417\n",
            "train loss:0.9511956160321752\n",
            "train loss:0.8622554658241336\n",
            "train loss:0.9441782619406763\n",
            "train loss:0.7894254637216865\n",
            "train loss:0.8558323708019658\n",
            "train loss:0.9094797172735483\n",
            "train loss:1.0318553566489352\n",
            "train loss:0.7876126058619374\n",
            "train loss:0.734135001120738\n",
            "train loss:0.8963953340833817\n",
            "train loss:0.8861254113598294\n",
            "train loss:0.790009802250055\n",
            "train loss:0.7010891086086345\n",
            "train loss:0.7523508237280161\n",
            "train loss:0.7096462694563286\n",
            "train loss:1.0586398975578328\n",
            "train loss:0.8222240260827322\n",
            "train loss:0.8491080739012807\n",
            "train loss:0.9602367613783038\n",
            "train loss:1.0968061427672937\n",
            "train loss:0.720700850188919\n",
            "train loss:0.9053667509773663\n",
            "train loss:0.8990476061456291\n",
            "train loss:0.7413127712948224\n",
            "train loss:0.6646623717360767\n",
            "train loss:0.9490057142439585\n",
            "train loss:0.8562677732921664\n",
            "train loss:0.8500299996874711\n",
            "train loss:0.628163349107323\n",
            "train loss:0.9158555129018905\n",
            "train loss:0.7672340833421744\n",
            "train loss:0.8658494516246954\n",
            "train loss:0.9066170717896539\n",
            "train loss:0.6936377651938203\n",
            "train loss:0.7502727451756095\n",
            "train loss:1.0042979059460684\n",
            "train loss:0.6876881754788491\n",
            "train loss:1.1116731276323017\n",
            "train loss:0.6890921992005087\n",
            "train loss:0.8588955446043438\n",
            "train loss:0.8181475029612126\n",
            "train loss:0.8578596165499291\n",
            "train loss:0.86964416774062\n",
            "train loss:0.7866634612633149\n",
            "train loss:0.906798463501519\n",
            "train loss:0.7936219103528722\n",
            "train loss:0.8843247901526321\n",
            "train loss:0.7304104973405322\n",
            "train loss:0.8306035115756236\n",
            "train loss:0.974771070859545\n",
            "train loss:0.876285923416649\n",
            "train loss:0.9380051807547247\n",
            "train loss:0.7480499267954416\n",
            "train loss:0.9354517032392138\n",
            "train loss:0.6775527215200857\n",
            "train loss:0.7772916460095484\n",
            "train loss:0.8387038748987217\n",
            "train loss:0.7926294433905148\n",
            "train loss:0.9447219146488705\n",
            "train loss:0.9241217272220393\n",
            "train loss:0.9421026003083685\n",
            "train loss:0.8310333423646782\n",
            "train loss:0.8176438333015823\n",
            "train loss:0.9356481604028768\n",
            "train loss:0.8306015054627038\n",
            "train loss:0.8557005706323632\n",
            "train loss:0.9440065106983481\n",
            "train loss:0.8431798528925206\n",
            "train loss:0.8052456242908577\n",
            "train loss:0.7850431611380847\n",
            "train loss:0.7995101174962734\n",
            "train loss:0.7647981226529968\n",
            "train loss:0.8262458946054879\n",
            "train loss:0.8227655662601543\n",
            "train loss:0.7900898178300008\n",
            "train loss:0.8181120325572543\n",
            "train loss:0.8730376623765805\n",
            "train loss:0.9533887984864969\n",
            "train loss:0.9436189289809452\n",
            "train loss:0.7119350616254068\n",
            "train loss:0.9642166527639141\n",
            "train loss:0.9034268165660532\n",
            "train loss:0.7713995674072067\n",
            "train loss:0.8762117171654826\n",
            "train loss:0.839028770959076\n",
            "train loss:0.7830575767380381\n",
            "train loss:0.9320497102920606\n",
            "train loss:0.6704861402933987\n",
            "train loss:0.8504438314398765\n",
            "train loss:0.9913126587457656\n",
            "train loss:0.8152540870687512\n",
            "train loss:0.9050080265293292\n",
            "train loss:0.9964004085949472\n",
            "train loss:0.8140096978354956\n",
            "train loss:0.8877175343392534\n",
            "train loss:0.9383843481188101\n",
            "train loss:0.7964102574729806\n",
            "train loss:0.9186420889798532\n",
            "train loss:0.8288611161241936\n",
            "train loss:0.8944725283785496\n",
            "train loss:0.9949147958236952\n",
            "train loss:0.8241989375722174\n",
            "train loss:0.891319631404967\n",
            "train loss:0.8656648717818319\n",
            "train loss:0.9067679154414944\n",
            "train loss:0.8024451666457886\n",
            "train loss:0.8103659440669528\n",
            "train loss:0.9254235832094296\n",
            "train loss:0.8016826195459984\n",
            "train loss:0.8155572459321629\n",
            "train loss:0.8675365767018853\n",
            "train loss:1.0700844432402794\n",
            "train loss:0.8065559822459385\n",
            "train loss:0.9660097703436668\n",
            "train loss:1.0309520062736002\n",
            "train loss:0.6843059174498304\n",
            "train loss:0.8783397938320125\n",
            "train loss:0.8617730270680471\n",
            "train loss:1.009047194100458\n",
            "train loss:0.8095118254747831\n",
            "train loss:0.8264039053465208\n",
            "train loss:1.0767049006019282\n",
            "train loss:0.7777395928403084\n",
            "train loss:0.8303303240276612\n",
            "train loss:0.8179304005152135\n",
            "train loss:0.8983315059836143\n",
            "train loss:0.8320353145680193\n",
            "train loss:0.7927618227634003\n",
            "train loss:0.8817816824980846\n",
            "train loss:0.9405523209725343\n",
            "train loss:1.0923404412276516\n",
            "train loss:1.012116489793805\n",
            "train loss:0.8166321896505064\n",
            "train loss:0.9879774634506177\n",
            "train loss:0.8436346883689736\n",
            "train loss:0.9268809288397444\n",
            "train loss:0.8221968991426982\n",
            "train loss:0.9206955538748095\n",
            "train loss:0.9930113696401218\n",
            "train loss:0.9004795660692883\n",
            "train loss:0.7366951991983453\n",
            "train loss:0.9312985868999636\n",
            "train loss:0.8433718191825278\n",
            "train loss:0.8716548236509628\n",
            "train loss:0.7326172826195367\n",
            "train loss:0.7398548327101463\n",
            "train loss:0.6928968428029889\n",
            "train loss:0.7021287669356207\n",
            "train loss:1.0000140198174414\n",
            "train loss:0.9745311511167535\n",
            "train loss:0.9892597273807548\n",
            "train loss:0.7697651852753359\n",
            "train loss:0.8356706341863546\n",
            "train loss:0.8887182818155545\n",
            "train loss:0.8785672717562492\n",
            "train loss:0.8581818268812578\n",
            "train loss:1.0412991621863659\n",
            "train loss:0.7295376202773605\n",
            "train loss:0.6873831810757522\n",
            "train loss:0.9549206457454682\n",
            "train loss:0.9532903381011202\n",
            "train loss:0.905780630959322\n",
            "train loss:0.8537432154415547\n",
            "train loss:0.8008283885739168\n",
            "train loss:0.83895501517335\n",
            "train loss:0.7573585288745958\n",
            "train loss:0.8306272838945352\n",
            "train loss:1.0610948223616696\n",
            "train loss:0.8272384003532436\n",
            "train loss:0.9198425556390416\n",
            "train loss:0.9175274514283484\n",
            "train loss:0.7452030560341985\n",
            "train loss:0.8499773408609991\n",
            "train loss:0.7598408636121802\n",
            "train loss:0.893179473052746\n",
            "train loss:0.8885483669277463\n",
            "train loss:0.8427688646608561\n",
            "train loss:0.9675726487883922\n",
            "train loss:0.8397280947937176\n",
            "train loss:0.8752908225223908\n",
            "train loss:0.8528133755084235\n",
            "train loss:0.782151269294897\n",
            "train loss:0.8004078059259893\n",
            "train loss:0.810238364448867\n",
            "train loss:0.8339295561639591\n",
            "train loss:0.7478539517800865\n",
            "train loss:0.8325855058578688\n",
            "train loss:0.7979931781587014\n",
            "train loss:0.9847896047360243\n",
            "train loss:0.7925690642711837\n",
            "train loss:0.9097279026217221\n",
            "train loss:0.8613587175792453\n",
            "train loss:0.8414533878236319\n",
            "train loss:0.7338449135862133\n",
            "train loss:0.9177582048315754\n",
            "train loss:0.919165271854532\n",
            "train loss:0.8493154915151432\n",
            "train loss:0.866674886004574\n",
            "train loss:0.9273507202861256\n",
            "train loss:0.7612883995494407\n",
            "train loss:0.8170910988757163\n",
            "train loss:0.9411172517166431\n",
            "train loss:0.9768582039370163\n",
            "train loss:0.7107347345819801\n",
            "train loss:0.8747229775309583\n",
            "train loss:0.8812480646735739\n",
            "train loss:0.946404913810374\n",
            "train loss:0.8083000208016969\n",
            "train loss:0.6478941960000499\n",
            "train loss:0.8233844453624921\n",
            "train loss:0.9791878622751632\n",
            "train loss:0.9421112444686283\n",
            "train loss:0.8641006194521682\n",
            "train loss:0.8209123833192183\n",
            "train loss:0.9736770176624789\n",
            "train loss:0.8417006970153654\n",
            "train loss:0.7992680449202046\n",
            "train loss:0.7556353352934334\n",
            "train loss:0.8648959279511063\n",
            "train loss:0.8513164738068913\n",
            "train loss:0.8074997093319483\n",
            "train loss:0.9204758151995701\n",
            "train loss:0.7444590493483695\n",
            "train loss:0.8237114399654688\n",
            "train loss:0.9442694756835437\n",
            "train loss:1.0009620580972944\n",
            "train loss:0.8784705682278624\n",
            "train loss:1.0769198247196656\n",
            "train loss:0.9865208515109084\n",
            "train loss:0.8801129374871928\n",
            "train loss:0.8456715590480431\n",
            "train loss:0.8455285378809336\n",
            "train loss:0.9441690869477956\n",
            "train loss:0.8678698396089182\n",
            "train loss:0.875660552375821\n",
            "train loss:0.7542222773507129\n",
            "train loss:0.7312562938189986\n",
            "train loss:0.9889813749734832\n",
            "train loss:0.802887756622808\n",
            "train loss:0.7932114796517561\n",
            "train loss:0.7422251928727579\n",
            "train loss:0.937743391417159\n",
            "train loss:0.8688044031345382\n",
            "train loss:0.8776304856250896\n",
            "train loss:0.896664952697193\n",
            "train loss:0.8437265740545399\n",
            "train loss:0.8454468919251521\n",
            "train loss:0.7807129790202241\n",
            "train loss:0.9206614317370664\n",
            "train loss:0.8358460469603795\n",
            "train loss:0.8731232600404499\n",
            "train loss:0.8409275517613948\n",
            "train loss:0.9335699421808397\n",
            "train loss:1.0241620415664832\n",
            "train loss:0.8651486331562184\n",
            "train loss:0.7189435164066771\n",
            "train loss:0.8782913905477697\n",
            "train loss:0.95807334223548\n",
            "train loss:0.8613003557832393\n",
            "train loss:0.9718940897397585\n",
            "train loss:0.9520943494968972\n",
            "train loss:0.9124312696636193\n",
            "train loss:0.6894917716671294\n",
            "train loss:0.7752321408179362\n",
            "train loss:0.7282555645050846\n",
            "train loss:0.8446342636421741\n",
            "train loss:0.9177078712868333\n",
            "train loss:0.8210509090694074\n",
            "train loss:0.7017414778919799\n",
            "train loss:0.8612949966966387\n",
            "train loss:0.8467777208561889\n",
            "train loss:0.7266724724748453\n",
            "train loss:0.9328882880981074\n",
            "train loss:0.8730849957950884\n",
            "train loss:0.9084903063716727\n",
            "train loss:0.8609681135312472\n",
            "train loss:0.795794604176112\n",
            "train loss:0.8159494587843713\n",
            "train loss:0.9093469545764067\n",
            "train loss:0.6969243614456091\n",
            "train loss:0.8318210772473545\n",
            "train loss:0.8491104165861438\n",
            "train loss:0.821791131828798\n",
            "train loss:0.8207581512473666\n",
            "train loss:0.7531431747930466\n",
            "train loss:0.8350953417130116\n",
            "train loss:0.8257673617195143\n",
            "train loss:0.916142443605651\n",
            "train loss:0.6603479709060432\n",
            "train loss:0.9826822643811177\n",
            "train loss:0.855364152115193\n",
            "train loss:0.7215408096298107\n",
            "train loss:0.8338804351551014\n",
            "train loss:0.7819417263031794\n",
            "train loss:0.8227504008389558\n",
            "train loss:0.8475928451872916\n",
            "train loss:0.8954220713097767\n",
            "train loss:0.7042765207862226\n",
            "train loss:0.8254113987688723\n",
            "train loss:0.730578253446168\n",
            "train loss:0.7971833150318719\n",
            "train loss:0.8287141155050708\n",
            "train loss:0.9162522078576077\n",
            "train loss:0.801421495105437\n",
            "train loss:0.9399601991721699\n",
            "train loss:0.8283340705634354\n",
            "train loss:0.6838772178327873\n",
            "train loss:0.7601554600050975\n",
            "train loss:0.8190110128549883\n",
            "train loss:0.9062036145784818\n",
            "train loss:1.0061338647176066\n",
            "train loss:0.9082841131671867\n",
            "train loss:0.8269302361364594\n",
            "train loss:0.9675925978809807\n",
            "train loss:0.9279054172554766\n",
            "train loss:0.7077773996872212\n",
            "train loss:0.9718517493005896\n",
            "train loss:0.7551965917893051\n",
            "train loss:0.8196375045489483\n",
            "train loss:0.8760856552964432\n",
            "train loss:0.7787517489428333\n",
            "train loss:0.8866984056627983\n",
            "train loss:0.783476420124751\n",
            "train loss:0.754796276203316\n",
            "train loss:0.8043353663325373\n",
            "train loss:0.7475558569397761\n",
            "train loss:0.8138140502756163\n",
            "train loss:0.7651642117384037\n",
            "train loss:0.9754048289017179\n",
            "train loss:1.0531225384738483\n",
            "train loss:0.7873104477351983\n",
            "train loss:0.809852108421484\n",
            "train loss:0.81011075685269\n",
            "train loss:0.8798741727667446\n",
            "train loss:0.8501481873301462\n",
            "train loss:0.8742255399969787\n",
            "train loss:0.8150910920214074\n",
            "train loss:0.9213531626693358\n",
            "train loss:0.8844251177653522\n",
            "train loss:0.8763230275455968\n",
            "train loss:0.8699565264411815\n",
            "train loss:0.8714091457810983\n",
            "train loss:0.9043472177812655\n",
            "train loss:0.7842868600305725\n",
            "train loss:0.8811996894955982\n",
            "train loss:0.9138233435360062\n",
            "train loss:0.904127209605948\n",
            "train loss:0.784181153070052\n",
            "train loss:0.7307889769589927\n",
            "train loss:0.8504675547006079\n",
            "train loss:0.9382017422673675\n",
            "train loss:0.7128918621176078\n",
            "train loss:0.8606798297915603\n",
            "train loss:0.81873556912033\n",
            "train loss:0.8954958580024716\n",
            "train loss:1.0092462067647832\n",
            "train loss:0.7973920622998238\n",
            "train loss:1.0606162534016201\n",
            "train loss:0.9388398723308287\n",
            "train loss:0.9427519494917764\n",
            "train loss:0.7642047490214255\n",
            "train loss:0.9162873034044493\n",
            "train loss:0.7626806855952962\n",
            "train loss:1.0682109865249867\n",
            "train loss:1.05507767173578\n",
            "train loss:0.9505661633432063\n",
            "train loss:0.8899230972638429\n",
            "train loss:0.8216643341769192\n",
            "train loss:0.7354904336735386\n",
            "train loss:0.7505671830610225\n",
            "train loss:1.2385720980956076\n",
            "train loss:0.7922725410596944\n",
            "train loss:0.7489105633107718\n",
            "train loss:1.0479513195169883\n",
            "train loss:0.9145310108973302\n",
            "train loss:0.9334764701718707\n",
            "train loss:0.623680956015875\n",
            "train loss:0.7665717753591934\n",
            "train loss:0.893908476600689\n",
            "train loss:0.9164261905882283\n",
            "train loss:0.9275530739907829\n",
            "train loss:0.7468997071589762\n",
            "train loss:0.8822226966759193\n",
            "train loss:0.7387847075840522\n",
            "train loss:0.8423469568405301\n",
            "train loss:0.7478025897710039\n",
            "train loss:0.8678130548722259\n",
            "train loss:0.9407574108859788\n",
            "train loss:0.8328894977569206\n",
            "train loss:0.853116000854778\n",
            "train loss:1.1051251805530244\n",
            "train loss:0.7771574391882677\n",
            "train loss:0.9074926744025393\n",
            "train loss:0.801973455579377\n",
            "train loss:0.8703013166345958\n",
            "train loss:0.9479442057921293\n",
            "train loss:0.8384000792368207\n",
            "train loss:0.8075259367920691\n",
            "train loss:0.9663903577612717\n",
            "train loss:0.8360766602099232\n",
            "train loss:0.9310122331024943\n",
            "train loss:0.8758318523148564\n",
            "train loss:0.7299879729229471\n",
            "train loss:1.0420908223573695\n",
            "train loss:0.8570579479369436\n",
            "train loss:0.7456352935596687\n",
            "train loss:0.7430909519111343\n",
            "train loss:0.869075034930739\n",
            "train loss:0.8365238578981347\n",
            "train loss:0.7372761883122511\n",
            "train loss:0.7550539816999111\n",
            "train loss:0.8578119485982094\n",
            "train loss:0.8107001908124181\n",
            "train loss:0.8684716657946159\n",
            "train loss:0.8900318975581257\n",
            "train loss:0.9426534800966812\n",
            "train loss:0.6961534720073456\n",
            "train loss:0.7949126964777856\n",
            "train loss:0.8759877775279068\n",
            "train loss:0.7140335610995523\n",
            "train loss:0.7966410715170604\n",
            "train loss:0.9294277978456255\n",
            "train loss:0.9003749146059988\n",
            "train loss:0.8581832278779795\n",
            "train loss:0.8600039678332969\n",
            "train loss:0.8056682042548222\n",
            "train loss:0.8474099286044461\n",
            "train loss:0.7161445672516881\n",
            "train loss:0.797243441437327\n",
            "train loss:0.8690966449730074\n",
            "train loss:0.995340146254211\n",
            "train loss:0.9264325169791722\n",
            "train loss:0.8109735773450235\n",
            "train loss:0.8701688735918823\n",
            "train loss:0.9206779207789222\n",
            "train loss:0.854568762063151\n",
            "train loss:0.907867425018286\n",
            "train loss:0.7354414862300722\n",
            "train loss:0.7397362097499203\n",
            "train loss:0.8089705837787331\n",
            "train loss:0.9892215023316494\n",
            "train loss:0.7615914264853092\n",
            "train loss:0.9438000997478091\n",
            "train loss:0.8461452335908384\n",
            "train loss:0.6723148236074188\n",
            "train loss:0.9843862603798079\n",
            "train loss:0.586836384866097\n",
            "train loss:0.9063626186390809\n",
            "train loss:0.8019684454991605\n",
            "train loss:0.8106505078162525\n",
            "train loss:0.8347206342527882\n",
            "train loss:0.705469379705754\n",
            "train loss:0.787838607948317\n",
            "train loss:0.8790972705863386\n",
            "train loss:0.849445411899708\n",
            "train loss:0.8895860521492537\n",
            "train loss:0.8333508907894661\n",
            "train loss:0.6371613990782568\n",
            "train loss:0.7564810587756741\n",
            "train loss:0.9191710935242666\n",
            "train loss:0.7294295011918428\n",
            "train loss:0.787590673937596\n",
            "train loss:1.0090018975018038\n",
            "train loss:1.0348013612614937\n",
            "train loss:0.8771503171666933\n",
            "train loss:0.9651214818969526\n",
            "train loss:0.8745934142198577\n",
            "train loss:1.0715132259260265\n",
            "train loss:0.7448782507331217\n",
            "train loss:0.9333945485867311\n",
            "train loss:0.7975292170900259\n",
            "train loss:0.8085949969393961\n",
            "train loss:0.7840878863723862\n",
            "train loss:0.8408543876362393\n",
            "train loss:0.8916796781641267\n",
            "train loss:0.8210992281520376\n",
            "train loss:0.8918922593313263\n",
            "train loss:0.9492792622888457\n",
            "train loss:0.7688304234250394\n",
            "train loss:0.854018629259506\n",
            "train loss:0.869138948040748\n",
            "train loss:0.7911644970568642\n",
            "train loss:0.9871269142819322\n",
            "train loss:1.0155692332776762\n",
            "train loss:0.8918738495393255\n",
            "train loss:0.8506911072896008\n",
            "train loss:0.7404208110250221\n",
            "train loss:0.8956769179112334\n",
            "=== epoch:14, train acc:0.996, test acc:0.996 ===\n",
            "train loss:0.901104424241427\n",
            "train loss:0.6833338449486999\n",
            "train loss:1.0035639333100073\n",
            "train loss:0.8691902984997849\n",
            "train loss:1.0259153099613791\n",
            "train loss:0.9779368661665534\n",
            "train loss:0.6820521433713194\n",
            "train loss:0.98768569502078\n",
            "train loss:0.9514110785454929\n",
            "train loss:0.8039936190825453\n",
            "train loss:0.8577031330861574\n",
            "train loss:0.8902449425502078\n",
            "train loss:0.9152637496479179\n",
            "train loss:0.8344242720739004\n",
            "train loss:1.0341600756951086\n",
            "train loss:0.8508180284512824\n",
            "train loss:0.7999667128087209\n",
            "train loss:0.9044040082095884\n",
            "train loss:1.0530159548305489\n",
            "train loss:0.9138951256198102\n",
            "train loss:0.9973387125376919\n",
            "train loss:0.9043729496662821\n",
            "train loss:0.8858392659655479\n",
            "train loss:0.7968687047895109\n",
            "train loss:0.7823210328812393\n",
            "train loss:0.762911485296032\n",
            "train loss:0.7227243972977963\n",
            "train loss:0.985576827328679\n",
            "train loss:0.9059781672030175\n",
            "train loss:0.8385918571910875\n",
            "train loss:1.0497187546366815\n",
            "train loss:0.8075256494963554\n",
            "train loss:0.9301543486000163\n",
            "train loss:0.8033350217050365\n",
            "train loss:0.9315761747245781\n",
            "train loss:1.103681573509547\n",
            "train loss:0.8765551639549389\n",
            "train loss:0.7569520428851869\n",
            "train loss:0.983151581340666\n",
            "train loss:0.8251627445044609\n",
            "train loss:0.8556822701027201\n",
            "train loss:0.916266542631774\n",
            "train loss:0.994294101316555\n",
            "train loss:1.0239497660765908\n",
            "train loss:0.826750168738109\n",
            "train loss:0.7649722175109106\n",
            "train loss:0.8852616062265496\n",
            "train loss:0.8435823413336742\n",
            "train loss:0.9353050284997575\n",
            "train loss:0.8384901327775673\n",
            "train loss:0.7988316972767078\n",
            "train loss:0.8659447165535867\n",
            "train loss:0.8390662432084683\n",
            "train loss:0.9150995175778864\n",
            "train loss:0.7318077252173208\n",
            "train loss:0.7906868010834546\n",
            "train loss:0.897727084021415\n",
            "train loss:0.8419995293711908\n",
            "train loss:0.8135737099695011\n",
            "train loss:0.9976294709087871\n",
            "train loss:0.8397734006977072\n",
            "train loss:0.9418043156409597\n",
            "train loss:0.8580727833839287\n",
            "train loss:0.9836460388930647\n",
            "train loss:0.8183645370975967\n",
            "train loss:1.0084717343604277\n",
            "train loss:0.8288247033572944\n",
            "train loss:0.8674308931389202\n",
            "train loss:0.7403263725908473\n",
            "train loss:0.8225596940232529\n",
            "train loss:0.8034527351890081\n",
            "train loss:0.8759997565116171\n",
            "train loss:0.8253930322763982\n",
            "train loss:0.7840100716032664\n",
            "train loss:1.0296798361350818\n",
            "train loss:0.9081848985323866\n",
            "train loss:1.0016450352301263\n",
            "train loss:0.6833704354489174\n",
            "train loss:0.8325025561507841\n",
            "train loss:0.8452299790216099\n",
            "train loss:0.9035792842952616\n",
            "train loss:0.8604575012292445\n",
            "train loss:0.9268681440791453\n",
            "train loss:0.8603428169068833\n",
            "train loss:0.9448737774550018\n",
            "train loss:0.881489188107507\n",
            "train loss:0.6977650240331712\n",
            "train loss:0.8909960322108493\n",
            "train loss:1.0193430988525802\n",
            "train loss:0.6734471938641388\n",
            "train loss:0.948181189736038\n",
            "train loss:1.00081364392248\n",
            "train loss:0.907738584049697\n",
            "train loss:0.864163979941514\n",
            "train loss:0.873123200609743\n",
            "train loss:0.8690169607254157\n",
            "train loss:0.7659178559899155\n",
            "train loss:0.7695617022609449\n",
            "train loss:0.9077044688902922\n",
            "train loss:0.9149513754323578\n",
            "train loss:0.8020394920433602\n",
            "train loss:0.857979069928466\n",
            "train loss:0.8106296812822545\n",
            "train loss:0.9935853795373333\n",
            "train loss:0.9904277102484248\n",
            "train loss:1.0447810886317503\n",
            "train loss:0.8476551457697167\n",
            "train loss:0.7771276528054453\n",
            "train loss:0.8936761002939726\n",
            "train loss:0.9073249620440501\n",
            "train loss:0.844083576975294\n",
            "train loss:0.9346545261251419\n",
            "train loss:0.976421490022745\n",
            "train loss:0.9799116712350946\n",
            "train loss:0.9037406312944194\n",
            "train loss:0.7443677531030147\n",
            "train loss:0.9455144441100006\n",
            "train loss:1.0074585343557034\n",
            "train loss:0.9815077167746631\n",
            "train loss:0.9060992413301503\n",
            "train loss:0.7014289778548672\n",
            "train loss:0.6953056257996131\n",
            "train loss:0.8673020332443496\n",
            "train loss:0.8736171522921731\n",
            "train loss:0.7083503496327356\n",
            "train loss:0.7882421774256453\n",
            "train loss:0.9814474724997096\n",
            "train loss:0.9645848220880608\n",
            "train loss:0.9304680884261067\n",
            "train loss:0.860629002540368\n",
            "train loss:0.9742337550360702\n",
            "train loss:0.8120696148043174\n",
            "train loss:0.8994616422869919\n",
            "train loss:0.8225718197358858\n",
            "train loss:0.9563533562690718\n",
            "train loss:0.8166389181343767\n",
            "train loss:1.0439561414952365\n",
            "train loss:0.7498761767000769\n",
            "train loss:0.929584486224064\n",
            "train loss:0.8063301625831016\n",
            "train loss:0.8145663251317291\n",
            "train loss:0.8626022008628668\n",
            "train loss:0.8293634654182758\n",
            "train loss:0.8076367903691727\n",
            "train loss:0.9291509796894133\n",
            "train loss:0.7597815210380915\n",
            "train loss:0.9692487678132545\n",
            "train loss:0.9520969820455686\n",
            "train loss:0.9823017456634854\n",
            "train loss:0.9075779131403539\n",
            "train loss:0.932499775926624\n",
            "train loss:0.7463179319840577\n",
            "train loss:0.8979799017264023\n",
            "train loss:0.6988153693041612\n",
            "train loss:0.8621715895627928\n",
            "train loss:0.7522547736541011\n",
            "train loss:0.9790544659026714\n",
            "train loss:1.0072841718005972\n",
            "train loss:0.7577512795214209\n",
            "train loss:0.8514999175242467\n",
            "train loss:0.860923133628093\n",
            "train loss:0.8201790939044858\n",
            "train loss:0.8422757218139251\n",
            "train loss:0.7771984963806083\n",
            "train loss:0.8171937317150825\n",
            "train loss:0.7292112216320946\n",
            "train loss:0.9195100660763177\n",
            "train loss:0.9248095224292247\n",
            "train loss:0.9593213296092656\n",
            "train loss:0.8111511210474691\n",
            "train loss:0.9316115404393271\n",
            "train loss:1.0639344476266204\n",
            "train loss:0.911359816204509\n",
            "train loss:0.8466455274203093\n",
            "train loss:0.9545148957501794\n",
            "train loss:0.8666188470882256\n",
            "train loss:0.8743179681005371\n",
            "train loss:0.8250160954176308\n",
            "train loss:1.0689544819630654\n",
            "train loss:0.8948983728991106\n",
            "train loss:0.8483090614427681\n",
            "train loss:0.7895376502419146\n",
            "train loss:0.9500849760589515\n",
            "train loss:0.8511020195544785\n",
            "train loss:0.5536895462273198\n",
            "train loss:0.9231783676239954\n",
            "train loss:0.8483581797213964\n",
            "train loss:0.8473947887662634\n",
            "train loss:0.7810309926345456\n",
            "train loss:0.931830838044525\n",
            "train loss:0.948822792880689\n",
            "train loss:0.7646359886205719\n",
            "train loss:0.851808065299575\n",
            "train loss:0.6713406070708274\n",
            "train loss:0.9200893126349867\n",
            "train loss:1.1014099028929856\n",
            "train loss:1.0141327083850866\n",
            "train loss:0.7825994683711016\n",
            "train loss:0.9095901378086896\n",
            "train loss:0.898991679662255\n",
            "train loss:0.7445579616093652\n",
            "train loss:1.061828346070807\n",
            "train loss:0.8395057493307287\n",
            "train loss:0.9971379284484391\n",
            "train loss:0.865336633803048\n",
            "train loss:0.7370941727243084\n",
            "train loss:1.0009283316198954\n",
            "train loss:0.9784163752892704\n",
            "train loss:0.9775651238328539\n",
            "train loss:0.8832360218991218\n",
            "train loss:0.8381591661301724\n",
            "train loss:0.8177099285670799\n",
            "train loss:0.8462043158272876\n",
            "train loss:0.8732164511161858\n",
            "train loss:0.9036124073421941\n",
            "train loss:0.7377355512853743\n",
            "train loss:0.8046220416916635\n",
            "train loss:0.8745517131845939\n",
            "train loss:0.8081858439391029\n",
            "train loss:0.89042469535166\n",
            "train loss:0.7223571536525201\n",
            "train loss:0.8834387893119192\n",
            "train loss:0.8005890850560583\n",
            "train loss:1.0914620594581792\n",
            "train loss:1.1275288445011835\n",
            "train loss:0.7592509805844944\n",
            "train loss:0.862393734850033\n",
            "train loss:0.8735402653587427\n",
            "train loss:0.9511348937618544\n",
            "train loss:1.0297453333467113\n",
            "train loss:0.9389328575640448\n",
            "train loss:0.8606555762647824\n",
            "train loss:0.7121514020713239\n",
            "train loss:0.7576147110910201\n",
            "train loss:0.6686238447074453\n",
            "train loss:0.9576551331341355\n",
            "train loss:0.8634942662597769\n",
            "train loss:0.9329072340772753\n",
            "train loss:0.7834778312726302\n",
            "train loss:0.8406734684378949\n",
            "train loss:1.002645143902794\n",
            "train loss:0.7950209741915438\n",
            "train loss:0.99504009619886\n",
            "train loss:0.8477012178484005\n",
            "train loss:0.726997410246596\n",
            "train loss:0.8135614149367046\n",
            "train loss:0.9532981697430096\n",
            "train loss:0.8107414848591904\n",
            "train loss:0.7652282622163833\n",
            "train loss:0.7652543316756035\n",
            "train loss:0.8422680757153378\n",
            "train loss:0.9068769654529619\n",
            "train loss:0.9579224734338478\n",
            "train loss:1.0426623749586268\n",
            "train loss:0.7493401258911154\n",
            "train loss:0.7252034858694053\n",
            "train loss:0.8157930680891277\n",
            "train loss:0.9108165312403912\n",
            "train loss:0.8256085998525325\n",
            "train loss:0.8346412221489647\n",
            "train loss:0.8578993709980405\n",
            "train loss:0.8679643631174723\n",
            "train loss:0.7412942788953006\n",
            "train loss:0.7050587178643921\n",
            "train loss:0.8403531894478007\n",
            "train loss:0.8380492445587145\n",
            "train loss:0.9078588756566205\n",
            "train loss:1.0364237904058289\n",
            "train loss:0.9402444406504726\n",
            "train loss:0.8849489101948274\n",
            "train loss:0.8783524292416952\n",
            "train loss:1.032379103800264\n",
            "train loss:0.8671564131021979\n",
            "train loss:0.844870999238836\n",
            "train loss:0.8185634877077331\n",
            "train loss:0.7513276491042501\n",
            "train loss:0.8761554304023653\n",
            "train loss:0.972073095854916\n",
            "train loss:0.8179946449009033\n",
            "train loss:0.8681249137296047\n",
            "train loss:1.0146202753789595\n",
            "train loss:0.932192759914272\n",
            "train loss:0.8788419731834525\n",
            "train loss:0.7532384952076826\n",
            "train loss:0.9589858213093676\n",
            "train loss:0.8439261926959181\n",
            "train loss:0.8588030065741973\n",
            "train loss:0.8644271668147527\n",
            "train loss:0.9202621876851619\n",
            "train loss:0.7710403952643249\n",
            "train loss:0.9457713174121757\n",
            "train loss:0.748514540084684\n",
            "train loss:0.8806619595293879\n",
            "train loss:1.0623702744457986\n",
            "train loss:0.9038389407588617\n",
            "train loss:0.9148421954026061\n",
            "train loss:0.9147117133095959\n",
            "train loss:0.8270316220426615\n",
            "train loss:0.8878520663743401\n",
            "train loss:0.9003926789001254\n",
            "train loss:0.7937720679447569\n",
            "train loss:0.9671679621365551\n",
            "train loss:0.6741358495614255\n",
            "train loss:0.814321629690629\n",
            "train loss:0.8610188221966868\n",
            "train loss:0.771816719108442\n",
            "train loss:0.8951412594674444\n",
            "train loss:0.7204632560320912\n",
            "train loss:1.017654921600768\n",
            "train loss:0.8530191679307582\n",
            "train loss:0.8489071464414683\n",
            "train loss:0.8222970888100468\n",
            "train loss:0.7782916645577381\n",
            "train loss:0.810135936657289\n",
            "train loss:0.788936539353497\n",
            "train loss:0.9514523067155158\n",
            "train loss:0.8278142110775013\n",
            "train loss:0.8926503585463696\n",
            "train loss:0.9136400690251084\n",
            "train loss:0.7775849211170296\n",
            "train loss:0.8899316890495856\n",
            "train loss:0.6825078565645684\n",
            "train loss:0.7256855819238346\n",
            "train loss:0.9165562235508662\n",
            "train loss:0.9052522557667837\n",
            "train loss:0.8131357271686552\n",
            "train loss:1.1244397094882759\n",
            "train loss:0.7519986024783052\n",
            "train loss:0.8313643088615896\n",
            "train loss:0.9783303251475179\n",
            "train loss:0.906941043177798\n",
            "train loss:0.8007172671600616\n",
            "train loss:0.9817604983167492\n",
            "train loss:0.6962830446964934\n",
            "train loss:0.8635111456530916\n",
            "train loss:0.9467337661758825\n",
            "train loss:0.8932718433710225\n",
            "train loss:0.920889286975581\n",
            "train loss:0.8698062847744816\n",
            "train loss:0.7548472054369922\n",
            "train loss:0.8899898537096286\n",
            "train loss:0.7357794082348933\n",
            "train loss:0.8135346499285719\n",
            "train loss:0.809438443393582\n",
            "train loss:0.8519610768891132\n",
            "train loss:1.0280123387176707\n",
            "train loss:0.8641287168541416\n",
            "train loss:0.7756623910475446\n",
            "train loss:0.8777872270358341\n",
            "train loss:0.7287313048271506\n",
            "train loss:0.9011780339046312\n",
            "train loss:0.6487630760245784\n",
            "train loss:0.8577364406730678\n",
            "train loss:0.8193177681210116\n",
            "train loss:0.9747920238151798\n",
            "train loss:0.847038331866908\n",
            "train loss:0.9288935053060549\n",
            "train loss:0.8370790986019933\n",
            "train loss:0.7393459578149455\n",
            "train loss:0.9490823962253843\n",
            "train loss:0.9087151334009773\n",
            "train loss:0.8244844647776042\n",
            "train loss:0.7182731079393634\n",
            "train loss:0.6993985662085724\n",
            "train loss:0.9544214405588101\n",
            "train loss:0.8297276435277583\n",
            "train loss:0.9320244253788073\n",
            "train loss:0.8564733508232321\n",
            "train loss:0.7803690568787252\n",
            "train loss:0.9028798149973879\n",
            "train loss:0.9080922504093799\n",
            "train loss:0.8228967916841172\n",
            "train loss:0.9461621852741764\n",
            "train loss:0.7542206603812092\n",
            "train loss:0.937195327098483\n",
            "train loss:0.9608278689856394\n",
            "train loss:0.8372876328545451\n",
            "train loss:0.6833388311712021\n",
            "train loss:0.8191782828190871\n",
            "train loss:0.8984381708352873\n",
            "train loss:0.8906505337025442\n",
            "train loss:0.7781014146352334\n",
            "train loss:1.021529375303619\n",
            "train loss:0.8061734181419489\n",
            "train loss:0.8565356667689629\n",
            "train loss:0.9056303868330225\n",
            "train loss:0.868000230429357\n",
            "train loss:0.8903840155351626\n",
            "train loss:0.6786176576211633\n",
            "train loss:0.9356887928230391\n",
            "train loss:0.9298434019357997\n",
            "train loss:0.8371323347633758\n",
            "train loss:0.9803674224687654\n",
            "train loss:0.7788214875720151\n",
            "train loss:0.8277364541106613\n",
            "train loss:0.8866546285621127\n",
            "train loss:0.9253025058697715\n",
            "train loss:0.7822164101166588\n",
            "train loss:0.8706519984436212\n",
            "train loss:0.8915845848279226\n",
            "train loss:0.8976687257634037\n",
            "train loss:0.8155884023324151\n",
            "train loss:1.1320144897113062\n",
            "train loss:0.9703354523076907\n",
            "train loss:0.961295138003496\n",
            "train loss:0.9407685144871207\n",
            "train loss:0.8416046898706836\n",
            "train loss:0.8799598445008624\n",
            "train loss:0.8940416507497382\n",
            "train loss:0.8959438457930369\n",
            "train loss:0.824862849542965\n",
            "train loss:0.8821807267638652\n",
            "train loss:0.8236566552711547\n",
            "train loss:0.6888520307526956\n",
            "train loss:0.9023386710878912\n",
            "train loss:1.008131853351097\n",
            "train loss:0.8373609163026365\n",
            "train loss:0.8675000824086968\n",
            "train loss:0.8775205934234782\n",
            "train loss:0.8484470779322408\n",
            "train loss:0.986390548082356\n",
            "train loss:0.918121170337299\n",
            "train loss:0.8268846783243611\n",
            "train loss:0.9570709369000451\n",
            "train loss:0.7873357791877457\n",
            "train loss:0.9328989705912878\n",
            "train loss:0.7771625538738469\n",
            "train loss:0.7906598242410955\n",
            "train loss:0.8413334975993851\n",
            "train loss:0.9954799878696611\n",
            "train loss:0.8396904713755724\n",
            "train loss:0.8203830663960717\n",
            "train loss:0.7766221292255312\n",
            "train loss:0.7854682236522228\n",
            "train loss:0.7657303242898854\n",
            "train loss:0.9170368709773544\n",
            "train loss:0.7930012278603964\n",
            "train loss:0.9470809452941493\n",
            "train loss:0.7754159517159437\n",
            "train loss:0.8026604583134292\n",
            "train loss:0.8153448084967202\n",
            "train loss:0.9424470933890874\n",
            "train loss:0.90176752600196\n",
            "train loss:0.7573284723082806\n",
            "train loss:0.8390920484739313\n",
            "train loss:0.9576105000028732\n",
            "train loss:0.9803475368287417\n",
            "train loss:0.8490284713328495\n",
            "train loss:0.8784317292176708\n",
            "train loss:1.0283301504199946\n",
            "train loss:0.9193231282378129\n",
            "train loss:0.809725690426991\n",
            "train loss:0.8252240257674319\n",
            "train loss:0.9465387171296029\n",
            "train loss:0.8312709205983916\n",
            "train loss:0.949067635219554\n",
            "train loss:0.8519633499969662\n",
            "train loss:0.7130240391717563\n",
            "train loss:0.8346033044061396\n",
            "train loss:0.8457910103672975\n",
            "train loss:0.9370054147228654\n",
            "train loss:0.7975175531427112\n",
            "train loss:0.929567111391547\n",
            "train loss:0.8506926851185928\n",
            "train loss:0.9033735835760747\n",
            "train loss:0.9317641449115783\n",
            "train loss:0.8527362111156943\n",
            "train loss:0.9979546768128654\n",
            "train loss:0.9581771000374156\n",
            "train loss:0.9975571423692539\n",
            "train loss:0.9202680503015545\n",
            "train loss:0.9779315121692318\n",
            "train loss:0.7481634490364761\n",
            "train loss:0.8219249643108258\n",
            "train loss:0.8011315270377214\n",
            "train loss:0.9041552898425473\n",
            "train loss:0.8822673574802371\n",
            "train loss:0.8950913229020716\n",
            "train loss:0.8237229985649229\n",
            "train loss:0.8618145614098719\n",
            "train loss:0.8728158498868531\n",
            "train loss:0.999927424593589\n",
            "train loss:0.6983272563192096\n",
            "train loss:0.9395545022638843\n",
            "train loss:0.9003214998005088\n",
            "train loss:0.78688633188925\n",
            "train loss:0.8701767736467313\n",
            "train loss:0.8815146793406149\n",
            "train loss:0.8038810664281099\n",
            "train loss:0.7759145403324453\n",
            "train loss:0.8784025962677876\n",
            "train loss:0.820079933425382\n",
            "train loss:0.853484375525587\n",
            "train loss:0.9014214548144475\n",
            "train loss:0.8107684804064248\n",
            "train loss:0.938751889585172\n",
            "train loss:0.905259483463512\n",
            "train loss:0.8490091334263044\n",
            "train loss:0.9127050934449983\n",
            "train loss:0.9289763003270701\n",
            "train loss:0.8671362843753854\n",
            "train loss:0.8148130420206656\n",
            "train loss:0.8553107246310193\n",
            "train loss:0.8702920805176704\n",
            "train loss:0.9337364984567393\n",
            "train loss:0.8715641670353904\n",
            "train loss:0.8414171291951474\n",
            "train loss:0.876178942204502\n",
            "train loss:0.7855620941950032\n",
            "train loss:0.7711050553373096\n",
            "train loss:0.6713292203164091\n",
            "train loss:0.9167260648618933\n",
            "train loss:0.8326786112715249\n",
            "train loss:0.6575959513986251\n",
            "train loss:0.7647255210918945\n",
            "train loss:0.9032644404381894\n",
            "train loss:0.9634641029698078\n",
            "train loss:0.7146649036641862\n",
            "train loss:0.8811584403200671\n",
            "train loss:0.9210851529068641\n",
            "train loss:0.8557074935249471\n",
            "train loss:0.737532312476654\n",
            "train loss:0.7211700451049233\n",
            "train loss:0.8826206139257242\n",
            "train loss:0.8558855975371128\n",
            "train loss:0.9320796985679685\n",
            "train loss:1.0126520181891006\n",
            "train loss:0.8525427970557942\n",
            "train loss:0.8715766997635354\n",
            "train loss:0.8582352624844459\n",
            "train loss:0.8302761391023403\n",
            "train loss:0.8473637374381441\n",
            "train loss:0.8376773513577485\n",
            "train loss:0.8863108263166629\n",
            "train loss:0.8920458194341317\n",
            "train loss:0.7550970538074319\n",
            "train loss:0.9499171383602873\n",
            "train loss:0.7811401675508856\n",
            "train loss:0.7908844926071812\n",
            "train loss:1.0217291030289573\n",
            "train loss:0.9468159207441947\n",
            "train loss:0.8226714597659189\n",
            "train loss:0.743690637341446\n",
            "train loss:0.9046259494100976\n",
            "train loss:0.9255445564396375\n",
            "train loss:0.7866617155185441\n",
            "train loss:0.879243035142419\n",
            "train loss:0.8786384198302689\n",
            "train loss:0.8501083897289348\n",
            "train loss:0.7206940210403973\n",
            "train loss:0.699221379319945\n",
            "train loss:0.7636350611622217\n",
            "train loss:0.7756358733087457\n",
            "train loss:0.949691451497862\n",
            "train loss:0.8935267426010887\n",
            "train loss:0.8637052083175074\n",
            "train loss:0.7429862362582963\n",
            "train loss:0.7786269039935365\n",
            "train loss:0.8930852457194384\n",
            "train loss:0.9621923535956936\n",
            "train loss:0.825493816884497\n",
            "train loss:0.9736244192404395\n",
            "train loss:0.8669159614569412\n",
            "train loss:0.7536449745745641\n",
            "train loss:0.8902558023454019\n",
            "train loss:0.8814639272716353\n",
            "train loss:0.9258778458413518\n",
            "train loss:0.8817816135454479\n",
            "train loss:0.8087519207657692\n",
            "train loss:0.9243950589537381\n",
            "train loss:0.9636959406489499\n",
            "train loss:0.9749840993210483\n",
            "train loss:0.8597118127961614\n",
            "train loss:0.9709260731397609\n",
            "train loss:0.7294682438988004\n",
            "train loss:0.8836089841154198\n",
            "train loss:0.986199122742125\n",
            "train loss:1.0937853518822598\n",
            "train loss:0.8047126086375023\n",
            "train loss:0.7679233286013246\n",
            "train loss:0.7866833972340972\n",
            "train loss:1.0045518444634245\n",
            "train loss:0.7987005279742095\n",
            "train loss:0.8279274834134476\n",
            "train loss:0.8448062769948034\n",
            "train loss:0.9692317500141823\n",
            "train loss:0.9268587836446817\n",
            "train loss:0.7502896965499545\n",
            "train loss:0.9501457841818928\n",
            "train loss:0.9792401968150004\n",
            "train loss:0.863252743419156\n",
            "train loss:0.7330813092895646\n",
            "train loss:0.9153352550726399\n",
            "train loss:0.8742216737908447\n",
            "train loss:0.8286402885256957\n",
            "train loss:0.7156833775216666\n",
            "train loss:0.7802339743054166\n",
            "train loss:0.9366983045079695\n",
            "train loss:1.022109409100155\n",
            "train loss:0.9020278707694274\n",
            "=== epoch:15, train acc:0.998, test acc:0.991 ===\n",
            "train loss:0.8040134560602346\n",
            "train loss:0.7857387151596636\n",
            "train loss:0.8826545727890399\n",
            "train loss:0.8318866461512614\n",
            "train loss:0.8342969328850844\n",
            "train loss:0.8198494712366111\n",
            "train loss:0.8647762663055315\n",
            "train loss:0.7981518600824681\n",
            "train loss:0.7541819928259007\n",
            "train loss:0.7659947534888958\n",
            "train loss:0.7475735294317412\n",
            "train loss:0.8702532416428497\n",
            "train loss:0.9047004079922217\n",
            "train loss:1.0814924240137678\n",
            "train loss:0.8614002037786741\n",
            "train loss:0.9205219300947018\n",
            "train loss:0.9394645556917044\n",
            "train loss:0.9120073954202762\n",
            "train loss:0.9650458810048579\n",
            "train loss:0.8930767411200516\n",
            "train loss:0.7393053261197733\n",
            "train loss:0.9018767965725626\n",
            "train loss:0.9079762927196956\n",
            "train loss:0.8073485326584589\n",
            "train loss:0.8571129511526494\n",
            "train loss:0.9374757198934341\n",
            "train loss:0.7522202410619835\n",
            "train loss:0.9184968166004585\n",
            "train loss:0.777714184121213\n",
            "train loss:0.8961698529154947\n",
            "train loss:0.877325825955629\n",
            "train loss:0.655044581026391\n",
            "train loss:0.760769557695393\n",
            "train loss:0.7473738017843051\n",
            "train loss:0.7421139439701908\n",
            "train loss:0.7785616831975162\n",
            "train loss:0.8986901969577409\n",
            "train loss:0.8650700792800297\n",
            "train loss:0.9862869063908154\n",
            "train loss:0.8413621445355817\n",
            "train loss:0.8333452308125535\n",
            "train loss:0.7746812830610764\n",
            "train loss:0.89694205897172\n",
            "train loss:0.8969140056558749\n",
            "train loss:0.7489012604660779\n",
            "train loss:0.8235308288454694\n",
            "train loss:0.772622516986585\n",
            "train loss:0.9750038528898851\n",
            "train loss:0.7810819082519624\n",
            "train loss:0.8314364967031085\n",
            "train loss:0.8452120572502696\n",
            "train loss:0.7537549071185817\n",
            "train loss:0.9646565803575481\n",
            "train loss:0.8356060871085583\n",
            "train loss:0.9517123933781183\n",
            "train loss:0.7369929366360765\n",
            "train loss:0.7215448643661531\n",
            "train loss:0.8280316900147996\n",
            "train loss:0.915170124603096\n",
            "train loss:0.9007367112969322\n",
            "train loss:0.8097859800517268\n",
            "train loss:0.8571572224538911\n",
            "train loss:0.7730295358772687\n",
            "train loss:0.8337460347782283\n",
            "train loss:0.8663983756566792\n",
            "train loss:1.039600786355983\n",
            "train loss:0.8473367488537158\n",
            "train loss:0.7660269973077938\n",
            "train loss:0.9006434222397601\n",
            "train loss:0.8756267005975709\n",
            "train loss:0.897366654765101\n",
            "train loss:0.95315770869597\n",
            "train loss:0.9826303092208619\n",
            "train loss:0.8482100007563002\n",
            "train loss:0.9483502658111291\n",
            "train loss:0.85709273428752\n",
            "train loss:0.8578284639729167\n",
            "train loss:0.8689385364392841\n",
            "train loss:0.8877932518789369\n",
            "train loss:0.8273967166203956\n",
            "train loss:0.8939952763510688\n",
            "train loss:0.8455289104656787\n",
            "train loss:0.8353963547974554\n",
            "train loss:0.8614532003483177\n",
            "train loss:0.723276308673898\n",
            "train loss:0.840853205066757\n",
            "train loss:0.899273306020197\n",
            "train loss:0.8028638076274377\n",
            "train loss:0.9866514176661925\n",
            "train loss:0.9738545164408572\n",
            "train loss:0.8426384527834706\n",
            "train loss:0.7956786010468936\n",
            "train loss:0.933811545128441\n",
            "train loss:0.752241711836418\n",
            "train loss:0.7164091770783177\n",
            "train loss:0.8289919420458763\n",
            "train loss:0.956650535341682\n",
            "train loss:1.1368846851562102\n",
            "train loss:0.9295136239085382\n",
            "train loss:0.9402511854361967\n",
            "train loss:1.0348928914784243\n",
            "train loss:0.9349369124254439\n",
            "train loss:0.7685796681764672\n",
            "train loss:0.9977130349466803\n",
            "train loss:0.6243314789835535\n",
            "train loss:0.9616554019389558\n",
            "train loss:0.8836770897515934\n",
            "train loss:0.8862521164385364\n",
            "train loss:0.9537943013083295\n",
            "train loss:0.9299148501402016\n",
            "train loss:0.7658502664132344\n",
            "train loss:0.9068836775017144\n",
            "train loss:0.8249808207636334\n",
            "train loss:0.7281902978579553\n",
            "train loss:0.8233947387858255\n",
            "train loss:0.8431444875578832\n",
            "train loss:0.9409768751635187\n",
            "train loss:0.8323273557849743\n",
            "train loss:0.9378153203904215\n",
            "train loss:0.8390137463244339\n",
            "train loss:0.9715523711357092\n",
            "train loss:1.0046286737859342\n",
            "train loss:0.7419978055469483\n",
            "train loss:0.9297603061295376\n",
            "train loss:0.8812883754101981\n",
            "train loss:0.9385740648914268\n",
            "train loss:0.807478594477842\n",
            "train loss:0.7617854254590212\n",
            "train loss:0.814270381723773\n",
            "train loss:0.8742032608946962\n",
            "train loss:0.7196292179642382\n",
            "train loss:0.7118567378659361\n",
            "train loss:0.8797273296822061\n",
            "train loss:0.8256352578910373\n",
            "train loss:0.953744572808662\n",
            "train loss:0.7799659255800512\n",
            "train loss:0.8591172233470046\n",
            "train loss:0.7401228302653214\n",
            "train loss:0.9673751338180996\n",
            "train loss:0.9133159287439463\n",
            "train loss:0.8356267237849824\n",
            "train loss:0.9011065586760526\n",
            "train loss:0.9038142575077782\n",
            "train loss:0.8002075342163941\n",
            "train loss:0.7805172949515664\n",
            "train loss:0.9615388797537325\n",
            "train loss:0.6791978038946889\n",
            "train loss:0.6130994651741419\n",
            "train loss:0.8078292530945846\n",
            "train loss:0.884801909758229\n",
            "train loss:0.8616423107559008\n",
            "train loss:0.7342374920225809\n",
            "train loss:0.736353145093359\n",
            "train loss:0.7234615205299764\n",
            "train loss:1.009474223848322\n",
            "train loss:0.8813930486171725\n",
            "train loss:0.8089603029035284\n",
            "train loss:0.8118360639406116\n",
            "train loss:0.8167732415658041\n",
            "train loss:0.8329599587403229\n",
            "train loss:0.9326111576013146\n",
            "train loss:0.8510510909745544\n",
            "train loss:0.7345895290843459\n",
            "train loss:0.8358415182418004\n",
            "train loss:0.9016547637565283\n",
            "train loss:1.006104524404521\n",
            "train loss:0.9119390835675303\n",
            "train loss:0.965069059399156\n",
            "train loss:0.7808253010679121\n",
            "train loss:0.8183593607899664\n",
            "train loss:0.9423003091899911\n",
            "train loss:0.9276076792700191\n",
            "train loss:0.8986340645417803\n",
            "train loss:0.8510901138917888\n",
            "train loss:0.8723271499777114\n",
            "train loss:0.8718100371055332\n",
            "train loss:0.8521629445206956\n",
            "train loss:0.7531182064905744\n",
            "train loss:0.7699496946659736\n",
            "train loss:0.9357658613475518\n",
            "train loss:0.9645234257770586\n",
            "train loss:0.7304111427716634\n",
            "train loss:1.0247731772418092\n",
            "train loss:1.01786629275991\n",
            "train loss:0.8956145660177594\n",
            "train loss:0.9535917044805781\n",
            "train loss:1.036293219684767\n",
            "train loss:0.7857762457770501\n",
            "train loss:0.7482794428931941\n",
            "train loss:0.7918207660213996\n",
            "train loss:0.9024142766954801\n",
            "train loss:0.9194096221736113\n",
            "train loss:0.6931900423223104\n",
            "train loss:0.9382364020653449\n",
            "train loss:0.8842260990211037\n",
            "train loss:0.8290853581497686\n",
            "train loss:0.8965157169491978\n",
            "train loss:0.6715927441118466\n",
            "train loss:0.6432893863354348\n",
            "train loss:0.9838265029617247\n",
            "train loss:0.7907592506304388\n",
            "train loss:0.8540385478676612\n",
            "train loss:0.801508836195213\n",
            "train loss:0.8933015390197077\n",
            "train loss:0.8431719678753824\n",
            "train loss:0.8278839942027774\n",
            "train loss:0.9801585388074914\n",
            "train loss:0.8608314005173758\n",
            "train loss:0.9582810173614054\n",
            "train loss:0.7389909457568034\n",
            "train loss:0.7332744332966005\n",
            "train loss:0.8492790951578941\n",
            "train loss:0.7821068172837005\n",
            "train loss:0.8627129878636048\n",
            "train loss:0.9455898425686471\n",
            "train loss:0.8213351353829076\n",
            "train loss:0.8862655313253779\n",
            "train loss:0.8612175019643749\n",
            "train loss:0.8532023915117274\n",
            "train loss:0.8515022907604707\n",
            "train loss:0.9483128735053438\n",
            "train loss:0.886214891613263\n",
            "train loss:0.9516044282528792\n",
            "train loss:0.9181841498008603\n",
            "train loss:0.8995957263709373\n",
            "train loss:0.8474234367323165\n",
            "train loss:0.8230111875208199\n",
            "train loss:0.930474946515953\n",
            "train loss:0.9421586919522287\n",
            "train loss:0.7666674370837673\n",
            "train loss:0.8216769653866837\n",
            "train loss:0.9112359646352656\n",
            "train loss:0.9012917376341715\n",
            "train loss:0.9340531564909582\n",
            "train loss:0.6871008309886609\n",
            "train loss:0.9319591934147303\n",
            "train loss:0.959058891837571\n",
            "train loss:0.9618648392159248\n",
            "train loss:1.0130430045137324\n",
            "train loss:0.893636479779995\n",
            "train loss:0.773812631929671\n",
            "train loss:0.9763630262129346\n",
            "train loss:0.8175225995736107\n",
            "train loss:0.8520715492712722\n",
            "train loss:0.8162039137472402\n",
            "train loss:0.7902555827894109\n",
            "train loss:0.8204390203320413\n",
            "train loss:0.7640067006097719\n",
            "train loss:1.0293241471758339\n",
            "train loss:0.9519642874609083\n",
            "train loss:0.9539660505291105\n",
            "train loss:0.7672957168400814\n",
            "train loss:0.7715149180133213\n",
            "train loss:1.045155055560536\n",
            "train loss:0.8928071506979862\n",
            "train loss:0.9453538947945076\n",
            "train loss:0.9607406131803373\n",
            "train loss:0.739785237400508\n",
            "train loss:0.7889309418982937\n",
            "train loss:0.8660915786564125\n",
            "train loss:0.8316352187955944\n",
            "train loss:1.0634929749509583\n",
            "train loss:0.9595457362022549\n",
            "train loss:0.8623351198112019\n",
            "train loss:0.7838008052650186\n",
            "train loss:0.8673411856161037\n",
            "train loss:0.8518473436878898\n",
            "train loss:0.9043973288607746\n",
            "train loss:0.9140604492861246\n",
            "train loss:0.8821007485623424\n",
            "train loss:0.853130303044429\n",
            "train loss:0.8364637407312259\n",
            "train loss:0.8369848916593479\n",
            "train loss:0.8033478071595715\n",
            "train loss:0.9006426191576814\n",
            "train loss:0.8859951708766751\n",
            "train loss:0.7946008634782251\n",
            "train loss:0.7475977349470928\n",
            "train loss:0.9726025213356734\n",
            "train loss:0.7220757723866932\n",
            "train loss:0.948568794121345\n",
            "train loss:0.8810050244618278\n",
            "train loss:0.9740310411583327\n",
            "train loss:0.8000266765519823\n",
            "train loss:0.7945830811794484\n",
            "train loss:0.7164654557747538\n",
            "train loss:0.7428927929213465\n",
            "train loss:0.9620887690545473\n",
            "train loss:1.0369416880214222\n",
            "train loss:0.847988077245073\n",
            "train loss:0.7960708650085188\n",
            "train loss:0.7418002716495665\n",
            "train loss:0.9060437038161155\n",
            "train loss:0.7850228998233417\n",
            "train loss:0.8335849324516829\n",
            "train loss:0.782122027369747\n",
            "train loss:0.8573485997552999\n",
            "train loss:0.9869461629730282\n",
            "train loss:0.8887968514526322\n",
            "train loss:0.7265291654632384\n",
            "train loss:0.8567283303791732\n",
            "train loss:0.7758389961093761\n",
            "train loss:0.9057513915195236\n",
            "train loss:0.7210540536586137\n",
            "train loss:0.99100630424985\n",
            "train loss:0.8278349681347494\n",
            "train loss:0.8351549551821293\n",
            "train loss:0.9606463773528027\n",
            "train loss:1.0685662613093796\n",
            "train loss:0.8159701742599362\n",
            "train loss:0.9812198683191707\n",
            "train loss:0.7812212253270739\n",
            "train loss:0.8245418600665912\n",
            "train loss:0.7302072397409372\n",
            "train loss:0.7024245006490972\n",
            "train loss:0.7140559631362775\n",
            "train loss:0.769195178674613\n",
            "train loss:0.7964060712057673\n",
            "train loss:0.8099375534963911\n",
            "train loss:0.67715016433039\n",
            "train loss:0.7445412657202835\n",
            "train loss:0.7076658818291182\n",
            "train loss:0.846280268992408\n",
            "train loss:1.028502946537908\n",
            "train loss:0.9105530016164575\n",
            "train loss:0.9565486088800412\n",
            "train loss:0.7975558262032021\n",
            "train loss:1.0667627126708117\n",
            "train loss:0.877634435023451\n",
            "train loss:0.8624558666895507\n",
            "train loss:0.9724012543167154\n",
            "train loss:0.9275273019401105\n",
            "train loss:0.8528314773754608\n",
            "train loss:0.8777340899497744\n",
            "train loss:0.7610953349165805\n",
            "train loss:0.7246177022356852\n",
            "train loss:0.8537221318000848\n",
            "train loss:0.8292577912272193\n",
            "train loss:0.8492906759076306\n",
            "train loss:0.8507818917898592\n",
            "train loss:0.8312251716926794\n",
            "train loss:0.8889881238000662\n",
            "train loss:0.8136716467321814\n",
            "train loss:0.8333243655125994\n",
            "train loss:0.8435800888320862\n",
            "train loss:0.8699146968808349\n",
            "train loss:0.7655814095056479\n",
            "train loss:0.8386178201252686\n",
            "train loss:0.8871343236696174\n",
            "train loss:0.9399062166124452\n",
            "train loss:0.7190402073109603\n",
            "train loss:0.8999580304413679\n",
            "train loss:0.8569869295156488\n",
            "train loss:0.8893480034632673\n",
            "train loss:0.7826764464594702\n",
            "train loss:0.7762312378455166\n",
            "train loss:0.8200172547790326\n",
            "train loss:0.8220982280030116\n",
            "train loss:0.9875670713056838\n",
            "train loss:0.8745190190964222\n",
            "train loss:0.8192035806642148\n",
            "train loss:0.736968984931782\n",
            "train loss:0.8073973384879671\n",
            "train loss:1.0117351221311124\n",
            "train loss:0.7524071500007052\n",
            "train loss:0.9003385433182187\n",
            "train loss:0.8091205629324439\n",
            "train loss:0.9570899009084539\n",
            "train loss:0.662706506600089\n",
            "train loss:0.7025365478832256\n",
            "train loss:0.8388247489289404\n",
            "train loss:0.8221762399354254\n",
            "train loss:0.6798394716629663\n",
            "train loss:0.8721456071168519\n",
            "train loss:0.8836088478181824\n",
            "train loss:0.8245568820118362\n",
            "train loss:0.9865391506489037\n",
            "train loss:0.8070179080182129\n",
            "train loss:0.7461435844421983\n",
            "train loss:0.9215821123430002\n",
            "train loss:0.8911372983205191\n",
            "train loss:0.8035776432357659\n",
            "train loss:0.8508243514533831\n",
            "train loss:1.0448324519162495\n",
            "train loss:0.6728328784363723\n",
            "train loss:0.7861653192048129\n",
            "train loss:0.860051752871219\n",
            "train loss:0.8660258233916643\n",
            "train loss:0.9374784977354979\n",
            "train loss:0.9695460942846428\n",
            "train loss:0.7573877822650129\n",
            "train loss:0.7656669668143246\n",
            "train loss:0.8209826719048262\n",
            "train loss:0.9465870600637386\n",
            "train loss:0.8721846498613921\n",
            "train loss:0.7757358063335412\n",
            "train loss:0.8434036977890322\n",
            "train loss:0.8299685226622979\n",
            "train loss:0.911580337338142\n",
            "train loss:0.8373451314853662\n",
            "train loss:0.9227630624069605\n",
            "train loss:0.9562319860205101\n",
            "train loss:0.8811554659167204\n",
            "train loss:0.9118468765175625\n",
            "train loss:0.8079406706796952\n",
            "train loss:0.8227772868778779\n",
            "train loss:0.8211011677759532\n",
            "train loss:0.9348145281216909\n",
            "train loss:1.0117535052516642\n",
            "train loss:0.8839068603543387\n",
            "train loss:1.0477133680375892\n",
            "train loss:0.7783153473804104\n",
            "train loss:0.9334589065692765\n",
            "train loss:0.8168238909747306\n",
            "train loss:0.8073208753394405\n",
            "train loss:0.742574815578293\n",
            "train loss:0.9033214292263725\n",
            "train loss:0.8171191427972495\n",
            "train loss:0.8904922323126577\n",
            "train loss:0.9073448895153655\n",
            "train loss:1.015938926703821\n",
            "train loss:0.7621376112363913\n",
            "train loss:0.91940799693186\n",
            "train loss:0.9639731473329571\n",
            "train loss:0.8600020692669105\n",
            "train loss:1.0213665372364558\n",
            "train loss:0.8369575151831353\n",
            "train loss:0.7965461629417188\n",
            "train loss:0.7029641702116006\n",
            "train loss:0.9550307424146327\n",
            "train loss:0.921797368445969\n",
            "train loss:0.7619133872164702\n",
            "train loss:0.8338460383818662\n",
            "train loss:0.7888330060491415\n",
            "train loss:0.8193996469359288\n",
            "train loss:0.77940783253085\n",
            "train loss:0.8757177907439332\n",
            "train loss:0.8669855027188424\n",
            "train loss:0.8917333428771912\n",
            "train loss:0.9050875506402108\n",
            "train loss:0.9262867375993477\n",
            "train loss:0.8660071764298759\n",
            "train loss:0.9029751820450058\n",
            "train loss:0.7142537957452315\n",
            "train loss:0.92216601405561\n",
            "train loss:0.7848001514691224\n",
            "train loss:0.803843100799101\n",
            "train loss:0.9888210607961839\n",
            "train loss:0.9838983295084853\n",
            "train loss:1.012217612614668\n",
            "train loss:1.0997801494212687\n",
            "train loss:0.7782183455215856\n",
            "train loss:0.882072883939371\n",
            "train loss:0.7542557834729802\n",
            "train loss:0.8488844367355535\n",
            "train loss:0.9637833049648857\n",
            "train loss:0.8239119703568716\n",
            "train loss:0.8346973340768187\n",
            "train loss:0.7637347112372328\n",
            "train loss:0.8563027965036524\n",
            "train loss:0.8252701383228833\n",
            "train loss:0.755987297512465\n",
            "train loss:0.8710432570991601\n",
            "train loss:0.8644564456554856\n",
            "train loss:0.6163672706592845\n",
            "train loss:0.9563406477679106\n",
            "train loss:0.8124579724962121\n",
            "train loss:0.7962675172879686\n",
            "train loss:0.8946987270011247\n",
            "train loss:0.8925429691167593\n",
            "train loss:0.9234411644885777\n",
            "train loss:0.9843071302822246\n",
            "train loss:0.8240466690325622\n",
            "train loss:0.9367580704352066\n",
            "train loss:0.8254117252561578\n",
            "train loss:0.8866857191906257\n",
            "train loss:0.8079610532353313\n",
            "train loss:0.8998324593451358\n",
            "train loss:0.7901377467325287\n",
            "train loss:0.8031114792121918\n",
            "train loss:0.8400772196285972\n",
            "train loss:0.9613570193757016\n",
            "train loss:0.7832661924085356\n",
            "train loss:0.8301045343152316\n",
            "train loss:0.9032101212823023\n",
            "train loss:1.0696565051905218\n",
            "train loss:0.764999046081834\n",
            "train loss:0.6523647241333806\n",
            "train loss:0.8850074832026901\n",
            "train loss:0.9388638350584765\n",
            "train loss:0.8568408840593338\n",
            "train loss:0.9356908397707939\n",
            "train loss:0.8343798103838442\n",
            "train loss:0.8106826817799316\n",
            "train loss:0.9484882879884198\n",
            "train loss:0.7500073165801965\n",
            "train loss:0.8682125183029814\n",
            "train loss:0.8929433497824019\n",
            "train loss:0.8382002392931882\n",
            "train loss:0.8508759360478757\n",
            "train loss:0.848036429969669\n",
            "train loss:0.8420859857798031\n",
            "train loss:0.8030711261243471\n",
            "train loss:0.8249747767397864\n",
            "train loss:0.8548641196514282\n",
            "train loss:0.7379497633372709\n",
            "train loss:0.7666036828385646\n",
            "train loss:0.9295961926350074\n",
            "train loss:0.9682603828661855\n",
            "train loss:0.8770006554915888\n",
            "train loss:0.7643077384546317\n",
            "train loss:0.87970297215067\n",
            "train loss:0.7950960550886031\n",
            "train loss:1.007033975974934\n",
            "train loss:0.8074723537714878\n",
            "train loss:0.8740715414551388\n",
            "train loss:0.8764128076819511\n",
            "train loss:0.9781425443634991\n",
            "train loss:0.8707887483851828\n",
            "train loss:0.8168759051396293\n",
            "train loss:0.9614279425654734\n",
            "train loss:1.0413437504732164\n",
            "train loss:0.914473782878445\n",
            "train loss:0.7750598607981712\n",
            "train loss:0.7169656282758953\n",
            "train loss:0.6902955758702959\n",
            "train loss:0.8989877096302963\n",
            "train loss:0.8499123880691218\n",
            "train loss:0.9170579661732582\n",
            "train loss:0.9270097791662684\n",
            "train loss:0.8190894079990625\n",
            "train loss:0.8853351741039224\n",
            "train loss:0.8163936795938705\n",
            "train loss:0.8347122282292254\n",
            "train loss:0.8334047604298698\n",
            "train loss:1.0654620659306269\n",
            "train loss:0.9185184025926181\n",
            "train loss:0.7939808782995084\n",
            "train loss:0.9875832739200768\n",
            "train loss:0.8045196965897377\n",
            "train loss:0.8564240932080384\n",
            "train loss:0.8589921751123577\n",
            "train loss:0.9943957599394548\n",
            "train loss:0.7719771825412264\n",
            "train loss:0.9002877290540435\n",
            "train loss:0.7973084936877993\n",
            "train loss:0.8831185512080967\n",
            "train loss:0.8937309460661321\n",
            "train loss:0.8654597652402832\n",
            "train loss:1.0528411647584202\n",
            "train loss:0.91515380913414\n",
            "train loss:1.0130424914724445\n",
            "train loss:0.8253831604128271\n",
            "train loss:0.9691408847018923\n",
            "train loss:0.9092265683712134\n",
            "train loss:0.8834012027299187\n",
            "train loss:0.8181750753269693\n",
            "train loss:0.8888599284097558\n",
            "train loss:0.8348351810682556\n",
            "train loss:0.748824983563785\n",
            "train loss:0.8018930218299832\n",
            "train loss:0.782499033516579\n",
            "train loss:0.7489219834471146\n",
            "train loss:0.8369225184130454\n",
            "train loss:0.8667490192319133\n",
            "train loss:0.8308347563482639\n",
            "train loss:0.8680038521808625\n",
            "train loss:0.8252713696474652\n",
            "train loss:0.763515094495327\n",
            "train loss:0.967795269161261\n",
            "train loss:0.7656472330951948\n",
            "train loss:0.8420205328690398\n",
            "train loss:0.7345561465651035\n",
            "train loss:0.8100788035546362\n",
            "train loss:0.817634186110909\n",
            "train loss:0.975521219652347\n",
            "train loss:0.8478007602287729\n",
            "train loss:0.6582173907316698\n",
            "train loss:0.7162453841414664\n",
            "train loss:0.9310254949306257\n",
            "train loss:0.8469121622167095\n",
            "train loss:0.8080577040159704\n",
            "train loss:0.9003234580861579\n",
            "train loss:0.930568100857544\n",
            "train loss:0.8621090147462454\n",
            "train loss:0.8060530243486198\n",
            "train loss:0.9794138303201527\n",
            "train loss:0.7547513001938851\n",
            "train loss:0.7831072985178151\n",
            "train loss:0.7240760168230913\n",
            "train loss:0.8963484040979606\n",
            "train loss:0.8531939996411321\n",
            "train loss:0.7176954630791604\n",
            "train loss:0.9215075383629724\n",
            "train loss:0.96862410823663\n",
            "train loss:0.7881585341251034\n",
            "train loss:0.8400617507681035\n",
            "train loss:0.9341998498961303\n",
            "train loss:0.8264801021291796\n",
            "train loss:0.9300938200246982\n",
            "=== epoch:16, train acc:0.995, test acc:0.989 ===\n",
            "train loss:0.8677233098337206\n",
            "train loss:0.802135114669192\n",
            "train loss:0.7140031201062187\n",
            "train loss:0.8317069304244237\n",
            "train loss:0.9665128522265725\n",
            "train loss:0.8123750699633208\n",
            "train loss:0.7639459617697897\n",
            "train loss:0.7328914479479731\n",
            "train loss:1.0432166735284685\n",
            "train loss:0.9926717259935889\n",
            "train loss:0.9786430648606398\n",
            "train loss:0.761718278980661\n",
            "train loss:0.7503457958587706\n",
            "train loss:0.7313400239317847\n",
            "train loss:0.90258289276114\n",
            "train loss:0.7238543080158574\n",
            "train loss:0.8285937082029702\n",
            "train loss:0.8605591250473736\n",
            "train loss:0.9206870876027261\n",
            "train loss:0.7060939920099903\n",
            "train loss:0.7675144047885871\n",
            "train loss:0.9630804274130667\n",
            "train loss:0.8544496571668809\n",
            "train loss:0.7828132472327877\n",
            "train loss:0.8658785582390752\n",
            "train loss:0.8489345508493692\n",
            "train loss:0.9033291438098052\n",
            "train loss:0.8618286510509389\n",
            "train loss:0.9761823246788272\n",
            "train loss:0.7506871872779604\n",
            "train loss:0.7953020782298204\n",
            "train loss:0.8698600561536897\n",
            "train loss:0.9424249359147742\n",
            "train loss:0.7595172891682946\n",
            "train loss:0.7645085979379715\n",
            "train loss:0.712188985915783\n",
            "train loss:0.839749079895823\n",
            "train loss:0.8061085944209847\n",
            "train loss:0.7626843852686052\n",
            "train loss:0.9685660242930865\n",
            "train loss:0.8312492725405924\n",
            "train loss:0.8152106904323976\n",
            "train loss:0.8859337562684187\n",
            "train loss:0.7386194316349023\n",
            "train loss:0.9272987904001536\n",
            "train loss:0.8816529896669211\n",
            "train loss:0.9437227793958628\n",
            "train loss:0.858553582541661\n",
            "train loss:0.9394763114810699\n",
            "train loss:0.7200817902488026\n",
            "train loss:0.8382962530264568\n",
            "train loss:0.9291297824934185\n",
            "train loss:0.8163070887541786\n",
            "train loss:0.9384542084067615\n",
            "train loss:0.8458136747428655\n",
            "train loss:1.0390658619608948\n",
            "train loss:0.8057304669133185\n",
            "train loss:0.9440440406392647\n",
            "train loss:0.8601685317084502\n",
            "train loss:0.7601697843028709\n",
            "train loss:0.8444931020926599\n",
            "train loss:0.7849972112260782\n",
            "train loss:0.7841350343106811\n",
            "train loss:0.7588282961919661\n",
            "train loss:0.9131686891518147\n",
            "train loss:0.8065641592655709\n",
            "train loss:0.9739602924923224\n",
            "train loss:0.8804290101389965\n",
            "train loss:0.7976223214340523\n",
            "train loss:0.850945605023175\n",
            "train loss:0.8411404773778426\n",
            "train loss:0.9725853859218881\n",
            "train loss:1.0335242746973665\n",
            "train loss:0.792554807245728\n",
            "train loss:0.8713642955236578\n",
            "train loss:0.7667323822782157\n",
            "train loss:0.880088171955868\n",
            "train loss:0.7880271389405454\n",
            "train loss:0.8631389744477773\n",
            "train loss:0.9364277207752835\n",
            "train loss:0.9174656157671996\n",
            "train loss:0.7932096567026649\n",
            "train loss:0.784837990674858\n",
            "train loss:0.740567572975943\n",
            "train loss:1.0896407926636649\n",
            "train loss:0.9424714569572855\n",
            "train loss:0.8022803170423926\n",
            "train loss:0.9300099850750988\n",
            "train loss:0.8213137471270628\n",
            "train loss:0.9195030300443529\n",
            "train loss:0.9851923449815976\n",
            "train loss:0.8034990137599771\n",
            "train loss:0.8002123501078628\n",
            "train loss:0.9624238291477256\n",
            "train loss:0.9077591835885813\n",
            "train loss:1.004183993271534\n",
            "train loss:0.832505303159445\n",
            "train loss:0.7936989787734415\n",
            "train loss:0.6837411173610279\n",
            "train loss:0.9162225337210511\n",
            "train loss:0.7726315093862881\n",
            "train loss:0.7786575889622046\n",
            "train loss:0.89191351845327\n",
            "train loss:1.0331695498441933\n",
            "train loss:0.7257258330627435\n",
            "train loss:0.8448651931466551\n",
            "train loss:0.8948780056860832\n",
            "train loss:0.9039914706886123\n",
            "train loss:0.8257064742433171\n",
            "train loss:0.7638467525978199\n",
            "train loss:0.8293149042426003\n",
            "train loss:0.9375462832436615\n",
            "train loss:0.8689479151250191\n",
            "train loss:0.7715759230133421\n",
            "train loss:0.906655369105755\n",
            "train loss:0.7131736886135998\n",
            "train loss:0.9084703872745916\n",
            "train loss:0.8046191403605563\n",
            "train loss:0.7097035496469007\n",
            "train loss:0.7769908471003609\n",
            "train loss:0.8815639378684307\n",
            "train loss:0.8882670990881806\n",
            "train loss:0.7658278876796086\n",
            "train loss:0.8397203308390231\n",
            "train loss:0.8160452760277576\n",
            "train loss:0.7201099825153848\n",
            "train loss:0.8973341254290846\n",
            "train loss:0.9038689070147418\n",
            "train loss:1.029913999064682\n",
            "train loss:1.0103571827998616\n",
            "train loss:0.8145148178041236\n",
            "train loss:1.0275950444838537\n",
            "train loss:0.8762030208790699\n",
            "train loss:0.772389551274709\n",
            "train loss:0.8279914656708288\n",
            "train loss:0.8194192674410136\n",
            "train loss:1.0063995733613342\n",
            "train loss:0.8132529862420901\n",
            "train loss:1.0460483081841465\n",
            "train loss:0.6879958059801357\n",
            "train loss:0.8003451385584257\n",
            "train loss:0.8733404631624387\n",
            "train loss:0.8965370663880228\n",
            "train loss:0.8353577063993827\n",
            "train loss:0.8698115165781407\n",
            "train loss:0.8407229412171305\n",
            "train loss:0.8275736359472674\n",
            "train loss:0.7995867412639752\n",
            "train loss:0.7908920016729464\n",
            "train loss:0.7591880118907842\n",
            "train loss:0.8939009498002639\n",
            "train loss:0.7365078098326003\n",
            "train loss:0.933759568013776\n",
            "train loss:1.0608372937364705\n",
            "train loss:0.7721043402386099\n",
            "train loss:0.9105528461047757\n",
            "train loss:0.9085056172819698\n",
            "train loss:0.8387904160474862\n",
            "train loss:0.8127122583501172\n",
            "train loss:0.7794796037423066\n",
            "train loss:0.8337870200080998\n",
            "train loss:0.7412910050541257\n",
            "train loss:0.7353282840592604\n",
            "train loss:0.7466124874299256\n",
            "train loss:0.8251392100151637\n",
            "train loss:0.8984151705786627\n",
            "train loss:0.9876832421586406\n",
            "train loss:1.0093767930979014\n",
            "train loss:0.9345159677832136\n",
            "train loss:1.0157421391276527\n",
            "train loss:0.6651766517553948\n",
            "train loss:0.9109031304253642\n",
            "train loss:0.7663379148768169\n",
            "train loss:0.9897124902630468\n",
            "train loss:0.9092091216472291\n",
            "train loss:0.9591422332644087\n",
            "train loss:0.7717189086386663\n",
            "train loss:0.8875490565440164\n",
            "train loss:0.8273381537568631\n",
            "train loss:0.911004273203521\n",
            "train loss:0.8899457375517211\n",
            "train loss:0.7535241919519516\n",
            "train loss:0.9896809344675742\n",
            "train loss:0.8198445926693794\n",
            "train loss:0.7975255848984996\n",
            "train loss:0.9867712214437786\n",
            "train loss:0.9480858367903877\n",
            "train loss:0.915207290990937\n",
            "train loss:0.8828547554522425\n",
            "train loss:0.6674319157958231\n",
            "train loss:0.9408905575098656\n",
            "train loss:0.8572599566143237\n",
            "train loss:0.8706824190780815\n",
            "train loss:0.763168340009623\n",
            "train loss:0.8798464268628511\n",
            "train loss:0.7246239725397302\n",
            "train loss:0.6942614320815196\n",
            "train loss:0.8518369055416412\n",
            "train loss:0.8678880847139476\n",
            "train loss:0.8240032387811279\n",
            "train loss:0.8103378017879567\n",
            "train loss:0.8289662129693354\n",
            "train loss:0.8777324708385723\n",
            "train loss:0.8444020440468565\n",
            "train loss:0.6398439524193085\n",
            "train loss:0.9647935843483423\n",
            "train loss:0.9515434968089133\n",
            "train loss:0.7684171154116509\n",
            "train loss:0.8589904284943053\n",
            "train loss:0.731958578381127\n",
            "train loss:0.7702563753961463\n",
            "train loss:0.9468743625651469\n",
            "train loss:0.8105101231024548\n",
            "train loss:0.7816945493552545\n",
            "train loss:0.8751038248547891\n",
            "train loss:0.8719828598655407\n",
            "train loss:0.7791997532207959\n",
            "train loss:0.7612987151079585\n",
            "train loss:0.8289829344139047\n",
            "train loss:0.8515738654619117\n",
            "train loss:0.8917479350514965\n",
            "train loss:0.7862877607988097\n",
            "train loss:1.0365815042721898\n",
            "train loss:0.7846924847543321\n",
            "train loss:0.8121832254267981\n",
            "train loss:0.8615522638973853\n",
            "train loss:0.727654331795243\n",
            "train loss:0.6608484126786877\n",
            "train loss:0.8627914797842803\n",
            "train loss:0.8636497334307269\n",
            "train loss:1.0562927808539542\n",
            "train loss:0.8809096837216209\n",
            "train loss:0.6417389406330054\n",
            "train loss:0.7031071571298537\n",
            "train loss:0.9047093784734136\n",
            "train loss:0.8096590491880871\n",
            "train loss:0.7788973385760598\n",
            "train loss:0.8848426247729048\n",
            "train loss:1.0058333080330346\n",
            "train loss:0.9367287885921288\n",
            "train loss:0.8550073269892419\n",
            "train loss:0.7345728958760226\n",
            "train loss:0.7940820460248154\n",
            "train loss:0.7574375970013598\n",
            "train loss:0.9843756476113015\n",
            "train loss:0.7226633480889383\n",
            "train loss:0.8101036212127016\n",
            "train loss:0.9774110313164868\n",
            "train loss:0.8652294691268146\n",
            "train loss:0.8007176325291323\n",
            "train loss:0.9607449857936652\n",
            "train loss:0.8967736650746109\n",
            "train loss:0.8546771192408756\n",
            "train loss:0.8292075549438036\n",
            "train loss:0.8585778462715856\n",
            "train loss:0.8400876802272146\n",
            "train loss:0.970351210764332\n",
            "train loss:0.742024323490692\n",
            "train loss:0.8243660079578183\n",
            "train loss:0.8182450564341817\n",
            "train loss:0.89519382198464\n",
            "train loss:0.9068072909354551\n",
            "train loss:0.8956076667876414\n",
            "train loss:0.8518329804442872\n",
            "train loss:0.9171515828090122\n",
            "train loss:0.8861741827966418\n",
            "train loss:1.0558792552487546\n",
            "train loss:0.7488849640962586\n",
            "train loss:0.880543205118707\n",
            "train loss:0.8109732552421763\n",
            "train loss:0.7012194282517927\n",
            "train loss:0.8871750981992448\n",
            "train loss:0.8091355408248265\n",
            "train loss:0.9607562481878066\n",
            "train loss:0.8258034044657352\n",
            "train loss:0.8854743483392409\n",
            "train loss:0.7126992036079051\n",
            "train loss:0.9242943940755229\n",
            "train loss:0.8032301278405418\n",
            "train loss:0.808077866091551\n",
            "train loss:0.98494447120697\n",
            "train loss:0.8795855767551436\n",
            "train loss:0.9172648550885825\n",
            "train loss:0.9565779170461258\n",
            "train loss:0.9108273897633725\n",
            "train loss:0.9006181595566413\n",
            "train loss:0.8937805832038166\n",
            "train loss:0.7736777774914263\n",
            "train loss:0.6722593622600341\n",
            "train loss:0.9337006547878051\n",
            "train loss:0.8936509602302739\n",
            "train loss:0.9325641935027958\n",
            "train loss:0.8883787659598781\n",
            "train loss:0.92655754004896\n",
            "train loss:0.8030503041048526\n",
            "train loss:0.7753478335825419\n",
            "train loss:0.9046114334583286\n",
            "train loss:0.7357096524816896\n",
            "train loss:0.9653567342020013\n",
            "train loss:1.0682946031294498\n",
            "train loss:0.8112712881695004\n",
            "train loss:0.8518031370480008\n",
            "train loss:0.7713243622015036\n",
            "train loss:1.0202136160471036\n",
            "train loss:0.6825202719800169\n",
            "train loss:0.6904447892555078\n",
            "train loss:0.9082937618816147\n",
            "train loss:0.963932930639321\n",
            "train loss:0.8664146183880329\n",
            "train loss:0.9339227916875399\n",
            "train loss:0.8104634331364688\n",
            "train loss:0.8294585752329983\n",
            "train loss:0.9477630810358957\n",
            "train loss:1.0064456239966015\n",
            "train loss:0.8586667031103186\n",
            "train loss:0.8557147158116808\n",
            "train loss:0.819274113568912\n",
            "train loss:0.6940668760492795\n",
            "train loss:0.8985210757812975\n",
            "train loss:1.0405625344308256\n",
            "train loss:0.9212389529559996\n",
            "train loss:1.005953779661143\n",
            "train loss:0.8313440948501822\n",
            "train loss:0.870965369361329\n",
            "train loss:0.8928889888720682\n",
            "train loss:0.7542834540822786\n",
            "train loss:0.7224732021321535\n",
            "train loss:0.8906995612224242\n",
            "train loss:0.8265847304006257\n",
            "train loss:0.7600513335223268\n",
            "train loss:1.0748262451823902\n",
            "train loss:0.7847111305599855\n",
            "train loss:0.8484630922623385\n",
            "train loss:0.8063644817615696\n",
            "train loss:0.9300578803616698\n",
            "train loss:0.8604745978675404\n",
            "train loss:1.018688841452216\n",
            "train loss:0.9801544195463368\n",
            "train loss:0.9269546306288948\n",
            "train loss:0.8322011443872777\n",
            "train loss:0.90480349150365\n",
            "train loss:0.8899537513574326\n",
            "train loss:0.8128393455282324\n",
            "train loss:0.8400647105471328\n",
            "train loss:0.8265084414502811\n",
            "train loss:0.8893137085974006\n",
            "train loss:0.8955433933595187\n",
            "train loss:0.8606384613141609\n",
            "train loss:0.9243016775233689\n",
            "train loss:1.0771642689134324\n",
            "train loss:0.7919190905328563\n",
            "train loss:0.9672266641860946\n",
            "train loss:0.8356756388915336\n",
            "train loss:0.8426858418911294\n",
            "train loss:0.8374004656465722\n",
            "train loss:0.8574197104233554\n",
            "train loss:0.9251935396558991\n",
            "train loss:0.7619243194959169\n",
            "train loss:0.8838811355272097\n",
            "train loss:0.8371416430205957\n",
            "train loss:0.8590049620658489\n",
            "train loss:0.6875617632386001\n",
            "train loss:0.8977147667774067\n",
            "train loss:0.9372219668391693\n",
            "train loss:0.8305365492118435\n",
            "train loss:1.0275309768269856\n",
            "train loss:0.9532590101367461\n",
            "train loss:0.9249750109357896\n",
            "train loss:0.960483152043656\n",
            "train loss:0.8173190976429558\n",
            "train loss:0.8875074213267756\n",
            "train loss:0.8793103479835925\n",
            "train loss:0.9938023565295029\n",
            "train loss:0.8610831082886824\n",
            "train loss:0.7962706873831686\n",
            "train loss:1.0853533266488034\n",
            "train loss:0.8426566324546664\n",
            "train loss:0.8416015438089188\n",
            "train loss:0.9037349276938441\n",
            "train loss:0.7894144833810285\n",
            "train loss:0.8383678379983566\n",
            "train loss:0.7602376989392248\n",
            "train loss:0.884598911131236\n",
            "train loss:0.9025259591536541\n",
            "train loss:0.6557117057509391\n",
            "train loss:0.8567245399471538\n",
            "train loss:0.7204460054581383\n",
            "train loss:0.9228976054684475\n",
            "train loss:0.8132406287998113\n",
            "train loss:1.076089381339215\n",
            "train loss:0.8941467734228083\n",
            "train loss:0.9041149485168996\n",
            "train loss:0.9051480902285743\n",
            "train loss:0.8139073550864359\n",
            "train loss:0.8659942199749027\n",
            "train loss:0.796965368638545\n",
            "train loss:0.9544426232176082\n",
            "train loss:0.8422942726427425\n",
            "train loss:0.7646250169848837\n",
            "train loss:0.8278582376528566\n",
            "train loss:0.9043135386391961\n",
            "train loss:0.9163122826236525\n",
            "train loss:0.7690716927833867\n",
            "train loss:0.9454304063533411\n",
            "train loss:0.8652218767702844\n",
            "train loss:0.8465575659291632\n",
            "train loss:1.0153153633314478\n",
            "train loss:0.7514198156849783\n",
            "train loss:0.8582739835528101\n",
            "train loss:0.8646293525014568\n",
            "train loss:0.7221940519003363\n",
            "train loss:0.9824168707061746\n",
            "train loss:0.7406187746938123\n",
            "train loss:0.8589211688638185\n",
            "train loss:0.8018823572152024\n",
            "train loss:0.7939896160919656\n",
            "train loss:0.9823791015320541\n",
            "train loss:0.8570539955789532\n",
            "train loss:0.9649352450132901\n",
            "train loss:0.8827502244232529\n",
            "train loss:0.7980100461936845\n",
            "train loss:0.9689120965072187\n",
            "train loss:0.8365472667961498\n",
            "train loss:0.8891911577851814\n",
            "train loss:0.8898565141755443\n",
            "train loss:0.9423101709756985\n",
            "train loss:0.9104039765813313\n",
            "train loss:0.9608641766254508\n",
            "train loss:0.6632248963294793\n",
            "train loss:0.8811961704081375\n",
            "train loss:0.8956653671841345\n",
            "train loss:0.7916583271938299\n",
            "train loss:0.9744413234140424\n",
            "train loss:0.8866140212035755\n",
            "train loss:0.9431021573818583\n",
            "train loss:0.827321510596527\n",
            "train loss:0.8425948236533884\n",
            "train loss:0.8734395103876018\n",
            "train loss:0.9162181778202406\n",
            "train loss:0.8834242821811972\n",
            "train loss:0.8890375547156519\n",
            "train loss:0.7139345679057183\n",
            "train loss:1.0453615197731247\n",
            "train loss:0.891011012820949\n",
            "train loss:0.8018788434802443\n",
            "train loss:1.0194566588446898\n",
            "train loss:0.9150719424444115\n",
            "train loss:0.7794386089100463\n",
            "train loss:0.835150541549293\n",
            "train loss:0.8616636201015149\n",
            "train loss:1.0197033280899008\n",
            "train loss:1.0451211503106692\n",
            "train loss:0.8238087932943217\n",
            "train loss:0.9651022243573965\n",
            "train loss:0.8523593761023693\n",
            "train loss:0.9901530306587695\n",
            "train loss:0.7561190461600509\n",
            "train loss:0.8278609142485375\n",
            "train loss:0.74792632146984\n",
            "train loss:0.818165521514763\n",
            "train loss:0.848241927367851\n",
            "train loss:0.8554281985053426\n",
            "train loss:1.0719925330053217\n",
            "train loss:1.046242819870411\n",
            "train loss:0.8057104520457611\n",
            "train loss:1.1239670861022457\n",
            "train loss:0.8677074457752033\n",
            "train loss:0.9835541607807358\n",
            "train loss:0.8196680853294289\n",
            "train loss:0.8931375207885967\n",
            "train loss:0.7873565632725726\n",
            "train loss:0.9384471350179756\n",
            "train loss:0.8023888808721185\n",
            "train loss:0.8181496988527708\n",
            "train loss:0.9907587737489453\n",
            "train loss:0.8118076527308493\n",
            "train loss:0.7540724308701379\n",
            "train loss:0.9776505693265647\n",
            "train loss:0.8773520864914002\n",
            "train loss:0.7558134586994462\n",
            "train loss:0.7948478612754255\n",
            "train loss:0.9260831088214643\n",
            "train loss:0.7707295341452581\n",
            "train loss:0.9705819749207119\n",
            "train loss:0.7270473095459432\n",
            "train loss:1.0002791394253792\n",
            "train loss:0.976328831633372\n",
            "train loss:0.9763584294551942\n",
            "train loss:0.8880865430822549\n",
            "train loss:0.8115039382570396\n",
            "train loss:0.9179265716526787\n",
            "train loss:0.8264599711617872\n",
            "train loss:0.8657533499709966\n",
            "train loss:0.9115010713156204\n",
            "train loss:0.8055449247128011\n",
            "train loss:1.0487623959538523\n",
            "train loss:0.8276774903356816\n",
            "train loss:0.8648309068606238\n",
            "train loss:0.9187533657338187\n",
            "train loss:0.9087339365115613\n",
            "train loss:0.8840285379758269\n",
            "train loss:0.6655481289419507\n",
            "train loss:0.8812837330530071\n",
            "train loss:0.8770131436659852\n",
            "train loss:0.9398589154484133\n",
            "train loss:0.8488933280096311\n",
            "train loss:0.7626053501670477\n",
            "train loss:0.8511791690802818\n",
            "train loss:0.9569654600873427\n",
            "train loss:0.8744481565203575\n",
            "train loss:0.8239078870374318\n",
            "train loss:0.8389453062301377\n",
            "train loss:0.8297220457387958\n",
            "train loss:0.7945622458456052\n",
            "train loss:0.8678882160222863\n",
            "train loss:0.8889162946138573\n",
            "train loss:0.7900578022549228\n",
            "train loss:0.8338117514281103\n",
            "train loss:0.7539253795125417\n",
            "train loss:0.9281407174391875\n",
            "train loss:0.6319140189585886\n",
            "train loss:0.9392442374735605\n",
            "train loss:0.8641993172960185\n",
            "train loss:0.8445022107747784\n",
            "train loss:0.8364328306418772\n",
            "train loss:0.8068699644940857\n",
            "train loss:0.9378685751202234\n",
            "train loss:0.7201842136298915\n",
            "train loss:0.8896920485957045\n",
            "train loss:0.9082463284519399\n",
            "train loss:0.8778237336918601\n",
            "train loss:0.7572157850541541\n",
            "train loss:0.7291520967051938\n",
            "train loss:0.8960808677273258\n",
            "train loss:0.856564542852769\n",
            "train loss:0.898897177780276\n",
            "train loss:0.9412594325816269\n",
            "train loss:0.7589862955223253\n",
            "train loss:1.010055783645801\n",
            "train loss:0.8415754935991712\n",
            "train loss:0.8906442917693014\n",
            "train loss:0.6551143931175407\n",
            "train loss:0.7206576851608432\n",
            "train loss:0.9495205991865698\n",
            "train loss:0.8761065805737027\n",
            "train loss:0.87966242603391\n",
            "train loss:0.8562156370797109\n",
            "train loss:0.8759508555623797\n",
            "train loss:0.8523947161360422\n",
            "train loss:0.8544195070605837\n",
            "train loss:0.7857133647349602\n",
            "train loss:0.7208180326084829\n",
            "train loss:0.8988085864283574\n",
            "train loss:0.7882235656224391\n",
            "train loss:0.8820461596222819\n",
            "train loss:0.9581646180897266\n",
            "train loss:0.8997371712335905\n",
            "train loss:0.9653170461842793\n",
            "train loss:0.8902733375922058\n",
            "train loss:1.0473407229896723\n",
            "train loss:0.884576406002773\n",
            "train loss:0.973408912828716\n",
            "train loss:0.8215475734593819\n",
            "train loss:0.8160627421638992\n",
            "train loss:0.9574926870910594\n",
            "train loss:0.821232366668214\n",
            "train loss:0.6711051804976134\n",
            "train loss:0.7505266254002193\n",
            "train loss:0.9449252293656163\n",
            "train loss:0.8744705012908763\n",
            "train loss:0.9426207779561062\n",
            "train loss:0.8329823348899091\n",
            "train loss:0.8580565582176957\n",
            "train loss:0.8330204804034664\n",
            "train loss:0.834356398528275\n",
            "train loss:0.773393592291191\n",
            "train loss:1.0670095940803543\n",
            "train loss:0.962328063556276\n",
            "train loss:0.8453856996267258\n",
            "train loss:0.9363578274646476\n",
            "train loss:0.9666173048680693\n",
            "train loss:0.8584021744443578\n",
            "train loss:0.7042824034535436\n",
            "train loss:0.8810272428432113\n",
            "train loss:0.8992162459575982\n",
            "train loss:1.0328742091746248\n",
            "train loss:0.9351515776605656\n",
            "train loss:0.8843239831539775\n",
            "train loss:1.0094769893454794\n",
            "train loss:0.8250209410360153\n",
            "train loss:0.7629274257676584\n",
            "train loss:0.8282123874666147\n",
            "train loss:0.8379836146815367\n",
            "train loss:0.9737534082236974\n",
            "train loss:0.8297190768058833\n",
            "train loss:0.92502219519735\n",
            "train loss:0.8945221713197741\n",
            "train loss:0.8514934956795257\n",
            "train loss:1.0581059639587254\n",
            "train loss:0.6773438828838023\n",
            "=== epoch:17, train acc:0.997, test acc:0.992 ===\n",
            "train loss:0.9502836065097182\n",
            "train loss:0.8154723930674254\n",
            "train loss:0.7797833727785293\n",
            "train loss:0.7774778621834909\n",
            "train loss:0.6290487087337063\n",
            "train loss:0.8196894428490951\n",
            "train loss:0.7648354241174329\n",
            "train loss:0.8358585215368229\n",
            "train loss:0.8599478879580937\n",
            "train loss:0.9173037922153369\n",
            "train loss:0.8360758025443217\n",
            "train loss:0.956625137502236\n",
            "train loss:0.892967876142696\n",
            "train loss:1.0023732008843587\n",
            "train loss:0.8321037453914876\n",
            "train loss:0.773645202771359\n",
            "train loss:0.7775065369219941\n",
            "train loss:0.9237544463997435\n",
            "train loss:0.7496414771613844\n",
            "train loss:0.9608188719979823\n",
            "train loss:1.0117853075463519\n",
            "train loss:0.7336637516285198\n",
            "train loss:0.7962465969970861\n",
            "train loss:0.9837537132350808\n",
            "train loss:0.7221872742270585\n",
            "train loss:0.7729757439003916\n",
            "train loss:0.731362805088428\n",
            "train loss:0.7058060968300477\n",
            "train loss:0.8580517222005167\n",
            "train loss:0.8597596995565925\n",
            "train loss:0.79003973404491\n",
            "train loss:0.820758378173455\n",
            "train loss:0.9142345694522396\n",
            "train loss:0.8607730951787542\n",
            "train loss:0.8452167238742567\n",
            "train loss:0.8517488433908157\n",
            "train loss:0.9147735023149905\n",
            "train loss:0.7699526749050921\n",
            "train loss:0.844102530198909\n",
            "train loss:0.8915335848053643\n",
            "train loss:0.7988907954641278\n",
            "train loss:0.8154733475974371\n",
            "train loss:0.7963587885512936\n",
            "train loss:0.808947802384387\n",
            "train loss:0.9801615624419584\n",
            "train loss:0.7411527106103613\n",
            "train loss:0.9533444500874715\n",
            "train loss:0.7871832574269821\n",
            "train loss:0.8514971867204096\n",
            "train loss:0.8450623744129123\n",
            "train loss:0.7877850603009345\n",
            "train loss:0.8977616240061559\n",
            "train loss:0.9315127917981195\n",
            "train loss:0.9020340563298592\n",
            "train loss:0.8767114528263437\n",
            "train loss:0.9089375814482245\n",
            "train loss:1.0915861480942646\n",
            "train loss:0.7927278829862587\n",
            "train loss:0.8146376974379783\n",
            "train loss:0.9991468611009077\n",
            "train loss:0.7535839005211319\n",
            "train loss:0.8706653876046084\n",
            "train loss:0.9188692781800195\n",
            "train loss:0.7813327705318578\n",
            "train loss:0.8019111922643423\n",
            "train loss:0.8151458092288507\n",
            "train loss:1.0886862882119717\n",
            "train loss:0.9570308861643652\n",
            "train loss:0.7113232727000232\n",
            "train loss:1.0060346376707985\n",
            "train loss:0.8584220790386516\n",
            "train loss:0.9044885301534665\n",
            "train loss:0.7802124742652151\n",
            "train loss:0.9988605297571793\n",
            "train loss:0.9266835863296141\n",
            "train loss:0.6525279735143702\n",
            "train loss:0.9116471659725973\n",
            "train loss:0.8167665117454598\n",
            "train loss:0.8464028880006478\n",
            "train loss:0.9766659848171\n",
            "train loss:0.8192657989495359\n",
            "train loss:0.6853378396817987\n",
            "train loss:0.848614057637565\n",
            "train loss:0.9202270212144131\n",
            "train loss:1.099139411599761\n",
            "train loss:0.7548844662688928\n",
            "train loss:1.103397436078993\n",
            "train loss:0.9710860781787477\n",
            "train loss:0.973586408838947\n",
            "train loss:0.8508344322279062\n",
            "train loss:0.9450684317132452\n",
            "train loss:0.8580857580063467\n",
            "train loss:0.8497386071761632\n",
            "train loss:0.8560821171303685\n",
            "train loss:0.7255790889351514\n",
            "train loss:0.7202239642204532\n",
            "train loss:0.7612684378692889\n",
            "train loss:1.0473590727347728\n",
            "train loss:0.8223505372187627\n",
            "train loss:0.9803094816716303\n",
            "train loss:0.803285810576783\n",
            "train loss:0.9174492558295628\n",
            "train loss:0.6831648574735404\n",
            "train loss:0.9543022896882283\n",
            "train loss:0.9463670065106164\n",
            "train loss:0.7384380949558086\n",
            "train loss:0.9907413484092851\n",
            "train loss:0.8686813745505944\n",
            "train loss:0.9584914501977764\n",
            "train loss:0.9344990850906992\n",
            "train loss:0.9877574633387083\n",
            "train loss:1.0353126375868063\n",
            "train loss:0.8494144809882094\n",
            "train loss:0.7963373685842654\n",
            "train loss:0.8330567128751621\n",
            "train loss:0.957271570178417\n",
            "train loss:0.9801267298271175\n",
            "train loss:0.7832999070152756\n",
            "train loss:0.8828865592158792\n",
            "train loss:0.8092563293884936\n",
            "train loss:0.8688575408951946\n",
            "train loss:0.7742784476446303\n",
            "train loss:0.9541429878015469\n",
            "train loss:1.0166958925001495\n",
            "train loss:1.004148674076862\n",
            "train loss:0.8214932668214279\n",
            "train loss:1.0265231353213207\n",
            "train loss:0.85155698893074\n",
            "train loss:0.911508448696052\n",
            "train loss:0.7793654490128858\n",
            "train loss:0.7847981307651071\n",
            "train loss:0.9216611487231842\n",
            "train loss:0.7728649015261077\n",
            "train loss:0.7589372007955287\n",
            "train loss:0.7896724467641593\n",
            "train loss:0.8709973263799795\n",
            "train loss:0.9102057879974899\n",
            "train loss:0.9050714791891457\n",
            "train loss:1.0487910071369773\n",
            "train loss:0.929229492531522\n",
            "train loss:0.8075708608391049\n",
            "train loss:0.857168774510008\n",
            "train loss:0.8307830143205446\n",
            "train loss:0.7877785426203109\n",
            "train loss:0.8026464547129128\n",
            "train loss:0.8841765754706122\n",
            "train loss:0.9110073179208266\n",
            "train loss:0.7089706849801215\n",
            "train loss:0.9207765389201866\n",
            "train loss:0.9366754802591787\n",
            "train loss:0.8678776607488469\n",
            "train loss:0.8119090715586792\n",
            "train loss:1.126305473527565\n",
            "train loss:0.7886036556003194\n",
            "train loss:0.8178736480319593\n",
            "train loss:0.7490623319831583\n",
            "train loss:0.9898771600325411\n",
            "train loss:0.868858108638034\n",
            "train loss:0.8060329618017572\n",
            "train loss:0.96059560843307\n",
            "train loss:0.8043080464186864\n",
            "train loss:0.9447094315272079\n",
            "train loss:0.9161267792529876\n",
            "train loss:0.7005763350905134\n",
            "train loss:0.8210313955203388\n",
            "train loss:0.7728811625022395\n",
            "train loss:0.8863968746114049\n",
            "train loss:0.9275782244438163\n",
            "train loss:0.8994852678786386\n",
            "train loss:0.9096447454796209\n",
            "train loss:0.7969343219919591\n",
            "train loss:0.8676471734289709\n",
            "train loss:0.9656072893009592\n",
            "train loss:0.7089994832684795\n",
            "train loss:0.9166490362846137\n",
            "train loss:0.8225405792710394\n",
            "train loss:0.8826711715926112\n",
            "train loss:0.7292823048598928\n",
            "train loss:0.8692373109242005\n",
            "train loss:0.8602511340943493\n",
            "train loss:0.9425394624525043\n",
            "train loss:0.6986129458822588\n",
            "train loss:0.8271469722839285\n",
            "train loss:0.7825458259459005\n",
            "train loss:0.8003432570177437\n",
            "train loss:0.8472786937628338\n",
            "train loss:0.9130409459248285\n",
            "train loss:0.919330641636252\n",
            "train loss:0.88438045467877\n",
            "train loss:0.8394775570631794\n",
            "train loss:0.9185653976415015\n",
            "train loss:0.8029556065226466\n",
            "train loss:0.8237701950913139\n",
            "train loss:0.8585931382055795\n",
            "train loss:0.7498559489084914\n",
            "train loss:0.8272960223712923\n",
            "train loss:0.8149131626663727\n",
            "train loss:0.9848497852873629\n",
            "train loss:0.781600134018142\n",
            "train loss:0.8834705185778788\n",
            "train loss:0.9682151440859776\n",
            "train loss:0.8557215620045782\n",
            "train loss:0.9488170536202246\n",
            "train loss:0.9654052639306145\n",
            "train loss:0.7490689155272267\n",
            "train loss:0.8880494238435258\n",
            "train loss:0.8490661603989413\n",
            "train loss:0.7713187325952728\n",
            "train loss:0.8214251755542485\n",
            "train loss:0.8966154869627039\n",
            "train loss:0.673296594691938\n",
            "train loss:0.712706926096299\n",
            "train loss:0.8903631739452028\n",
            "train loss:0.7941691031780951\n",
            "train loss:0.7729328216400364\n",
            "train loss:0.7876693246513782\n",
            "train loss:0.9519425716739564\n",
            "train loss:0.9004532036718556\n",
            "train loss:0.9267154157910419\n",
            "train loss:0.7365043874289993\n",
            "train loss:0.809204418773238\n",
            "train loss:0.7673325696763613\n",
            "train loss:0.913322208260223\n",
            "train loss:0.774699074941647\n",
            "train loss:0.9862785933054197\n",
            "train loss:0.6848262044043385\n",
            "train loss:0.7912805194883162\n",
            "train loss:0.9191512215563951\n",
            "train loss:0.6884163847448436\n",
            "train loss:0.8476489678846781\n",
            "train loss:0.7941023551184291\n",
            "train loss:1.0488233608392372\n",
            "train loss:1.0458551524152042\n",
            "train loss:0.8515620779955312\n",
            "train loss:0.8991808310779258\n",
            "train loss:1.0347038570804497\n",
            "train loss:0.7907790448808791\n",
            "train loss:0.8013320006906882\n",
            "train loss:0.7704147856204107\n",
            "train loss:0.8487875855899657\n",
            "train loss:0.7742434801166859\n",
            "train loss:0.7666388262550707\n",
            "train loss:0.8816230238969528\n",
            "train loss:1.048794434650919\n",
            "train loss:0.8117951182010376\n",
            "train loss:0.9729707876918161\n",
            "train loss:0.9848521176674078\n",
            "train loss:0.9228517359168299\n",
            "train loss:1.0093145542045736\n",
            "train loss:0.9429015651353063\n",
            "train loss:0.9444567492460787\n",
            "train loss:0.7650205080919495\n",
            "train loss:0.8939958656272659\n",
            "train loss:0.7925561563732336\n",
            "train loss:0.677898907901308\n",
            "train loss:0.9002234006104074\n",
            "train loss:0.7640095646234123\n",
            "train loss:0.8567270184838476\n",
            "train loss:0.899564098110894\n",
            "train loss:0.8632342506234016\n",
            "train loss:0.9133037947434505\n",
            "train loss:0.8916328876588476\n",
            "train loss:0.7640231873933817\n",
            "train loss:0.7829082781635013\n",
            "train loss:0.8753814788457132\n",
            "train loss:0.7910957430254671\n",
            "train loss:0.769860867992254\n",
            "train loss:0.9433713327887482\n",
            "train loss:0.8808780419105033\n",
            "train loss:0.8997173281607505\n",
            "train loss:1.0406978037323593\n",
            "train loss:0.7926594646867103\n",
            "train loss:0.8643252488455658\n",
            "train loss:0.875267590997706\n",
            "train loss:0.7972216686846306\n",
            "train loss:0.9353210007401884\n",
            "train loss:0.9177217832995476\n",
            "train loss:0.8545304737119187\n",
            "train loss:0.8622044127426349\n",
            "train loss:0.920282505233598\n",
            "train loss:0.7592183472299016\n",
            "train loss:0.6873545363513236\n",
            "train loss:0.9691437213020565\n",
            "train loss:0.7421228902915494\n",
            "train loss:0.8580271119405858\n",
            "train loss:0.8718927941829661\n",
            "train loss:0.8946256297720155\n",
            "train loss:0.8298298027760631\n",
            "train loss:0.8622061897123924\n",
            "train loss:0.745110007505645\n",
            "train loss:0.8883593428584795\n",
            "train loss:0.7391068920243269\n",
            "train loss:0.8880071354332142\n",
            "train loss:0.7640406413479173\n",
            "train loss:0.9270326543315435\n",
            "train loss:0.9684935478300362\n",
            "train loss:0.9660600301619648\n",
            "train loss:0.8880385235733652\n",
            "train loss:0.9883965215175375\n",
            "train loss:0.8621974090930103\n",
            "train loss:0.8176973969077982\n",
            "train loss:0.7751481984295436\n",
            "train loss:0.9900843200499263\n",
            "train loss:0.9765055904607945\n",
            "train loss:0.9471705801728291\n",
            "train loss:0.767666470215513\n",
            "train loss:0.8799791487629766\n",
            "train loss:0.9944766418140225\n",
            "train loss:0.8799365117280018\n",
            "train loss:0.9528969536495197\n",
            "train loss:0.8800249184132264\n",
            "train loss:0.7573343856702495\n",
            "train loss:0.9215159797610387\n",
            "train loss:0.8080185383309905\n",
            "train loss:0.7720732495652021\n",
            "train loss:0.8720773877055416\n",
            "train loss:0.6693221088270507\n",
            "train loss:0.8610213563702814\n",
            "train loss:0.81117583973597\n",
            "train loss:0.8926041863993057\n",
            "train loss:0.8423053168731708\n",
            "train loss:0.8704213711761983\n",
            "train loss:0.853220182789385\n",
            "train loss:0.8411692241115509\n",
            "train loss:0.7051451856201244\n",
            "train loss:0.8885969933250719\n",
            "train loss:0.8511345391148069\n",
            "train loss:0.8847252037135196\n",
            "train loss:0.8954595385633453\n",
            "train loss:0.9361927956006985\n",
            "train loss:0.7346234591573352\n",
            "train loss:0.7951344992418214\n",
            "train loss:0.7735642505050672\n",
            "train loss:0.8384507087664024\n",
            "train loss:0.779514340075142\n",
            "train loss:0.8574011059973856\n",
            "train loss:0.7691519894653684\n",
            "train loss:0.9407187241001332\n",
            "train loss:0.9108587433637275\n",
            "train loss:0.7014434345769044\n",
            "train loss:0.9938737769938979\n",
            "train loss:0.8604409024180469\n",
            "train loss:0.7603974799733416\n",
            "train loss:0.852276074195081\n",
            "train loss:0.9319415626210333\n",
            "train loss:0.7890286115298788\n",
            "train loss:0.8217226284331676\n",
            "train loss:0.8777109439402928\n",
            "train loss:0.8825187132105072\n",
            "train loss:0.9835367067405768\n",
            "train loss:0.836321086820669\n",
            "train loss:0.9462907281901333\n",
            "train loss:0.9922326502135874\n",
            "train loss:0.8248656592431317\n",
            "train loss:0.8250452434171747\n",
            "train loss:0.7926468807213564\n",
            "train loss:0.687173317033755\n",
            "train loss:0.9870759227843077\n",
            "train loss:0.8909508851842913\n",
            "train loss:0.9175217797987879\n",
            "train loss:0.706541256032798\n",
            "train loss:0.9917671363544348\n",
            "train loss:0.7352923094237059\n",
            "train loss:1.0437581703686092\n",
            "train loss:1.0729099716288413\n",
            "train loss:0.7673981842929081\n",
            "train loss:0.879083698836103\n",
            "train loss:0.8362336414632753\n",
            "train loss:0.7170212496477962\n",
            "train loss:0.8643102814598183\n",
            "train loss:0.6945038981576755\n",
            "train loss:0.8565368277784146\n",
            "train loss:0.7632385911994694\n",
            "train loss:1.0129902329739284\n",
            "train loss:0.8093258441360386\n",
            "train loss:0.7880765970900753\n",
            "train loss:0.9119659908242613\n",
            "train loss:0.8421637934926332\n",
            "train loss:0.9246448988412804\n",
            "train loss:0.7742811303129506\n",
            "train loss:0.818998439683142\n",
            "train loss:0.974674029703164\n",
            "train loss:0.836607975944052\n",
            "train loss:0.8017303534741802\n",
            "train loss:0.8942569429547976\n",
            "train loss:0.8601797450487197\n",
            "train loss:0.8511307505104377\n",
            "train loss:0.9418073430364999\n",
            "train loss:0.8554952188591446\n",
            "train loss:0.9584532352675511\n",
            "train loss:0.8545591005738642\n",
            "train loss:0.8679167193694517\n",
            "train loss:0.8281573839490057\n",
            "train loss:0.7684336973471844\n",
            "train loss:0.8591189472434729\n",
            "train loss:0.8340214189754738\n",
            "train loss:0.8616509184246705\n",
            "train loss:0.9197563118689764\n",
            "train loss:0.8483233286878812\n",
            "train loss:0.8565049774190489\n",
            "train loss:0.9850203527600075\n",
            "train loss:0.882520859975216\n",
            "train loss:0.688293637258843\n",
            "train loss:0.8136988393687722\n",
            "train loss:0.8683401912676213\n",
            "train loss:0.8610971793819111\n",
            "train loss:0.9087961664403856\n",
            "train loss:0.8537748190824509\n",
            "train loss:0.7960843488996413\n",
            "train loss:0.7637768461451546\n",
            "train loss:0.7953723532616175\n",
            "train loss:0.7374574739310353\n",
            "train loss:1.1713550906867465\n",
            "train loss:0.8572030632247202\n",
            "train loss:0.839543315998814\n",
            "train loss:0.8009344481766246\n",
            "train loss:0.7372324760514795\n",
            "train loss:1.052377266458274\n",
            "train loss:0.8580886092377017\n",
            "train loss:0.9523005376027105\n",
            "train loss:0.8741675864644922\n",
            "train loss:0.7776907473695289\n",
            "train loss:0.930832418892535\n",
            "train loss:0.9274054221501822\n",
            "train loss:0.9697673014129955\n",
            "train loss:0.8035981918918614\n",
            "train loss:0.8957426054347885\n",
            "train loss:0.9829023785092426\n",
            "train loss:0.9529591511834892\n",
            "train loss:0.9818927498914607\n",
            "train loss:1.1091707597781524\n",
            "train loss:0.9983679874971435\n",
            "train loss:0.7484645918975273\n",
            "train loss:0.8138920375866477\n",
            "train loss:0.8337878193904565\n",
            "train loss:0.7441757375625366\n",
            "train loss:0.8874023246323687\n",
            "train loss:0.8080452144398172\n",
            "train loss:0.78112825905395\n",
            "train loss:0.917356316721454\n",
            "train loss:0.9891156051099921\n",
            "train loss:0.9204439118785964\n",
            "train loss:0.8022973199335537\n",
            "train loss:0.9504371675466797\n",
            "train loss:0.9012403021549399\n",
            "train loss:0.9997277793517473\n",
            "train loss:0.7609587906320913\n",
            "train loss:0.8074847088986047\n",
            "train loss:0.6710567630261867\n",
            "train loss:1.1019542222346823\n",
            "train loss:0.858714288823687\n",
            "train loss:0.8423212706984465\n",
            "train loss:0.863369211733354\n",
            "train loss:0.9570848583250081\n",
            "train loss:0.9104224761273869\n",
            "train loss:0.9287980591337973\n",
            "train loss:0.9608393930745135\n",
            "train loss:0.6862535696265334\n",
            "train loss:0.9180463829311235\n",
            "train loss:0.684096831523824\n",
            "train loss:0.7635327305245754\n",
            "train loss:0.8735343088906433\n",
            "train loss:0.8966444357801882\n",
            "train loss:0.9898634152522461\n",
            "train loss:0.7546040846797533\n",
            "train loss:0.8910114946377574\n",
            "train loss:0.8587382740374666\n",
            "train loss:0.9068929507202321\n",
            "train loss:0.776424928235388\n",
            "train loss:0.7606210583241888\n",
            "train loss:0.7247719142379233\n",
            "train loss:0.8482606702991144\n",
            "train loss:0.8648483181699702\n",
            "train loss:0.633328681763067\n",
            "train loss:0.9977247182851415\n",
            "train loss:0.7787463571456432\n",
            "train loss:0.8365240688793556\n",
            "train loss:0.9081369145105355\n",
            "train loss:0.9116241427749838\n",
            "train loss:0.7670985776985567\n",
            "train loss:0.7751062328164392\n",
            "train loss:0.9239948689020919\n",
            "train loss:0.9849089797049924\n",
            "train loss:0.9221165073115607\n",
            "train loss:0.9396584356904607\n",
            "train loss:0.8676306696565722\n",
            "train loss:1.0526275041074642\n",
            "train loss:0.8521483992272024\n",
            "train loss:0.9259648388418669\n",
            "train loss:0.8170775971505485\n",
            "train loss:0.7522936439039821\n",
            "train loss:0.8958784977209996\n",
            "train loss:0.9652193358659659\n",
            "train loss:0.9087318543748527\n",
            "train loss:0.9190741352929146\n",
            "train loss:0.9235282201582845\n",
            "train loss:0.828901351181435\n",
            "train loss:0.8978477595172465\n",
            "train loss:0.8632229796892821\n",
            "train loss:0.8361952404978551\n",
            "train loss:1.0260343867415882\n",
            "train loss:0.7394744412721785\n",
            "train loss:0.9043545483184361\n",
            "train loss:0.8581439104142641\n",
            "train loss:0.9141655889336349\n",
            "train loss:0.8665803008102481\n",
            "train loss:0.8889217174183982\n",
            "train loss:0.8603189106211208\n",
            "train loss:0.8652471049781505\n",
            "train loss:0.8386691366583103\n",
            "train loss:0.7988281992795101\n",
            "train loss:0.8190739707068047\n",
            "train loss:0.8012906837469166\n",
            "train loss:1.0198839944335534\n",
            "train loss:0.8166258185600693\n",
            "train loss:0.8562645401167626\n",
            "train loss:0.8491022141064843\n",
            "train loss:0.7925307159570497\n",
            "train loss:0.9056217341007847\n",
            "train loss:0.8236404853186258\n",
            "train loss:0.7022475934875523\n",
            "train loss:0.8061633225897892\n",
            "train loss:0.898689457442388\n",
            "train loss:0.9130068423856599\n",
            "train loss:0.7664866067495802\n",
            "train loss:0.9112827608186815\n",
            "train loss:0.9315763865584866\n",
            "train loss:0.8635348221200034\n",
            "train loss:0.892912218411163\n",
            "train loss:0.814491099841461\n",
            "train loss:0.9814168977540824\n",
            "train loss:0.9835119573498736\n",
            "train loss:0.8594207031194243\n",
            "train loss:0.9879555668623966\n",
            "train loss:0.7808167800942069\n",
            "train loss:0.8223873843501102\n",
            "train loss:0.932564156586579\n",
            "train loss:0.7322725110875663\n",
            "train loss:0.944817606736482\n",
            "train loss:0.8553718655037508\n",
            "train loss:0.803926426416328\n",
            "train loss:0.953227824797874\n",
            "train loss:0.8079000002420966\n",
            "train loss:0.9518723258009942\n",
            "train loss:0.7958659522070625\n",
            "train loss:1.0761790554572384\n",
            "train loss:0.8228432845321183\n",
            "train loss:0.9333779526324428\n",
            "train loss:0.8017860169433717\n",
            "train loss:0.7732159057382154\n",
            "train loss:0.9288224589520482\n",
            "train loss:0.8460434484632793\n",
            "train loss:0.6468295457319391\n",
            "train loss:1.0021888704547883\n",
            "train loss:0.7717024971293713\n",
            "train loss:0.7730971034744222\n",
            "train loss:0.8962314930019392\n",
            "train loss:0.7955430083778171\n",
            "train loss:0.9040768585611351\n",
            "train loss:0.7295319123387727\n",
            "train loss:0.9396430207895919\n",
            "train loss:0.9288678458347635\n",
            "train loss:0.7510324069349289\n",
            "train loss:0.7468255693611419\n",
            "train loss:1.0384487889357892\n",
            "train loss:0.7476621089030816\n",
            "train loss:0.9007765764370498\n",
            "train loss:0.9694582098210941\n",
            "train loss:0.7881209345207233\n",
            "train loss:0.6977645496852785\n",
            "train loss:0.7796600413845571\n",
            "train loss:0.7895369774512893\n",
            "train loss:0.830335438359949\n",
            "train loss:0.7376465332228976\n",
            "train loss:0.940402677011431\n",
            "train loss:0.9401963681406315\n",
            "train loss:0.8244958244449054\n",
            "train loss:0.8904015140330168\n",
            "train loss:0.8233552209938773\n",
            "train loss:0.7195830382067774\n",
            "train loss:0.7346101774597187\n",
            "train loss:0.8133978348695844\n",
            "train loss:0.7027241271536081\n",
            "train loss:0.7573699244274711\n",
            "train loss:0.8134987604619941\n",
            "train loss:1.0449285599093303\n",
            "train loss:0.7177338595303548\n",
            "train loss:0.79727205139558\n",
            "train loss:0.9428655128112919\n",
            "train loss:0.870093463138599\n",
            "train loss:0.8929134579595305\n",
            "train loss:0.9620879368803741\n",
            "train loss:0.9456476629588323\n",
            "train loss:0.7783191380170742\n",
            "train loss:0.9657693575631838\n",
            "train loss:0.9565553589784557\n",
            "train loss:0.8177424526037753\n",
            "train loss:0.8203200365610721\n",
            "train loss:0.9328757178748113\n",
            "train loss:0.662492909277376\n",
            "=== epoch:18, train acc:0.997, test acc:0.988 ===\n",
            "train loss:0.8137433832453448\n",
            "train loss:0.9037055154069289\n",
            "train loss:0.7471640783397717\n",
            "train loss:0.8596373440089211\n",
            "train loss:0.8597553256129808\n",
            "train loss:0.7318690834703401\n",
            "train loss:1.0057316717242086\n",
            "train loss:0.9012971880931177\n",
            "train loss:1.0663829020094395\n",
            "train loss:0.9618703892638818\n",
            "train loss:0.824687130704194\n",
            "train loss:0.7556295293086299\n",
            "train loss:0.8236697830688656\n",
            "train loss:0.7286272179144383\n",
            "train loss:0.877259178502024\n",
            "train loss:0.9057917857708788\n",
            "train loss:0.8349969334211003\n",
            "train loss:0.8187998403617159\n",
            "train loss:0.7668377477247246\n",
            "train loss:0.6587755718942894\n",
            "train loss:0.8822077178488695\n",
            "train loss:0.7688812233160092\n",
            "train loss:0.790247331176582\n",
            "train loss:0.7271280649918772\n",
            "train loss:0.8482197074170256\n",
            "train loss:0.8555467243523189\n",
            "train loss:0.8269189140395378\n",
            "train loss:0.7564721170171331\n",
            "train loss:0.8747795157975261\n",
            "train loss:0.9102688226889494\n",
            "train loss:0.772180863508938\n",
            "train loss:0.8889743884862392\n",
            "train loss:0.9459549420032505\n",
            "train loss:0.8880558671295944\n",
            "train loss:1.0216911605828596\n",
            "train loss:0.7846006632168467\n",
            "train loss:0.7906645888243362\n",
            "train loss:0.8048365764873153\n",
            "train loss:0.7601308266703966\n",
            "train loss:0.7390721524088534\n",
            "train loss:0.8223319535500014\n",
            "train loss:1.0298762131616415\n",
            "train loss:0.8976017126915553\n",
            "train loss:0.8496579537147575\n",
            "train loss:0.8455518554882492\n",
            "train loss:0.9073926417836757\n",
            "train loss:0.8092271341462588\n",
            "train loss:0.998192484171244\n",
            "train loss:0.7414427144189832\n",
            "train loss:0.9733908275906836\n",
            "train loss:1.009415017121504\n",
            "train loss:0.8534707657378727\n",
            "train loss:1.0273973424606033\n",
            "train loss:1.0142672671939774\n",
            "train loss:0.7932912521808025\n",
            "train loss:0.9959659398043059\n",
            "train loss:0.9273147957869783\n",
            "train loss:0.8586800166430715\n",
            "train loss:0.7490356966177238\n",
            "train loss:0.8898403198727377\n",
            "train loss:0.977604311425424\n",
            "train loss:0.9503561238788336\n",
            "train loss:0.8409954871498643\n",
            "train loss:0.968071587642977\n",
            "train loss:0.9978855057409709\n",
            "train loss:0.9641866247844822\n",
            "train loss:0.8796406994854168\n",
            "train loss:0.9880255811388085\n",
            "train loss:0.9001434193413453\n",
            "train loss:0.8283123642666674\n",
            "train loss:0.8124898342540982\n",
            "train loss:0.9888430473488298\n",
            "train loss:0.8644726561660409\n",
            "train loss:0.7381845501518366\n",
            "train loss:1.02866949716939\n",
            "train loss:0.8985526152312605\n",
            "train loss:0.8695392639262539\n",
            "train loss:0.8178674119943973\n",
            "train loss:0.8990083356754414\n",
            "train loss:0.8716745262515652\n",
            "train loss:0.7226142769338175\n",
            "train loss:0.7933702000893258\n",
            "train loss:0.9028719152633619\n",
            "train loss:0.8396344785380879\n",
            "train loss:0.9245365643072486\n",
            "train loss:0.94448278857147\n",
            "train loss:0.8607296930052145\n",
            "train loss:0.848937867268127\n",
            "train loss:0.8213700910849017\n",
            "train loss:0.8241651592640511\n",
            "train loss:0.8333872622307846\n",
            "train loss:1.0127370028248925\n",
            "train loss:0.9550640063548105\n",
            "train loss:0.9267698523168914\n",
            "train loss:0.8391198406331817\n",
            "train loss:1.0043831998570267\n",
            "train loss:0.8656196571143069\n",
            "train loss:0.8973696913428504\n",
            "train loss:0.8665192961102934\n",
            "train loss:1.082918406213735\n",
            "train loss:0.920660218245546\n",
            "train loss:1.004469564254853\n",
            "train loss:0.8359972710954637\n",
            "train loss:0.7018924038878027\n",
            "train loss:1.073175063131337\n",
            "train loss:0.7816393968377946\n",
            "train loss:0.9162921161295801\n",
            "train loss:0.8531041059951376\n",
            "train loss:0.951283988323041\n",
            "train loss:0.9389736268289351\n",
            "train loss:0.8364892106380278\n",
            "train loss:0.9122645324911375\n",
            "train loss:0.8547193068705075\n",
            "train loss:0.9050873787182073\n",
            "train loss:0.8695156413473302\n",
            "train loss:1.0382683356634002\n",
            "train loss:0.8683666731078636\n",
            "train loss:0.8068160285400262\n",
            "train loss:0.7909779152073488\n",
            "train loss:0.8994140720542684\n",
            "train loss:0.6679787779100583\n",
            "train loss:0.9654931632647107\n",
            "train loss:0.990278398478399\n",
            "train loss:0.6652400632941302\n",
            "train loss:0.8417493730235764\n",
            "train loss:0.8311212107579949\n",
            "train loss:0.8791288513123173\n",
            "train loss:0.9300561978249188\n",
            "train loss:0.7350779857876762\n",
            "train loss:0.968937625521652\n",
            "train loss:0.7422452657159129\n",
            "train loss:0.9507592138029934\n",
            "train loss:0.8434567853464476\n",
            "train loss:0.7401013594001112\n",
            "train loss:0.7937018527216568\n",
            "train loss:0.963677335788867\n",
            "train loss:0.8657133644438418\n",
            "train loss:1.032745781799759\n",
            "train loss:0.865064257320925\n",
            "train loss:0.8913245679835721\n",
            "train loss:0.8803899405975087\n",
            "train loss:0.8510414585645809\n",
            "train loss:0.7634765193658414\n",
            "train loss:0.9337263578351748\n",
            "train loss:0.8956458633853782\n",
            "train loss:0.8249224577084335\n",
            "train loss:0.759505643570252\n",
            "train loss:0.6920075918949906\n",
            "train loss:1.0104024442311668\n",
            "train loss:0.8720219955188365\n",
            "train loss:0.836128695542465\n",
            "train loss:0.8198262654585511\n",
            "train loss:0.7097688411764155\n",
            "train loss:0.8189157649834913\n",
            "train loss:0.7579074656377215\n",
            "train loss:0.8952969180061778\n",
            "train loss:0.8257543675301865\n",
            "train loss:0.6589001457174714\n",
            "train loss:0.8039362962689429\n",
            "train loss:0.9362722827146925\n",
            "train loss:0.9821192792406676\n",
            "train loss:0.8593166602669907\n",
            "train loss:0.7470089762563293\n",
            "train loss:0.7243942100787379\n",
            "train loss:0.8739495793826434\n",
            "train loss:0.8562341293980754\n",
            "train loss:0.9191073061034919\n",
            "train loss:0.8555482479795073\n",
            "train loss:0.8518526042311898\n",
            "train loss:0.8375037486092965\n",
            "train loss:0.9622448628627369\n",
            "train loss:0.929291804191719\n",
            "train loss:0.828324905736581\n",
            "train loss:0.7749968183002791\n",
            "train loss:0.8213421513951953\n",
            "train loss:0.7097808990771614\n",
            "train loss:0.8958421442250468\n",
            "train loss:0.8527978185359857\n",
            "train loss:0.8570769565318449\n",
            "train loss:0.8113197521884485\n",
            "train loss:0.9051118178763418\n",
            "train loss:0.9305862317217023\n",
            "train loss:0.8376520975379249\n",
            "train loss:0.7617745942688318\n",
            "train loss:0.8499679886228289\n",
            "train loss:0.9000382383593959\n",
            "train loss:0.8327628241652153\n",
            "train loss:0.7975976363796883\n",
            "train loss:0.8377003087838031\n",
            "train loss:0.8012871465204127\n",
            "train loss:0.9033410046999135\n",
            "train loss:0.9356730430135951\n",
            "train loss:0.7797635270116232\n",
            "train loss:0.8753749873235678\n",
            "train loss:0.7726507865806427\n",
            "train loss:0.8911307580584503\n",
            "train loss:1.006417010885428\n",
            "train loss:0.8340559376804147\n",
            "train loss:0.688042208125639\n",
            "train loss:0.6807610896532134\n",
            "train loss:0.8282729787126157\n",
            "train loss:0.820043787573193\n",
            "train loss:0.9598653998394034\n",
            "train loss:0.9821639507156092\n",
            "train loss:0.7217712186496023\n",
            "train loss:0.9420514235916696\n",
            "train loss:0.832142068788063\n",
            "train loss:0.8676341385038318\n",
            "train loss:0.9384575288978577\n",
            "train loss:0.885551012648977\n",
            "train loss:0.8639887222260362\n",
            "train loss:0.8500713398490202\n",
            "train loss:1.0195619135709266\n",
            "train loss:0.7137543220272365\n",
            "train loss:0.7650233721907044\n",
            "train loss:0.9106872998580985\n",
            "train loss:0.7097549067177001\n",
            "train loss:0.7660982433077117\n",
            "train loss:0.9424077681863479\n",
            "train loss:0.9380470923269384\n",
            "train loss:0.7008579354711185\n",
            "train loss:0.73433147653662\n",
            "train loss:0.9229178123139568\n",
            "train loss:0.9644627119045666\n",
            "train loss:0.6514409681774344\n",
            "train loss:0.9083493159959594\n",
            "train loss:0.7440238457716505\n",
            "train loss:0.8828457536954458\n",
            "train loss:0.8913145953020971\n",
            "train loss:0.9750133057433285\n",
            "train loss:0.7846045571887956\n",
            "train loss:0.6533083074366449\n",
            "train loss:0.9584396333799832\n",
            "train loss:0.8638144055420719\n",
            "train loss:0.9399567916735382\n",
            "train loss:0.9102080913204921\n",
            "train loss:0.8818688864911216\n",
            "train loss:0.8384148653315768\n",
            "train loss:0.8602499420924302\n",
            "train loss:0.7938720581418139\n",
            "train loss:0.8824969897120631\n",
            "train loss:0.8201992155631558\n",
            "train loss:0.8583251938650585\n",
            "train loss:0.8003277577687471\n",
            "train loss:0.9223390652991781\n",
            "train loss:0.8093142797568049\n",
            "train loss:0.8438194966316059\n",
            "train loss:0.7032549370022434\n",
            "train loss:0.9491875875418898\n",
            "train loss:0.8382738199835345\n",
            "train loss:0.8072076301976422\n",
            "train loss:0.983283727492602\n",
            "train loss:0.779465660769157\n",
            "train loss:0.8225254798248547\n",
            "train loss:0.793939182226119\n",
            "train loss:0.7643578665326339\n",
            "train loss:0.8388488060161695\n",
            "train loss:0.9184696000116704\n",
            "train loss:0.9606023394762393\n",
            "train loss:0.8674139776602635\n",
            "train loss:0.7494855801633407\n",
            "train loss:0.7973811063131963\n",
            "train loss:0.8362702972037022\n",
            "train loss:0.7872310079696735\n",
            "train loss:0.9246608463815279\n",
            "train loss:0.84731786478633\n",
            "train loss:0.8388784872443563\n",
            "train loss:1.0495371276695202\n",
            "train loss:0.8639481646298363\n",
            "train loss:1.0824936755253147\n",
            "train loss:0.8264677692236837\n",
            "train loss:0.8336659219263537\n",
            "train loss:0.7566877458528112\n",
            "train loss:0.8872492520763784\n",
            "train loss:0.8749260479320651\n",
            "train loss:0.822502551805337\n",
            "train loss:0.9022195367259802\n",
            "train loss:1.0289104529690274\n",
            "train loss:0.9563497089926319\n",
            "train loss:0.9418626966528736\n",
            "train loss:0.7846477822503036\n",
            "train loss:0.9032692578956102\n",
            "train loss:0.7962039792944593\n",
            "train loss:0.8556300571682027\n",
            "train loss:0.7802645762559216\n",
            "train loss:0.803062601228239\n",
            "train loss:0.909346132811693\n",
            "train loss:0.97744418357774\n",
            "train loss:0.7392679205829128\n",
            "train loss:0.9344188073848011\n",
            "train loss:1.0527769182347022\n",
            "train loss:1.0602087286623572\n",
            "train loss:0.8905133003230518\n",
            "train loss:0.8455860720018387\n",
            "train loss:0.6003112883809311\n",
            "train loss:0.7665411828224847\n",
            "train loss:1.0036078295390656\n",
            "train loss:0.9137672816023276\n",
            "train loss:0.8097016772473554\n",
            "train loss:0.7322888374127351\n",
            "train loss:0.7838654072421484\n",
            "train loss:0.9178271371924706\n",
            "train loss:0.8992052923900161\n",
            "train loss:0.8514045882382832\n",
            "train loss:0.9332928563416631\n",
            "train loss:0.8451789431162006\n",
            "train loss:0.8729722604168028\n",
            "train loss:0.8046484192954622\n",
            "train loss:0.832149562891968\n",
            "train loss:0.7374561332932963\n",
            "train loss:0.7712458499394151\n",
            "train loss:0.8105029602722724\n",
            "train loss:0.8801584246895264\n",
            "train loss:0.8460291499408654\n",
            "train loss:0.8040898264960563\n",
            "train loss:0.8500299181534551\n",
            "train loss:0.8272303174183875\n",
            "train loss:0.6502851729339325\n",
            "train loss:0.9023868399999215\n",
            "train loss:0.7811408439346039\n",
            "train loss:0.8778967529774039\n",
            "train loss:0.9853860954447303\n",
            "train loss:0.6770227441114788\n",
            "train loss:0.8055175663284473\n",
            "train loss:0.9885269324495226\n",
            "train loss:0.8985263920668501\n",
            "train loss:0.9394156855353529\n",
            "train loss:0.788738540267061\n",
            "train loss:0.7127336636411292\n",
            "train loss:0.8968595501672308\n",
            "train loss:0.8890456334930923\n",
            "train loss:0.723494590136598\n",
            "train loss:0.9525868957378363\n",
            "train loss:0.8801260263922752\n",
            "train loss:0.8697964285053986\n",
            "train loss:0.833416427241208\n",
            "train loss:0.7082174194747469\n",
            "train loss:0.8532953587606724\n",
            "train loss:0.8282700569938993\n",
            "train loss:0.9308238282723081\n",
            "train loss:0.8379175637935667\n",
            "train loss:0.8195986697778167\n",
            "train loss:0.999634599194128\n",
            "train loss:0.7770758396238127\n",
            "train loss:0.7793970550825537\n",
            "train loss:0.8461845502210381\n",
            "train loss:0.7848854805969425\n",
            "train loss:0.9398304885345994\n",
            "train loss:0.8427847209304793\n",
            "train loss:1.0254505662883473\n",
            "train loss:0.9837779168576004\n",
            "train loss:0.9202891972179811\n",
            "train loss:0.9707974872889409\n",
            "train loss:0.7503153460865138\n",
            "train loss:0.8954104830286411\n",
            "train loss:0.7365172607404802\n",
            "train loss:0.9346906637820952\n",
            "train loss:0.9861668829602604\n",
            "train loss:0.8462818252491523\n",
            "train loss:0.8477074223042932\n",
            "train loss:0.908275235983111\n",
            "train loss:0.7765572034341041\n",
            "train loss:0.7325493413306455\n",
            "train loss:1.0038558666355604\n",
            "train loss:0.7812133683808574\n",
            "train loss:0.7870844351322818\n",
            "train loss:0.868619363071839\n",
            "train loss:0.753464056920528\n",
            "train loss:0.7193893352003784\n",
            "train loss:0.754067838255746\n",
            "train loss:0.8551056787282404\n",
            "train loss:0.9270852103048535\n",
            "train loss:0.6019744488871225\n",
            "train loss:0.8420313964971246\n",
            "train loss:0.7712987038536433\n",
            "train loss:0.8658722603619299\n",
            "train loss:0.7845105320832358\n",
            "train loss:0.8931702300188019\n",
            "train loss:0.8243770153901937\n",
            "train loss:0.7489432410721076\n",
            "train loss:0.8164358507870727\n",
            "train loss:0.9049483426371262\n",
            "train loss:0.9266765025388467\n",
            "train loss:0.7820647208229046\n",
            "train loss:0.8566769233292958\n",
            "train loss:0.7974801606563169\n",
            "train loss:0.9224804682714194\n",
            "train loss:0.9627014708539393\n",
            "train loss:0.9438582918038463\n",
            "train loss:0.8427652131697861\n",
            "train loss:0.8268039915409289\n",
            "train loss:0.7518252768388608\n",
            "train loss:0.9966043713145032\n",
            "train loss:0.7996703782706135\n",
            "train loss:0.9721661638651036\n",
            "train loss:0.9252282830336838\n",
            "train loss:0.8178278213820421\n",
            "train loss:0.8087885661888938\n",
            "train loss:0.8267793605283391\n",
            "train loss:0.8481411808253592\n",
            "train loss:0.861979909152833\n",
            "train loss:0.8421672498250501\n",
            "train loss:0.818782964445843\n",
            "train loss:0.8177558762682042\n",
            "train loss:0.827224613441982\n",
            "train loss:0.8838874600137866\n",
            "train loss:0.922067623793129\n",
            "train loss:0.6716224901023701\n",
            "train loss:0.8842757147260223\n",
            "train loss:1.0646037564972286\n",
            "train loss:0.894149907419806\n",
            "train loss:0.932056545463007\n",
            "train loss:0.7616329529249702\n",
            "train loss:0.8948721954106331\n",
            "train loss:0.973194973641272\n",
            "train loss:0.8575993479497481\n",
            "train loss:0.9277342889779715\n",
            "train loss:0.6549697079215834\n",
            "train loss:0.8187851910812879\n",
            "train loss:0.9168511761582434\n",
            "train loss:0.7431855662137484\n",
            "train loss:0.9114241175423621\n",
            "train loss:0.7055687811315556\n",
            "train loss:0.672810597478838\n",
            "train loss:0.8222410581854911\n",
            "train loss:0.8145113870813988\n",
            "train loss:0.8238909342891582\n",
            "train loss:0.7401015855896003\n",
            "train loss:0.6964514427145007\n",
            "train loss:0.8878996438072448\n",
            "train loss:0.936167411451921\n",
            "train loss:0.7653524540287125\n",
            "train loss:0.8234924058794707\n",
            "train loss:0.7472897847440119\n",
            "train loss:0.745876485997655\n",
            "train loss:0.8230898333690913\n",
            "train loss:0.8615401728753511\n",
            "train loss:0.8920490846373264\n",
            "train loss:0.8654191716020958\n",
            "train loss:0.9277626311407684\n",
            "train loss:0.9205157727923376\n",
            "train loss:0.8642155223788166\n",
            "train loss:0.9205719346999784\n",
            "train loss:0.8010397800742648\n",
            "train loss:0.9046045866154881\n",
            "train loss:0.8344271579631019\n",
            "train loss:0.7497615872162529\n",
            "train loss:0.9055739361429616\n",
            "train loss:0.894203806083246\n",
            "train loss:0.9297036166549151\n",
            "train loss:0.8339747098884162\n",
            "train loss:0.8524801330092475\n",
            "train loss:0.8338026435556242\n",
            "train loss:0.8456748802399291\n",
            "train loss:0.8104704463984941\n",
            "train loss:0.8942418145652161\n",
            "train loss:0.8821862447901202\n",
            "train loss:0.7876092801558825\n",
            "train loss:0.828044921022638\n",
            "train loss:0.8082874726160493\n",
            "train loss:0.8721907773918681\n",
            "train loss:0.7274226662966751\n",
            "train loss:0.8553359377456208\n",
            "train loss:0.9108341337323188\n",
            "train loss:0.6983369061199098\n",
            "train loss:0.7440721510200333\n",
            "train loss:0.6437743611327303\n",
            "train loss:0.7171247149707098\n",
            "train loss:0.871895760302307\n",
            "train loss:0.8565479018127445\n",
            "train loss:0.6996404295424317\n",
            "train loss:0.7771258045271625\n",
            "train loss:0.8547662686608577\n",
            "train loss:0.7733575546115803\n",
            "train loss:0.9558546391119215\n",
            "train loss:0.8722904410115369\n",
            "train loss:0.8366796649721919\n",
            "train loss:0.8970348759716649\n",
            "train loss:0.8844466151598802\n",
            "train loss:0.8398846898365124\n",
            "train loss:0.875378848415644\n",
            "train loss:0.7265708698756035\n",
            "train loss:0.911057887950474\n",
            "train loss:0.845635447972141\n",
            "train loss:0.644730471335592\n",
            "train loss:0.7040693467449184\n",
            "train loss:0.877861655158731\n",
            "train loss:0.8098006603325026\n",
            "train loss:0.794557891276825\n",
            "train loss:0.9520460549600164\n",
            "train loss:0.8843792853933904\n",
            "train loss:0.8833725113929574\n",
            "train loss:0.8039607822064029\n",
            "train loss:0.9389407434183654\n",
            "train loss:0.757957861799577\n",
            "train loss:0.9392102155199398\n",
            "train loss:1.029201286732792\n",
            "train loss:0.8823421605434888\n",
            "train loss:0.9785176338876411\n",
            "train loss:0.8474937990725436\n",
            "train loss:0.7342650049953582\n",
            "train loss:0.8701404584554728\n",
            "train loss:0.7045568247420947\n",
            "train loss:0.9115457889178467\n",
            "train loss:0.8345073648672441\n",
            "train loss:0.8204072785051717\n",
            "train loss:0.8374223062118916\n",
            "train loss:0.9344172694335856\n",
            "train loss:0.9045931059274835\n",
            "train loss:0.7921481141339922\n",
            "train loss:0.8696068687897471\n",
            "train loss:0.8569652568092794\n",
            "train loss:0.7540078510812317\n",
            "train loss:1.0498042529349518\n",
            "train loss:0.8140653748079236\n",
            "train loss:0.7976224134269768\n",
            "train loss:0.9916527119674781\n",
            "train loss:0.8179315021002469\n",
            "train loss:0.9030716885412737\n",
            "train loss:0.7694543755757293\n",
            "train loss:0.6998974166493168\n",
            "train loss:0.7153296397448081\n",
            "train loss:0.9058756474038197\n",
            "train loss:0.8061068069373373\n",
            "train loss:0.8744806397766479\n",
            "train loss:0.9137756811329416\n",
            "train loss:0.836880361542267\n",
            "train loss:0.648245839895924\n",
            "train loss:0.9638443944429627\n",
            "train loss:1.0097554755273328\n",
            "train loss:0.7927567310174964\n",
            "train loss:0.7932615321045706\n",
            "train loss:0.8618852335588718\n",
            "train loss:0.9160831043413893\n",
            "train loss:1.04305404424662\n",
            "train loss:0.895048381466944\n",
            "train loss:0.7920247381625336\n",
            "train loss:0.8536653609871656\n",
            "train loss:0.8431088898845845\n",
            "train loss:0.8198522210933825\n",
            "train loss:0.9398159202428576\n",
            "train loss:0.8611653609489855\n",
            "train loss:0.812146299361512\n",
            "train loss:0.7988751573323818\n",
            "train loss:0.7506042309722675\n",
            "train loss:0.850635864609342\n",
            "train loss:0.9150677618234521\n",
            "train loss:0.9094261572768736\n",
            "train loss:0.8788328248311056\n",
            "train loss:0.8238985804968486\n",
            "train loss:0.9610456204687955\n",
            "train loss:0.8890725765785393\n",
            "train loss:0.691930414547226\n",
            "train loss:0.8100155492056692\n",
            "train loss:0.7837964140255516\n",
            "train loss:0.7394652152181054\n",
            "train loss:0.9351981103642458\n",
            "train loss:0.7375027175446773\n",
            "train loss:1.0229305010455818\n",
            "train loss:0.7468501029609078\n",
            "train loss:1.0719876941764996\n",
            "train loss:0.8626630273937498\n",
            "train loss:0.7502090955561874\n",
            "train loss:0.9372403316935544\n",
            "train loss:0.8485074142503811\n",
            "train loss:0.8486433322721164\n",
            "train loss:0.8820818566984479\n",
            "train loss:0.821644015352347\n",
            "train loss:0.9163159899209002\n",
            "train loss:1.105992594505763\n",
            "train loss:0.7663692522070323\n",
            "train loss:0.8817328221079721\n",
            "train loss:0.816705320916955\n",
            "train loss:0.9309838336575429\n",
            "train loss:0.9181349732146394\n",
            "train loss:0.8462639545357024\n",
            "train loss:0.7926298983596769\n",
            "train loss:0.9364675116486703\n",
            "train loss:0.7259128757718704\n",
            "train loss:0.8102224381670675\n",
            "train loss:0.8796927111165409\n",
            "train loss:0.9253069376636799\n",
            "train loss:0.9962743947253878\n",
            "train loss:0.6558791915665619\n",
            "train loss:0.8247528318736471\n",
            "train loss:0.752136660242589\n",
            "train loss:0.804070052829904\n",
            "train loss:0.9131285652639035\n",
            "train loss:0.7469409958995599\n",
            "train loss:0.9257271724232977\n",
            "train loss:0.6917462197153031\n",
            "train loss:0.6817444097864346\n",
            "train loss:0.7328768271299364\n",
            "train loss:0.8307084196359962\n",
            "train loss:0.7779722376540404\n",
            "train loss:0.6690952334591326\n",
            "train loss:0.7117405972284924\n",
            "train loss:0.9146762618494096\n",
            "train loss:0.7665556579201226\n",
            "train loss:1.0120215680621174\n",
            "=== epoch:19, train acc:0.998, test acc:0.991 ===\n",
            "train loss:0.777918141203327\n",
            "train loss:0.9069276557303819\n",
            "train loss:0.888919421228824\n",
            "train loss:0.8307380867950741\n",
            "train loss:0.8560518720003313\n",
            "train loss:0.8611386813323932\n",
            "train loss:0.9364792170735204\n",
            "train loss:0.9034898517056636\n",
            "train loss:0.8149297030390131\n",
            "train loss:0.9465780662055547\n",
            "train loss:0.7736769952767097\n",
            "train loss:0.8146950687433039\n",
            "train loss:0.6522565162095154\n",
            "train loss:0.840766710459387\n",
            "train loss:1.0187656209371179\n",
            "train loss:0.9011610684400152\n",
            "train loss:0.9097004695217619\n",
            "train loss:0.8779768089115878\n",
            "train loss:0.8178216848350912\n",
            "train loss:0.8692987606940696\n",
            "train loss:0.9280682048281558\n",
            "train loss:0.7780645294977303\n",
            "train loss:0.9759361801164311\n",
            "train loss:0.9231129230343624\n",
            "train loss:0.7723630478896325\n",
            "train loss:0.9688221463872382\n",
            "train loss:0.8067140956332913\n",
            "train loss:0.956126192698092\n",
            "train loss:0.8540944118275795\n",
            "train loss:0.8597030062074411\n",
            "train loss:1.0164889427706822\n",
            "train loss:0.9713821162007436\n",
            "train loss:0.7545827727247592\n",
            "train loss:0.9001460693571981\n",
            "train loss:0.7492150306922956\n",
            "train loss:0.9002053379177544\n",
            "train loss:0.9682654363090321\n",
            "train loss:0.8397964191966248\n",
            "train loss:0.9626118608090688\n",
            "train loss:0.7592666925030461\n",
            "train loss:0.7326366903337572\n",
            "train loss:0.83912678015848\n",
            "train loss:0.8008263228765987\n",
            "train loss:0.8059395185541024\n",
            "train loss:0.7914975158890303\n",
            "train loss:0.7569183705125349\n",
            "train loss:0.7624848434905519\n",
            "train loss:0.9211740227727405\n",
            "train loss:0.9267937544277673\n",
            "train loss:0.8932479339082046\n",
            "train loss:0.7619820044935026\n",
            "train loss:0.9627246016591775\n",
            "train loss:0.945796808635097\n",
            "train loss:0.9275367001810225\n",
            "train loss:0.7708888519927907\n",
            "train loss:0.771889805970083\n",
            "train loss:0.8694621482251548\n",
            "train loss:0.8588737437096182\n",
            "train loss:0.6406886883065777\n",
            "train loss:0.9525772093655831\n",
            "train loss:0.6971666185142431\n",
            "train loss:0.9349377203510173\n",
            "train loss:0.8940764322411973\n",
            "train loss:0.8313079290101563\n",
            "train loss:0.7746037057482357\n",
            "train loss:0.8178000821717017\n",
            "train loss:0.7920145012427495\n",
            "train loss:0.9913984389399689\n",
            "train loss:0.898992746576596\n",
            "train loss:0.9571590495876353\n",
            "train loss:0.8320509005116952\n",
            "train loss:0.7916746879761634\n",
            "train loss:0.8978583473947985\n",
            "train loss:0.8165721499612505\n",
            "train loss:0.7759462680589714\n",
            "train loss:0.8401922391913291\n",
            "train loss:0.7616101257740314\n",
            "train loss:0.8022769339931175\n",
            "train loss:0.8263167388174689\n",
            "train loss:0.8885938281050413\n",
            "train loss:0.7484341092832427\n",
            "train loss:0.7738363052881607\n",
            "train loss:0.8301770104647094\n",
            "train loss:0.928733203780146\n",
            "train loss:0.7339893301189088\n",
            "train loss:0.7616461463469638\n",
            "train loss:0.9388183371040267\n",
            "train loss:0.8221803154604634\n",
            "train loss:1.0492509066723563\n",
            "train loss:0.9627000202506093\n",
            "train loss:0.8498056585427451\n",
            "train loss:0.9532404039804174\n",
            "train loss:0.925228574531363\n",
            "train loss:0.9780211418013681\n",
            "train loss:0.8609095624697268\n",
            "train loss:0.7871697569022426\n",
            "train loss:0.8173355174302595\n",
            "train loss:1.0344442164555616\n",
            "train loss:0.7764328597778575\n",
            "train loss:0.9637113634019343\n",
            "train loss:0.810225051562878\n",
            "train loss:0.8217227486098624\n",
            "train loss:0.8887902692201457\n",
            "train loss:0.8378180588799158\n",
            "train loss:0.9448456438921232\n",
            "train loss:0.767518663382659\n",
            "train loss:0.8659706747123521\n",
            "train loss:0.98679548394058\n",
            "train loss:0.8586342540640386\n",
            "train loss:0.9390347957806537\n",
            "train loss:0.8398409940535307\n",
            "train loss:0.8114178589577008\n",
            "train loss:0.8120577151778234\n",
            "train loss:0.9302551629013132\n",
            "train loss:0.90039014164825\n",
            "train loss:0.8282804848466256\n",
            "train loss:0.7370438555752272\n",
            "train loss:0.8175554819429388\n",
            "train loss:0.8426746576251449\n",
            "train loss:0.8457725547791997\n",
            "train loss:0.7200187194838336\n",
            "train loss:0.8315171194515216\n",
            "train loss:0.9253179730339914\n",
            "train loss:0.7856785319143065\n",
            "train loss:0.8163692364932039\n",
            "train loss:0.9336846342711614\n",
            "train loss:0.9301084352853789\n",
            "train loss:0.8111722407775097\n",
            "train loss:0.7612502632368677\n",
            "train loss:0.9011340461220523\n",
            "train loss:0.8341816620949237\n",
            "train loss:0.8254648783816468\n",
            "train loss:0.8193181533981057\n",
            "train loss:0.8638117730497125\n",
            "train loss:0.9200449010832753\n",
            "train loss:1.017102078497256\n",
            "train loss:0.895855966269734\n",
            "train loss:1.0262383039567597\n",
            "train loss:0.7787058627085011\n",
            "train loss:0.8150249239636951\n",
            "train loss:0.9221492071971923\n",
            "train loss:0.7995487759256248\n",
            "train loss:0.8380672266860593\n",
            "train loss:0.9104081659523345\n",
            "train loss:0.7983282830278364\n",
            "train loss:0.7625691474192887\n",
            "train loss:0.782180116572279\n",
            "train loss:0.7661425930548785\n",
            "train loss:0.8247878966878461\n",
            "train loss:0.9234415216044336\n",
            "train loss:0.8014312365496318\n",
            "train loss:0.781814667939819\n",
            "train loss:0.8176263636367683\n",
            "train loss:0.6084559156330648\n",
            "train loss:0.8966590611983185\n",
            "train loss:1.0478103544274593\n",
            "train loss:0.7365179450595043\n",
            "train loss:0.7459989770400379\n",
            "train loss:0.9412913156188829\n",
            "train loss:0.8315694386201896\n",
            "train loss:1.0500120961045483\n",
            "train loss:0.7737998118153671\n",
            "train loss:0.8091766500684097\n",
            "train loss:0.871333915828463\n",
            "train loss:0.9147344030570026\n",
            "train loss:0.8728848951383463\n",
            "train loss:0.890134589279471\n",
            "train loss:0.9381390292809222\n",
            "train loss:0.6738003854046726\n",
            "train loss:0.9234688180558506\n",
            "train loss:0.991976969767447\n",
            "train loss:0.7919889941935981\n",
            "train loss:0.8642531969917475\n",
            "train loss:0.9139728623620365\n",
            "train loss:0.7898795147548835\n",
            "train loss:0.9019501890335836\n",
            "train loss:0.8142596276003939\n",
            "train loss:0.7911583883783245\n",
            "train loss:0.9117461353174562\n",
            "train loss:0.7865887091567714\n",
            "train loss:0.9057292805378208\n",
            "train loss:0.8270636207750433\n",
            "train loss:0.9819220724691227\n",
            "train loss:0.9353386908763729\n",
            "train loss:0.8101732469795607\n",
            "train loss:0.857674022862464\n",
            "train loss:0.8153865842937605\n",
            "train loss:0.7641239462288493\n",
            "train loss:0.9019624919822451\n",
            "train loss:0.8042818167053274\n",
            "train loss:0.875128619714173\n",
            "train loss:0.7431271601512758\n",
            "train loss:1.0150249791717794\n",
            "train loss:0.9108942533207873\n",
            "train loss:0.8459803641927034\n",
            "train loss:0.7861465963241611\n",
            "train loss:0.7655252812375464\n",
            "train loss:0.7681139990878978\n",
            "train loss:0.8012538454304828\n",
            "train loss:0.9043067500328341\n",
            "train loss:0.903644714856432\n",
            "train loss:0.7549147990097677\n",
            "train loss:0.7792684103125325\n",
            "train loss:0.8176754335363902\n",
            "train loss:0.8838194937185606\n",
            "train loss:0.8099409106108773\n",
            "train loss:0.8746241286488899\n",
            "train loss:0.8620897582961979\n",
            "train loss:0.805276687031445\n",
            "train loss:0.8221179299156315\n",
            "train loss:0.867961988527881\n",
            "train loss:0.8112343533626952\n",
            "train loss:0.7320188144660932\n",
            "train loss:0.8338664591839016\n",
            "train loss:0.8002786702417114\n",
            "train loss:0.7809944823560201\n",
            "train loss:0.9305677260169938\n",
            "train loss:0.9515694825979361\n",
            "train loss:0.8063219706900401\n",
            "train loss:0.6797137859296812\n",
            "train loss:0.8519638831066558\n",
            "train loss:0.8340820202344628\n",
            "train loss:0.8213369988225461\n",
            "train loss:0.761061664393161\n",
            "train loss:0.7856423335248977\n",
            "train loss:0.8151838512941147\n",
            "train loss:1.0184466479116578\n",
            "train loss:0.8917438660596855\n",
            "train loss:0.8764075405150481\n",
            "train loss:0.954452086535269\n",
            "train loss:0.862833304218586\n",
            "train loss:0.8028021291793443\n",
            "train loss:0.8734145005015769\n",
            "train loss:1.0641525906587261\n",
            "train loss:0.7867861732416729\n",
            "train loss:0.8885329957051169\n",
            "train loss:0.7610942714801766\n",
            "train loss:0.9649704107105204\n",
            "train loss:0.8782970085310157\n",
            "train loss:0.9089265204825456\n",
            "train loss:0.8627888562352665\n",
            "train loss:0.9660298081387831\n",
            "train loss:0.9579125812629473\n",
            "train loss:1.0471234847553896\n",
            "train loss:0.8288528908714413\n",
            "train loss:0.8915729340059915\n",
            "train loss:0.7467030771903054\n",
            "train loss:0.7841071529426091\n",
            "train loss:0.6627362133748582\n",
            "train loss:0.798506369876599\n",
            "train loss:0.8180699367220605\n",
            "train loss:1.0481758200171039\n",
            "train loss:0.8567416142045206\n",
            "train loss:0.7594361431205057\n",
            "train loss:0.9495767059429281\n",
            "train loss:0.8652546180457574\n",
            "train loss:0.6827403910224674\n",
            "train loss:0.8312392173835835\n",
            "train loss:0.9009229740366775\n",
            "train loss:0.7978018698499946\n",
            "train loss:0.7273476206361847\n",
            "train loss:0.7343402664893667\n",
            "train loss:0.699739623976484\n",
            "train loss:0.9395892732520142\n",
            "train loss:0.8658212539414702\n",
            "train loss:0.8855003390034767\n",
            "train loss:0.7316639775058306\n",
            "train loss:0.9179544233616639\n",
            "train loss:0.8881755573101823\n",
            "train loss:0.9857663499400977\n",
            "train loss:0.907852553730985\n",
            "train loss:1.0208256564129856\n",
            "train loss:0.9868023549167304\n",
            "train loss:0.8196867021844124\n",
            "train loss:0.8207565639267937\n",
            "train loss:0.9288899029949766\n",
            "train loss:0.716078559988767\n",
            "train loss:0.7383909864143324\n",
            "train loss:0.9835942728159265\n",
            "train loss:0.8853607853509899\n",
            "train loss:0.9051471542273346\n",
            "train loss:0.9793237295266024\n",
            "train loss:0.8594410165719836\n",
            "train loss:0.7011061021707023\n",
            "train loss:0.9329298727242812\n",
            "train loss:0.963001578992476\n",
            "train loss:0.793776256231518\n",
            "train loss:1.0180940278173307\n",
            "train loss:0.9110847223164421\n",
            "train loss:1.0440911350287378\n",
            "train loss:0.8663706606095887\n",
            "train loss:1.165257884580772\n",
            "train loss:0.8461575079650032\n",
            "train loss:0.9739630123463275\n",
            "train loss:0.7753991964932386\n",
            "train loss:0.7209269596196737\n",
            "train loss:0.7206372782780213\n",
            "train loss:0.9141277556935187\n",
            "train loss:0.7771439007318263\n",
            "train loss:0.7407928367912787\n",
            "train loss:0.8898133938877095\n",
            "train loss:0.81523317601129\n",
            "train loss:0.8281642517416491\n",
            "train loss:0.8041588030349596\n",
            "train loss:0.903053273126127\n",
            "train loss:0.8321746655907392\n",
            "train loss:0.9410675155544187\n",
            "train loss:0.8700121871759715\n",
            "train loss:0.9221495645365002\n",
            "train loss:0.7284760726566647\n",
            "train loss:0.7973348758270339\n",
            "train loss:0.7863561055007198\n",
            "train loss:0.8410131307441499\n",
            "train loss:0.8991620351620244\n",
            "train loss:0.8530515863736602\n",
            "train loss:0.8786755902300937\n",
            "train loss:1.0687224792213466\n",
            "train loss:0.9298132984753147\n",
            "train loss:0.8916952168873279\n",
            "train loss:0.7915757924336966\n",
            "train loss:0.9242152597978874\n",
            "train loss:0.9135858505628137\n",
            "train loss:0.9226527499237815\n",
            "train loss:0.9302718593526318\n",
            "train loss:0.8353138355409954\n",
            "train loss:0.9364373544075733\n",
            "train loss:0.8768167337726099\n",
            "train loss:0.9617898567606806\n",
            "train loss:0.8818524352242321\n",
            "train loss:0.9075788599042302\n",
            "train loss:0.6849766658593546\n",
            "train loss:0.799038342016725\n",
            "train loss:0.8403036527159412\n",
            "train loss:0.8372868132279723\n",
            "train loss:0.848272071782163\n",
            "train loss:0.8774765520475483\n",
            "train loss:0.8980792598582173\n",
            "train loss:0.9075556448814721\n",
            "train loss:0.8458336684411656\n",
            "train loss:0.8568046230296864\n",
            "train loss:0.856241396837068\n",
            "train loss:0.8478949941760955\n",
            "train loss:0.7469528100639562\n",
            "train loss:0.8773499641602078\n",
            "train loss:0.845786784370527\n",
            "train loss:0.8985011983388657\n",
            "train loss:0.8115050083063692\n",
            "train loss:0.7886904508139478\n",
            "train loss:0.7935859508152432\n",
            "train loss:0.8300651174647655\n",
            "train loss:0.9107912295304688\n",
            "train loss:0.8270898513318931\n",
            "train loss:0.9704085809081369\n",
            "train loss:0.9384013902023909\n",
            "train loss:0.9099264397404976\n",
            "train loss:0.7306548728925074\n",
            "train loss:0.7054366350769676\n",
            "train loss:0.7993378336891597\n",
            "train loss:0.9396009174573241\n",
            "train loss:0.9005828200340065\n",
            "train loss:0.808499927415483\n",
            "train loss:0.864673353784911\n",
            "train loss:0.9435406031142289\n",
            "train loss:0.7402029940168514\n",
            "train loss:0.914916508235285\n",
            "train loss:0.8886134274116239\n",
            "train loss:0.7856207176153908\n",
            "train loss:0.9921030100352514\n",
            "train loss:0.7602009938826368\n",
            "train loss:0.8615852813629542\n",
            "train loss:1.022051035550578\n",
            "train loss:0.890934478405235\n",
            "train loss:0.8138747308471719\n",
            "train loss:0.7923250574967438\n",
            "train loss:0.8744988612699536\n",
            "train loss:0.927444611479082\n",
            "train loss:0.8671118801553519\n",
            "train loss:0.9690784162396383\n",
            "train loss:0.9789076370483752\n",
            "train loss:0.9996249785062471\n",
            "train loss:0.8245864339685204\n",
            "train loss:1.0174866038284882\n",
            "train loss:0.771528154891141\n",
            "train loss:0.8080157962184858\n",
            "train loss:0.9566005362893298\n",
            "train loss:0.7102250002219594\n",
            "train loss:1.0588775058057371\n",
            "train loss:0.8579492434251681\n",
            "train loss:0.7550942362281197\n",
            "train loss:0.88252826225051\n",
            "train loss:0.8907616034320988\n",
            "train loss:0.8089292178631095\n",
            "train loss:0.9457087987013731\n",
            "train loss:0.7982442132955911\n",
            "train loss:0.6033394081559436\n",
            "train loss:0.8417787599028341\n",
            "train loss:0.7339425064071046\n",
            "train loss:0.952869404203298\n",
            "train loss:0.848067641556916\n",
            "train loss:0.8274232046118877\n",
            "train loss:0.9046895073694387\n",
            "train loss:0.8381926632741704\n",
            "train loss:0.7872363126948947\n",
            "train loss:0.8959213707660824\n",
            "train loss:0.9053182126871435\n",
            "train loss:0.9236176616416922\n",
            "train loss:0.7805703176548205\n",
            "train loss:0.8478015502182459\n",
            "train loss:0.864289288067199\n",
            "train loss:0.7969106826642816\n",
            "train loss:0.8422728569282283\n",
            "train loss:0.8227467190005275\n",
            "train loss:1.0029210185406716\n",
            "train loss:0.6995671610581081\n",
            "train loss:0.896765692559091\n",
            "train loss:0.9127200227347059\n",
            "train loss:0.979088703569916\n",
            "train loss:0.8381645951391399\n",
            "train loss:0.7367154553921904\n",
            "train loss:0.8366643138598012\n",
            "train loss:1.0439537699883912\n",
            "train loss:0.8838494402954374\n",
            "train loss:0.8202398771185755\n",
            "train loss:0.9216349598969612\n",
            "train loss:0.9354591664312779\n",
            "train loss:1.0973109555402363\n",
            "train loss:0.8189749622182612\n",
            "train loss:0.7357408397304722\n",
            "train loss:0.7690036097591818\n",
            "train loss:0.8765503590697294\n",
            "train loss:0.8335893704304002\n",
            "train loss:0.7929745012667366\n",
            "train loss:0.7189222592783588\n",
            "train loss:0.8729925857393837\n",
            "train loss:0.875633853406748\n",
            "train loss:0.7706876350968866\n",
            "train loss:0.81372492455697\n",
            "train loss:0.8504320117296387\n",
            "train loss:1.002809585074777\n",
            "train loss:0.8402388409743233\n",
            "train loss:0.7276058757201658\n",
            "train loss:0.8975841634528153\n",
            "train loss:0.9012010213960686\n",
            "train loss:0.9036111649920062\n",
            "train loss:0.701817802134618\n",
            "train loss:0.9202903789884833\n",
            "train loss:0.8102773810062412\n",
            "train loss:0.6458742340242317\n",
            "train loss:0.8583708775666326\n",
            "train loss:0.8207910331804941\n",
            "train loss:0.8927245521359213\n",
            "train loss:0.774391561825922\n",
            "train loss:0.8254595632394952\n",
            "train loss:0.8239183852378668\n",
            "train loss:0.8946990167872984\n",
            "train loss:0.9496296834159168\n",
            "train loss:0.917123438407187\n",
            "train loss:0.9211076366105965\n",
            "train loss:0.8577425251121198\n",
            "train loss:0.9373040505732527\n",
            "train loss:0.8663861793272861\n",
            "train loss:0.7637315801445301\n",
            "train loss:0.991171163742883\n",
            "train loss:0.7376014036738144\n",
            "train loss:0.8382940096075088\n",
            "train loss:0.9195187862972068\n",
            "train loss:0.7882533266246311\n",
            "train loss:1.0456687789825707\n",
            "train loss:0.88003503614915\n",
            "train loss:0.7965611258566685\n",
            "train loss:0.8095964504660571\n",
            "train loss:0.9096348126329424\n",
            "train loss:0.7357584016956527\n",
            "train loss:0.8303292189755723\n",
            "train loss:0.8246852257989329\n",
            "train loss:0.7700460130785232\n",
            "train loss:0.8709076415176594\n",
            "train loss:0.784717619489914\n",
            "train loss:0.9885983662905486\n",
            "train loss:0.7481491419365339\n",
            "train loss:0.786408535008516\n",
            "train loss:0.9013619318270238\n",
            "train loss:0.9808138165709412\n",
            "train loss:0.766853629456115\n",
            "train loss:0.9780264982924034\n",
            "train loss:0.8832042742804687\n",
            "train loss:0.8479132240512596\n",
            "train loss:0.8176471534682916\n",
            "train loss:0.9812067137288739\n",
            "train loss:0.7783334290344635\n",
            "train loss:0.8922126630212608\n",
            "train loss:0.8791887045477575\n",
            "train loss:0.7943771668478218\n",
            "train loss:0.8954992096051261\n",
            "train loss:1.0115979458463413\n",
            "train loss:0.8988435123849545\n",
            "train loss:0.865617573155676\n",
            "train loss:0.7461634299599104\n",
            "train loss:0.9865777847554021\n",
            "train loss:0.7003113747065507\n",
            "train loss:0.7786721223829529\n",
            "train loss:0.7837081464342239\n",
            "train loss:0.9325109315879466\n",
            "train loss:0.8964033755967218\n",
            "train loss:0.6405906909651411\n",
            "train loss:0.861938862604397\n",
            "train loss:0.9713101378536725\n",
            "train loss:0.9761455905896548\n",
            "train loss:0.9295403340855108\n",
            "train loss:0.9354674739307198\n",
            "train loss:0.9812499992246864\n",
            "train loss:0.9299541931514406\n",
            "train loss:0.8612694264730523\n",
            "train loss:0.7398398552029193\n",
            "train loss:0.8877102564709891\n",
            "train loss:0.714087168430509\n",
            "train loss:0.7039156712185861\n",
            "train loss:0.7243592265711009\n",
            "train loss:0.7916804848475294\n",
            "train loss:0.9872648965832519\n",
            "train loss:0.8311763765477161\n",
            "train loss:0.755274131024852\n",
            "train loss:0.889651730432802\n",
            "train loss:0.8539834384549931\n",
            "train loss:0.7644965623220482\n",
            "train loss:0.8262887854313867\n",
            "train loss:0.8176947407289756\n",
            "train loss:0.958877748453027\n",
            "train loss:0.7774779042027485\n",
            "train loss:0.7907147855479235\n",
            "train loss:0.9676758382721108\n",
            "train loss:0.7923043974925279\n",
            "train loss:0.8038625079816757\n",
            "train loss:0.9512931694067405\n",
            "train loss:0.6491738555053366\n",
            "train loss:0.8814586248027066\n",
            "train loss:0.8459215669888797\n",
            "train loss:0.9706339880975702\n",
            "train loss:0.7362352268675407\n",
            "train loss:0.6911351554709911\n",
            "train loss:0.9120518448688862\n",
            "train loss:0.8862635252020277\n",
            "train loss:1.0608998485020427\n",
            "train loss:0.7662429091705141\n",
            "train loss:0.9943268824079569\n",
            "train loss:0.8186566391824581\n",
            "train loss:0.8394669797164477\n",
            "train loss:0.990546116871048\n",
            "train loss:1.0001450976160056\n",
            "train loss:0.8458319909942911\n",
            "train loss:0.7977447801331272\n",
            "train loss:0.8175192531345226\n",
            "train loss:0.8809205981008918\n",
            "train loss:0.8309464516543256\n",
            "train loss:0.7613521803088464\n",
            "train loss:0.7627194658873339\n",
            "train loss:0.7529437306992642\n",
            "train loss:0.7980068701221761\n",
            "train loss:0.8896600366973832\n",
            "train loss:0.9942975400465844\n",
            "train loss:0.8346634277320851\n",
            "train loss:0.848909048572509\n",
            "train loss:0.6904957772231904\n",
            "train loss:0.898691238468646\n",
            "train loss:0.795941675727476\n",
            "train loss:0.7715105672038354\n",
            "train loss:0.8453610430285036\n",
            "train loss:0.7911921273742989\n",
            "train loss:0.8464230272559393\n",
            "train loss:0.8830734725910208\n",
            "train loss:0.8664907658847666\n",
            "train loss:0.8928204277233869\n",
            "train loss:0.8053442690756945\n",
            "train loss:0.697550258530854\n",
            "train loss:0.8901365313479793\n",
            "train loss:0.8624082519200207\n",
            "train loss:0.8440727805170379\n",
            "train loss:0.8676769408355276\n",
            "train loss:0.8851602700788489\n",
            "train loss:0.7275278517318351\n",
            "train loss:0.8247925713443673\n",
            "train loss:0.7675625612691946\n",
            "train loss:0.9057561944881979\n",
            "train loss:0.8415573688142003\n",
            "train loss:0.8546936909896374\n",
            "train loss:0.8645962279555574\n",
            "train loss:0.9425577525448361\n",
            "train loss:1.0436663488443783\n",
            "train loss:0.8827174807877537\n",
            "train loss:0.7062910723905285\n",
            "train loss:0.7515921175030321\n",
            "train loss:0.8675418413120844\n",
            "train loss:0.932664588669259\n",
            "train loss:0.978420222553665\n",
            "train loss:0.8632144530924425\n",
            "train loss:0.756341321227126\n",
            "train loss:0.7964531641398064\n",
            "train loss:1.0019925164255061\n",
            "train loss:0.83227769349693\n",
            "train loss:0.7519185555754251\n",
            "=== epoch:20, train acc:0.996, test acc:0.992 ===\n",
            "train loss:0.7002632410049483\n",
            "train loss:1.0507360474266407\n",
            "train loss:0.9438743780536608\n",
            "train loss:0.867456308801152\n",
            "train loss:0.8751721396126934\n",
            "train loss:0.9297449458285856\n",
            "train loss:0.9528191328862881\n",
            "train loss:0.9076063837377244\n",
            "train loss:0.9727411182219137\n",
            "train loss:0.8750087205022248\n",
            "train loss:1.0447163339087147\n",
            "train loss:0.8402283140583904\n",
            "train loss:0.9872418165961314\n",
            "train loss:0.8821765992023218\n",
            "train loss:0.9300782863313978\n",
            "train loss:0.9228109629884524\n",
            "train loss:0.9752206320488175\n",
            "train loss:0.8176756054924454\n",
            "train loss:0.7887679420210304\n",
            "train loss:0.9643062230437759\n",
            "train loss:0.8658596284637724\n",
            "train loss:0.8701752375795124\n",
            "train loss:0.9169693659588084\n",
            "train loss:0.9203003648462075\n",
            "train loss:0.8187842891355993\n",
            "train loss:0.8263058671691543\n",
            "train loss:0.983747270268105\n",
            "train loss:0.774749377764644\n",
            "train loss:0.8166013333124639\n",
            "train loss:0.7660360734745782\n",
            "train loss:0.7235139475474398\n",
            "train loss:0.907446605398695\n",
            "train loss:0.8294886148818797\n",
            "train loss:0.8055813760009797\n",
            "train loss:0.8251943437700947\n",
            "train loss:0.7241466169331119\n",
            "train loss:0.9718827039180524\n",
            "train loss:0.616246827699742\n",
            "train loss:0.7073096493016483\n",
            "train loss:0.8924452028104767\n",
            "train loss:0.9510284073089552\n",
            "train loss:0.9533991937892199\n",
            "train loss:0.900151451410776\n",
            "train loss:0.8080192521051061\n",
            "train loss:0.963964116169718\n",
            "train loss:0.9264298564807996\n",
            "train loss:0.7669685750389502\n",
            "train loss:0.7736098181245227\n",
            "train loss:0.7750778779488503\n",
            "train loss:0.9347621047350795\n",
            "train loss:0.7101520059885195\n",
            "train loss:0.8196639291230948\n",
            "train loss:0.7758142494595269\n",
            "train loss:0.9307576324016443\n",
            "train loss:0.7855585423254388\n",
            "train loss:0.8740336955200632\n",
            "train loss:0.7761755360389385\n",
            "train loss:0.9130044571406961\n",
            "train loss:0.8252660497436708\n",
            "train loss:0.8135128961466862\n",
            "train loss:0.8619087997416834\n",
            "train loss:0.7991158967239498\n",
            "train loss:0.8269253047093522\n",
            "train loss:0.9219536005460963\n",
            "train loss:0.8397294816438332\n",
            "train loss:0.8110019359499652\n",
            "train loss:0.770480093464713\n",
            "train loss:0.7998298706428238\n",
            "train loss:0.9087750574162174\n",
            "train loss:0.9260636053386864\n",
            "train loss:0.9395765396636456\n",
            "train loss:0.8851567432696994\n",
            "train loss:0.792808077003761\n",
            "train loss:0.8382004389822535\n",
            "train loss:0.8871510136902575\n",
            "train loss:0.9297271708991462\n",
            "train loss:0.8643004188867087\n",
            "train loss:0.808999332231291\n",
            "train loss:0.7638180583955672\n",
            "train loss:0.9810837448391395\n",
            "train loss:0.9212797330027446\n",
            "train loss:0.7070637231619691\n",
            "train loss:0.7323638656544382\n",
            "train loss:0.7545340191726402\n",
            "train loss:0.8342957088051428\n",
            "train loss:0.8292392850717603\n",
            "train loss:0.7166437635211887\n",
            "train loss:0.8572930512965274\n",
            "train loss:0.8223554774507504\n",
            "train loss:0.822634363151263\n",
            "train loss:1.0076524787088301\n",
            "train loss:0.8950768959067906\n",
            "train loss:0.8512725142128679\n",
            "train loss:0.8133214328953438\n",
            "train loss:0.7319463544745072\n",
            "train loss:0.9407888208045866\n",
            "train loss:0.7137073961589698\n",
            "train loss:0.8656544150103884\n",
            "train loss:0.8879950990678573\n",
            "train loss:1.003913840821381\n",
            "train loss:0.9169010654524453\n",
            "train loss:0.8747379565993225\n",
            "train loss:0.8667491998874912\n",
            "train loss:0.907194434624332\n",
            "train loss:0.8882402611858119\n",
            "train loss:0.8672325680127447\n",
            "train loss:0.9526833941684849\n",
            "train loss:0.7544759946291906\n",
            "train loss:0.8047747530239328\n",
            "train loss:0.8248633672764799\n",
            "train loss:0.9876413794040027\n",
            "train loss:0.8468435444324929\n",
            "train loss:0.922166247784552\n",
            "train loss:0.8687303867403903\n",
            "train loss:0.8901844929277363\n",
            "train loss:0.831076648839102\n",
            "train loss:0.5975879315165663\n",
            "train loss:0.7969058920332758\n",
            "train loss:0.8466137531082516\n",
            "train loss:0.9254351357524393\n",
            "train loss:0.8749001349336325\n",
            "train loss:0.8977860068806183\n",
            "train loss:1.0380958418425505\n",
            "train loss:0.7429245543256077\n",
            "train loss:0.8849726179999936\n",
            "train loss:0.933768394425826\n",
            "train loss:0.9927729202856703\n",
            "train loss:0.8495338400829476\n",
            "train loss:0.7135793174976252\n",
            "train loss:0.821669048534979\n",
            "train loss:0.7779337899270855\n",
            "train loss:0.8957901732524518\n",
            "train loss:0.8975989152996054\n",
            "train loss:0.9468147088944896\n",
            "train loss:0.8687466697978414\n",
            "train loss:0.892878900358138\n",
            "train loss:0.9595826350985106\n",
            "train loss:0.9427284128700357\n",
            "train loss:0.8816385504232133\n",
            "train loss:0.8552920446298764\n",
            "train loss:0.8300041238454952\n",
            "train loss:1.0035869234000834\n",
            "train loss:0.9587427279217706\n",
            "train loss:0.9894460962085188\n",
            "train loss:0.848163620085152\n",
            "train loss:0.9113167957053204\n",
            "train loss:0.9036522882028118\n",
            "train loss:0.7931124231732423\n",
            "train loss:0.8957124968178093\n",
            "train loss:0.8830511015340992\n",
            "train loss:0.8649462125971782\n",
            "train loss:0.8711055408840949\n",
            "train loss:0.7421674820481676\n",
            "train loss:0.8871838326280787\n",
            "train loss:0.8727740315209251\n",
            "train loss:0.7958950581287867\n",
            "train loss:0.8154758501590864\n",
            "train loss:0.7499097260182591\n",
            "train loss:0.8466290605997071\n",
            "train loss:0.8976700758251213\n",
            "train loss:0.9886619150637848\n",
            "train loss:0.9416883106759946\n",
            "train loss:0.8400745677471825\n",
            "train loss:0.9114034720399564\n",
            "train loss:0.793318403951032\n",
            "train loss:1.0576182783791792\n",
            "train loss:0.8314078658319524\n",
            "train loss:0.784940847252065\n",
            "train loss:0.8044118469308402\n",
            "train loss:0.9420613587756285\n",
            "train loss:0.7874691287984011\n",
            "train loss:1.0291914375029936\n",
            "train loss:0.9212469307968811\n",
            "train loss:0.9635743001719267\n",
            "train loss:0.7390171245494408\n",
            "train loss:0.7225532074680591\n",
            "train loss:0.8865214544138007\n",
            "train loss:0.7669169404437335\n",
            "train loss:0.8645649779774537\n",
            "train loss:0.9010219081794121\n",
            "train loss:0.8332460001334305\n",
            "train loss:0.8434960684180214\n",
            "train loss:0.7824239427207942\n",
            "train loss:0.726959395156152\n",
            "train loss:0.798457256895702\n",
            "train loss:0.7965785840950288\n",
            "train loss:0.8039764505825211\n",
            "train loss:0.8028722129154952\n",
            "train loss:0.8223102410901195\n",
            "train loss:0.9688111440434374\n",
            "train loss:0.8033301494049239\n",
            "train loss:0.9324337379317887\n",
            "train loss:0.7909939482771154\n",
            "train loss:1.0516140848017854\n",
            "train loss:0.6904048942934564\n",
            "train loss:0.6861293314995491\n",
            "train loss:0.8282923863590216\n",
            "train loss:0.7187180067796041\n",
            "train loss:0.8551776399462706\n",
            "train loss:0.7815140581963061\n",
            "train loss:1.0057502495361466\n",
            "train loss:0.8169055553654277\n",
            "train loss:0.8799340416705672\n",
            "train loss:0.7616087407640019\n",
            "train loss:0.8378387215084444\n",
            "train loss:0.8286445277334634\n",
            "train loss:0.8140718346448326\n",
            "train loss:0.8883686299909789\n",
            "train loss:1.0381063576403562\n",
            "train loss:0.7226234767834251\n",
            "train loss:1.0000168402764067\n",
            "train loss:0.9000805850952527\n",
            "train loss:0.9475147147135518\n",
            "train loss:0.8844766721935332\n",
            "train loss:0.7451927397735723\n",
            "train loss:0.8091557410171462\n",
            "train loss:0.9580594501488113\n",
            "train loss:0.6955993861934828\n",
            "train loss:0.7105906205304936\n",
            "train loss:0.81197793648685\n",
            "train loss:1.0661921460362707\n",
            "train loss:0.8480864133086898\n",
            "train loss:0.9532477925083437\n",
            "train loss:0.9717469273845112\n",
            "train loss:0.8780338949376056\n",
            "train loss:0.9687844053662223\n",
            "train loss:1.022146529196185\n",
            "train loss:0.7777990043087074\n",
            "train loss:0.7759108989474406\n",
            "train loss:0.9888996974270455\n",
            "train loss:0.7760381504536749\n",
            "train loss:0.8332544548502816\n",
            "train loss:0.7331307927253836\n",
            "train loss:0.8469878552449996\n",
            "train loss:0.8831185222330902\n",
            "train loss:1.0609635191764653\n",
            "train loss:0.8990351295324899\n",
            "train loss:0.778742541772455\n",
            "train loss:0.7651422869778333\n",
            "train loss:0.9704644872717312\n",
            "train loss:0.8599979710076784\n",
            "train loss:0.9229938047150604\n",
            "train loss:0.973356272277126\n",
            "train loss:0.9443085925321851\n",
            "train loss:0.8356898120287148\n",
            "train loss:0.8445840343649427\n",
            "train loss:0.882148879930886\n",
            "train loss:0.8049914095808756\n",
            "train loss:0.7987210435890063\n",
            "train loss:1.0480880869694622\n",
            "train loss:0.9301203113219052\n",
            "train loss:0.8529337572613401\n",
            "train loss:0.9241817856480268\n",
            "train loss:0.7876093281161487\n",
            "train loss:0.918860248695069\n",
            "train loss:0.8394971523321949\n",
            "train loss:0.8460992856950188\n",
            "train loss:0.7090812913717696\n",
            "train loss:0.7157053189353009\n",
            "train loss:0.9550971860263315\n",
            "train loss:0.875029860691935\n",
            "train loss:0.8911590597573081\n",
            "train loss:0.961512614443331\n",
            "train loss:0.9297079980520238\n",
            "train loss:0.9344386899579022\n",
            "train loss:0.9737732999461519\n",
            "train loss:0.8792474986256218\n",
            "train loss:0.8834927198460168\n",
            "train loss:0.9035568107635141\n",
            "train loss:0.9352163819250089\n",
            "train loss:0.8392152003929978\n",
            "train loss:0.9703081486923791\n",
            "train loss:0.9599770168815975\n",
            "train loss:0.8109103241792187\n",
            "train loss:0.8627736395407698\n",
            "train loss:0.8365295646598692\n",
            "train loss:0.8384172406542325\n",
            "train loss:0.7310531147516436\n",
            "train loss:0.9207647235725821\n",
            "train loss:0.8057791235550856\n",
            "train loss:0.9209804978378778\n",
            "train loss:0.8145785094912417\n",
            "train loss:0.7953339228664836\n",
            "train loss:0.7351465130889558\n",
            "train loss:0.9967969473198586\n",
            "train loss:0.8326764210605287\n",
            "train loss:0.8107948630082435\n",
            "train loss:0.9044367221603502\n",
            "train loss:0.8099253980004505\n",
            "train loss:0.9549033869521293\n",
            "train loss:0.9293368449028259\n",
            "train loss:0.8660681376830381\n",
            "train loss:0.988531436842445\n",
            "train loss:0.803925188914032\n",
            "train loss:0.9316760035943692\n",
            "train loss:1.0454877592090572\n",
            "train loss:0.821016070711348\n",
            "train loss:0.7548473368572937\n",
            "train loss:0.9567293573852234\n",
            "train loss:0.6620686860497491\n",
            "train loss:0.7910588139105105\n",
            "train loss:0.9227158896781034\n",
            "train loss:0.7110772339799649\n",
            "train loss:0.8345799508989163\n",
            "train loss:0.9642589122343428\n",
            "train loss:0.758834435367564\n",
            "train loss:0.8484076876788579\n",
            "train loss:1.0350758203896184\n",
            "train loss:0.9190704370289001\n",
            "train loss:0.7313642788684512\n",
            "train loss:0.9932165507484911\n",
            "train loss:0.6574268060454491\n",
            "train loss:0.8336474974257793\n",
            "train loss:0.9385879813447352\n",
            "train loss:0.8762727407734743\n",
            "train loss:0.9280226946566061\n",
            "train loss:0.6997180571341327\n",
            "train loss:0.8348720158428492\n",
            "train loss:0.861878534309765\n",
            "train loss:0.8855965064770668\n",
            "train loss:0.7176187768052228\n",
            "train loss:0.8601848236310878\n",
            "train loss:0.8985237566712501\n",
            "train loss:0.772436473764552\n",
            "train loss:0.7669241780410271\n",
            "train loss:0.8286641782982965\n",
            "train loss:0.9386260143756924\n",
            "train loss:0.7642594252823361\n",
            "train loss:0.8417905883481069\n",
            "train loss:0.7457678643910788\n",
            "train loss:1.0455828113954586\n",
            "train loss:0.9095860230871722\n",
            "train loss:0.8890393067806635\n",
            "train loss:0.6745311388830291\n",
            "train loss:0.8678366789003172\n",
            "train loss:0.8242474548860431\n",
            "train loss:0.9326839464993957\n",
            "train loss:0.7611243189145448\n",
            "train loss:0.9067626767211595\n",
            "train loss:0.9077142727518788\n",
            "train loss:0.9554899707332096\n",
            "train loss:0.7264707855618565\n",
            "train loss:0.7497425891954633\n",
            "train loss:1.0316850883749589\n",
            "train loss:0.8750421206264606\n",
            "train loss:0.8993643848748802\n",
            "train loss:1.1260989197128304\n",
            "train loss:0.7965030663028485\n",
            "train loss:0.9336257575930569\n",
            "train loss:0.7134220205208242\n",
            "train loss:0.7177308385507446\n",
            "train loss:0.9142928904156384\n",
            "train loss:0.7725880972089558\n",
            "train loss:0.8177275973328219\n",
            "train loss:0.7902307586053045\n",
            "train loss:0.8071136155684953\n",
            "train loss:0.7960942605502374\n",
            "train loss:0.9322560366120767\n",
            "train loss:0.7480996355710674\n",
            "train loss:0.8084606732056592\n",
            "train loss:0.9219104526617969\n",
            "train loss:0.8756566568433779\n",
            "train loss:0.8914810488595802\n",
            "train loss:0.9145780682066961\n",
            "train loss:0.9563256570190432\n",
            "train loss:0.7526348283549262\n",
            "train loss:0.8553907935087917\n",
            "train loss:0.9937575405610076\n",
            "train loss:0.8147730495487079\n",
            "train loss:1.0541759060259903\n",
            "train loss:0.986395830956673\n",
            "train loss:0.9358061975599005\n",
            "train loss:0.8178377688617795\n",
            "train loss:0.9486629454982869\n",
            "train loss:0.8350557477698034\n",
            "train loss:0.9485310302009785\n",
            "train loss:0.8273581502787786\n",
            "train loss:0.7358886400622943\n",
            "train loss:1.0112726304675754\n",
            "train loss:0.9730058322793872\n",
            "train loss:0.8774487415024802\n",
            "train loss:0.9360427688539015\n",
            "train loss:0.8511456479246778\n",
            "train loss:0.7551501665376215\n",
            "train loss:0.8182977848217026\n",
            "train loss:0.824521804395102\n",
            "train loss:0.7540295790668758\n",
            "train loss:0.810175814765438\n",
            "train loss:0.8880386333161612\n",
            "train loss:0.6501300918715698\n",
            "train loss:0.6752712582422774\n",
            "train loss:1.0721795568918282\n",
            "train loss:0.9148016387438596\n",
            "train loss:0.9424333046229778\n",
            "train loss:0.6842235784115576\n",
            "train loss:0.8674788884295098\n",
            "train loss:0.7543910384563504\n",
            "train loss:0.9971189969138272\n",
            "train loss:0.7637721414400717\n",
            "train loss:0.8178192422023695\n",
            "train loss:0.9302813226397021\n",
            "train loss:0.8874637947709237\n",
            "train loss:0.9509353593384431\n",
            "train loss:0.7362020126814934\n",
            "train loss:0.8337889168076866\n",
            "train loss:0.814756183652157\n",
            "train loss:0.8625284081537496\n",
            "train loss:0.9735122686395848\n",
            "train loss:0.8146572135239878\n",
            "train loss:0.7933968320092545\n",
            "train loss:0.8173664496647538\n",
            "train loss:1.0030643164518298\n",
            "train loss:0.9430865509396607\n",
            "train loss:0.752221178872187\n",
            "train loss:0.9008029237919927\n",
            "train loss:0.8105653673299948\n",
            "train loss:0.8688902438071746\n",
            "train loss:0.839763566404472\n",
            "train loss:0.790147395100977\n",
            "train loss:0.6786224001765692\n",
            "train loss:0.881870484283146\n",
            "train loss:0.8195328548807735\n",
            "train loss:0.837956896205364\n",
            "train loss:0.8385189090382593\n",
            "train loss:0.6514992831346916\n",
            "train loss:0.741543237942758\n",
            "train loss:0.9378430601488167\n",
            "train loss:0.896020795358081\n",
            "train loss:0.9082672244656451\n",
            "train loss:0.9019458258262428\n",
            "train loss:0.8900260095616336\n",
            "train loss:0.9042194653027795\n",
            "train loss:0.9386812653678719\n",
            "train loss:0.8447058006126327\n",
            "train loss:0.8797253186060567\n",
            "train loss:0.887443811361954\n",
            "train loss:0.7115934822188358\n",
            "train loss:1.0699836953366506\n",
            "train loss:0.8494767739630746\n",
            "train loss:0.8085461436503161\n",
            "train loss:0.913262157547715\n",
            "train loss:0.9582076426588222\n",
            "train loss:0.7445965290779348\n",
            "train loss:0.8160946417072488\n",
            "train loss:0.9611403202274746\n",
            "train loss:0.8125871052533746\n",
            "train loss:0.8119372054085838\n",
            "train loss:0.8143027636101204\n",
            "train loss:0.974617009779816\n",
            "train loss:0.7184798215751539\n",
            "train loss:0.7987851197870951\n",
            "train loss:0.9693631816894873\n",
            "train loss:0.7744237290781575\n",
            "train loss:0.8817041636762996\n",
            "train loss:0.6757738651578683\n",
            "train loss:0.7301480532199623\n",
            "train loss:0.9638834994582226\n",
            "train loss:0.7790824497294316\n",
            "train loss:0.9588671034004208\n",
            "train loss:0.7211366247838363\n",
            "train loss:0.7127377555812321\n",
            "train loss:0.862483407181521\n",
            "train loss:0.7520997336552019\n",
            "train loss:0.8279426699702576\n",
            "train loss:0.746758615606486\n",
            "train loss:0.8234307614497887\n",
            "train loss:0.9492069898246633\n",
            "train loss:1.082095434125173\n",
            "train loss:0.8813528263310441\n",
            "train loss:0.6832673259539997\n",
            "train loss:0.8797676003913408\n",
            "train loss:0.8498346519493655\n",
            "train loss:0.9337322099959678\n",
            "train loss:1.0014938046333024\n",
            "train loss:0.8866839294226944\n",
            "train loss:0.7153537211195825\n",
            "train loss:0.8227267728463653\n",
            "train loss:0.7473239342997435\n",
            "train loss:0.800466291806363\n",
            "train loss:0.9497816758760896\n",
            "train loss:0.8951201189629814\n",
            "train loss:0.8135435141490727\n",
            "train loss:0.9863522310083168\n",
            "train loss:0.8627465968272829\n",
            "train loss:0.8754591372588684\n",
            "train loss:0.8788896757530452\n",
            "train loss:0.8206333230001007\n",
            "train loss:0.8305995973282405\n",
            "train loss:0.7857431948229977\n",
            "train loss:0.8411230936884205\n",
            "train loss:0.798092758048514\n",
            "train loss:0.7423593976187851\n",
            "train loss:0.9170530045558717\n",
            "train loss:0.7327023369518337\n",
            "train loss:0.9741036043613325\n",
            "train loss:0.7674147205335978\n",
            "train loss:0.7853981771938022\n",
            "train loss:1.0003204532313172\n",
            "train loss:0.8761968682617385\n",
            "train loss:0.854467715973476\n",
            "train loss:0.8419844484305621\n",
            "train loss:0.9656635460601768\n",
            "train loss:0.834019685113481\n",
            "train loss:0.748644863599842\n",
            "train loss:0.8056088117700902\n",
            "train loss:0.9371362081920708\n",
            "train loss:0.8027056347501221\n",
            "train loss:0.8218184115647316\n",
            "train loss:0.9369184915239518\n",
            "train loss:0.8224380177802922\n",
            "train loss:0.8369626302674967\n",
            "train loss:0.9222999448076367\n",
            "train loss:0.8655165709904473\n",
            "train loss:0.9981969188298104\n",
            "train loss:0.6909008978195242\n",
            "train loss:0.8463616932791318\n",
            "train loss:0.6103328718553864\n",
            "train loss:0.7939732032222282\n",
            "train loss:0.8830187928171982\n",
            "train loss:0.8521458859986533\n",
            "train loss:0.8270900512156536\n",
            "train loss:0.8280728458916562\n",
            "train loss:0.7790456525705701\n",
            "train loss:0.8655109825164514\n",
            "train loss:0.8584885721454809\n",
            "train loss:0.7524153953621839\n",
            "train loss:0.8148072608313969\n",
            "train loss:0.8372002723381743\n",
            "train loss:0.8086010136641921\n",
            "train loss:0.7632283174054264\n",
            "train loss:0.6963648950717293\n",
            "train loss:0.9190483385561705\n",
            "train loss:0.8722690543716065\n",
            "train loss:0.8449424278362315\n",
            "train loss:0.8545019566938511\n",
            "train loss:0.7988530674432258\n",
            "train loss:0.9369594127634706\n",
            "train loss:0.6741258543404487\n",
            "train loss:0.831462547328428\n",
            "train loss:0.8946767794391498\n",
            "train loss:0.9147477170090862\n",
            "train loss:0.7586871618451678\n",
            "train loss:1.0527275146404866\n",
            "train loss:0.9066476037297069\n",
            "train loss:0.7251279864472031\n",
            "train loss:0.9152985826814433\n",
            "train loss:0.7884216156556841\n",
            "train loss:0.8460166993745721\n",
            "train loss:0.8815610292702794\n",
            "train loss:0.8997225873467005\n",
            "train loss:0.8278330988951221\n",
            "train loss:1.0006254872530476\n",
            "train loss:1.0575051816809808\n",
            "train loss:0.841228257295903\n",
            "train loss:0.8256349765209973\n",
            "train loss:0.8080509090410202\n",
            "train loss:0.7351467909498562\n",
            "train loss:0.8257088855458268\n",
            "train loss:0.8837177446509\n",
            "train loss:0.8968105381738427\n",
            "train loss:0.7935924385457621\n",
            "train loss:0.9045954275190676\n",
            "train loss:0.9538001506922794\n",
            "train loss:0.7375989247995725\n",
            "train loss:0.7276214707157637\n",
            "train loss:1.0130988129281355\n",
            "train loss:0.9052462464189657\n",
            "train loss:0.9929850756227157\n",
            "train loss:0.8859612842527861\n",
            "train loss:0.7591196615258272\n",
            "train loss:0.9735386027008955\n",
            "train loss:1.0258559968799847\n",
            "train loss:0.8389992368388994\n",
            "train loss:0.8227930517234704\n",
            "train loss:0.8897500603832204\n",
            "train loss:0.902777106913951\n",
            "train loss:0.93739689095257\n",
            "train loss:0.8849238636881452\n",
            "train loss:0.886316513948019\n",
            "train loss:0.7353977156386499\n",
            "train loss:0.8481319413094408\n",
            "train loss:0.8509485576829009\n",
            "train loss:0.7892235755432231\n",
            "train loss:0.7068037222285836\n",
            "train loss:0.8424184699892995\n",
            "train loss:0.8009760623991794\n",
            "train loss:0.8638543619070009\n",
            "train loss:0.8045246173042352\n",
            "train loss:0.7938519817265697\n",
            "train loss:0.6866945040755212\n",
            "train loss:0.8014059266975373\n",
            "train loss:0.7865392487524843\n",
            "train loss:0.8965061919322891\n",
            "train loss:0.7586768380244415\n",
            "train loss:0.8352287416752802\n",
            "train loss:0.9137184167223675\n",
            "train loss:1.1241200442161685\n",
            "train loss:1.0069220293223031\n",
            "train loss:0.8370436487939901\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9939\n",
            "Saved Network Parameters!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}