{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5471bd42338a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX2D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PCA in module sklearn.decomposition._pca object:\n",
      "\n",
      "class PCA(sklearn.decomposition._base._BasePCA)\n",
      " |  PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
      " |  \n",
      " |  Principal component analysis (PCA).\n",
      " |  \n",
      " |  Linear dimensionality reduction using Singular Value Decomposition of the\n",
      " |  data to project it to a lower dimensional space. The input data is centered\n",
      " |  but not scaled for each feature before applying the SVD.\n",
      " |  \n",
      " |  It uses the LAPACK implementation of the full SVD or a randomized truncated\n",
      " |  SVD by the method of Halko et al. 2009, depending on the shape of the input\n",
      " |  data and the number of components to extract.\n",
      " |  \n",
      " |  It can also use the scipy.sparse.linalg ARPACK implementation of the\n",
      " |  truncated SVD.\n",
      " |  \n",
      " |  Notice that this class does not support sparse input. See\n",
      " |  :class:`TruncatedSVD` for an alternative with sparse data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <PCA>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, float, None or str\n",
      " |      Number of components to keep.\n",
      " |      if n_components is not set all components are kept::\n",
      " |  \n",
      " |          n_components == min(n_samples, n_features)\n",
      " |  \n",
      " |      If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n",
      " |      MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n",
      " |      will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n",
      " |  \n",
      " |      If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n",
      " |      number of components such that the amount of variance that needs to be\n",
      " |      explained is greater than the percentage specified by n_components.\n",
      " |  \n",
      " |      If ``svd_solver == 'arpack'``, the number of components must be\n",
      " |      strictly less than the minimum of n_features and n_samples.\n",
      " |  \n",
      " |      Hence, the None case results in::\n",
      " |  \n",
      " |          n_components == min(n_samples, n_features) - 1\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      If False, data passed to fit are overwritten and running\n",
      " |      fit(X).transform(X) will not yield the expected results,\n",
      " |      use fit_transform(X) instead.\n",
      " |  \n",
      " |  whiten : bool, optional (default False)\n",
      " |      When True (False by default) the `components_` vectors are multiplied\n",
      " |      by the square root of n_samples and then divided by the singular values\n",
      " |      to ensure uncorrelated outputs with unit component-wise variances.\n",
      " |  \n",
      " |      Whitening will remove some information from the transformed signal\n",
      " |      (the relative variance scales of the components) but can sometime\n",
      " |      improve the predictive accuracy of the downstream estimators by\n",
      " |      making their data respect some hard-wired assumptions.\n",
      " |  \n",
      " |  svd_solver : str {'auto', 'full', 'arpack', 'randomized'}\n",
      " |      If auto :\n",
      " |          The solver is selected by a default policy based on `X.shape` and\n",
      " |          `n_components`: if the input data is larger than 500x500 and the\n",
      " |          number of components to extract is lower than 80% of the smallest\n",
      " |          dimension of the data, then the more efficient 'randomized'\n",
      " |          method is enabled. Otherwise the exact full SVD is computed and\n",
      " |          optionally truncated afterwards.\n",
      " |      If full :\n",
      " |          run exact full SVD calling the standard LAPACK solver via\n",
      " |          `scipy.linalg.svd` and select the components by postprocessing\n",
      " |      If arpack :\n",
      " |          run SVD truncated to n_components calling ARPACK solver via\n",
      " |          `scipy.sparse.linalg.svds`. It requires strictly\n",
      " |          0 < n_components < min(X.shape)\n",
      " |      If randomized :\n",
      " |          run randomized SVD by the method of Halko et al.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  tol : float >= 0, optional (default .0)\n",
      " |      Tolerance for singular values computed by svd_solver == 'arpack'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  iterated_power : int >= 0, or 'auto', (default 'auto')\n",
      " |      Number of iterations for the power method computed by\n",
      " |      svd_solver == 'randomized'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``svd_solver`` == 'arpack' or 'randomized'. Pass an int\n",
      " |      for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  components_ : array, shape (n_components, n_features)\n",
      " |      Principal axes in feature space, representing the directions of\n",
      " |      maximum variance in the data. The components are sorted by\n",
      " |      ``explained_variance_``.\n",
      " |  \n",
      " |  explained_variance_ : array, shape (n_components,)\n",
      " |      The amount of variance explained by each of the selected components.\n",
      " |  \n",
      " |      Equal to n_components largest eigenvalues\n",
      " |      of the covariance matrix of X.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  explained_variance_ratio_ : array, shape (n_components,)\n",
      " |      Percentage of variance explained by each of the selected components.\n",
      " |  \n",
      " |      If ``n_components`` is not set then all components are stored and the\n",
      " |      sum of the ratios is equal to 1.0.\n",
      " |  \n",
      " |  singular_values_ : array, shape (n_components,)\n",
      " |      The singular values corresponding to each of the selected components.\n",
      " |      The singular values are equal to the 2-norms of the ``n_components``\n",
      " |      variables in the lower-dimensional space.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  mean_ : array, shape (n_features,)\n",
      " |      Per-feature empirical mean, estimated from the training set.\n",
      " |  \n",
      " |      Equal to `X.mean(axis=0)`.\n",
      " |  \n",
      " |  n_components_ : int\n",
      " |      The estimated number of components. When n_components is set\n",
      " |      to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n",
      " |      number is estimated from input data. Otherwise it equals the parameter\n",
      " |      n_components, or the lesser value of n_features and n_samples\n",
      " |      if n_components is None.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      Number of features in the training data.\n",
      " |  \n",
      " |  n_samples_ : int\n",
      " |      Number of samples in the training data.\n",
      " |  \n",
      " |  noise_variance_ : float\n",
      " |      The estimated noise covariance following the Probabilistic PCA model\n",
      " |      from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
      " |      Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
      " |      http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n",
      " |      compute the estimated data covariance and score samples.\n",
      " |  \n",
      " |      Equal to the average of (min(n_features, n_samples) - n_components)\n",
      " |      smallest eigenvalues of the covariance matrix of X.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  KernelPCA : Kernel Principal Component Analysis.\n",
      " |  SparsePCA : Sparse Principal Component Analysis.\n",
      " |  TruncatedSVD : Dimensionality reduction using truncated SVD.\n",
      " |  IncrementalPCA : Incremental Principal Component Analysis.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  For n_components == 'mle', this class uses the method of *Minka, T. P.\n",
      " |  \"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*\n",
      " |  \n",
      " |  Implements the probabilistic PCA model from:\n",
      " |  Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n",
      " |  component analysis\". Journal of the Royal Statistical Society:\n",
      " |  Series B (Statistical Methodology), 61(3), 611-622.\n",
      " |  via the score and score_samples methods.\n",
      " |  See http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |  \n",
      " |  For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n",
      " |  \n",
      " |  For svd_solver == 'randomized', see:\n",
      " |  *Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n",
      " |  \"Finding structure with randomness: Probabilistic algorithms for\n",
      " |  constructing approximate matrix decompositions\".\n",
      " |  SIAM review, 53(2), 217-288.* and also\n",
      " |  *Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n",
      " |  \"A randomized algorithm for the decomposition of matrices\".\n",
      " |  Applied and Computational Harmonic Analysis, 30(1), 47-68.*\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.decomposition import PCA\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> pca = PCA(n_components=2)\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(n_components=2)\n",
      " |  >>> print(pca.explained_variance_ratio_)\n",
      " |  [0.9924... 0.0075...]\n",
      " |  >>> print(pca.singular_values_)\n",
      " |  [6.30061... 0.54980...]\n",
      " |  \n",
      " |  >>> pca = PCA(n_components=2, svd_solver='full')\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(n_components=2, svd_solver='full')\n",
      " |  >>> print(pca.explained_variance_ratio_)\n",
      " |  [0.9924... 0.00755...]\n",
      " |  >>> print(pca.singular_values_)\n",
      " |  [6.30061... 0.54980...]\n",
      " |  \n",
      " |  >>> pca = PCA(n_components=1, svd_solver='arpack')\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(n_components=1, svd_solver='arpack')\n",
      " |  >>> print(pca.explained_variance_ratio_)\n",
      " |  [0.99244...]\n",
      " |  >>> print(pca.singular_values_)\n",
      " |  [6.30061...]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PCA\n",
      " |      sklearn.decomposition._base._BasePCA\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the model with X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored variable.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit the model with X and apply the dimensionality reduction on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored variable.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array-like, shape (n_samples, n_components)\n",
      " |          Transformed values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method returns a Fortran-ordered array. To convert it to a\n",
      " |      C-ordered array, use 'np.ascontiguousarray'.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Return the average log-likelihood of all samples.\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array, shape(n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored variable.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll : float\n",
      " |          Average log-likelihood of the samples under the current model.\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Return the log-likelihood of each sample.\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array, shape(n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll : array, shape (n_samples,)\n",
      " |          Log-likelihood of each sample under the current model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.decomposition._base._BasePCA:\n",
      " |  \n",
      " |  get_covariance(self)\n",
      " |      Compute data covariance with the generative model.\n",
      " |      \n",
      " |      ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n",
      " |      where S**2 contains the explained variances, and sigma2 contains the\n",
      " |      noise variances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cov : array, shape=(n_features, n_features)\n",
      " |          Estimated covariance of data.\n",
      " |  \n",
      " |  get_precision(self)\n",
      " |      Compute data precision matrix with the generative model.\n",
      " |      \n",
      " |      Equals the inverse of the covariance but computed with\n",
      " |      the matrix inversion lemma for efficiency.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      precision : array, shape=(n_features, n_features)\n",
      " |          Estimated precision of data.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Transform data back to its original space.\n",
      " |      \n",
      " |      In other words, return an input X_original whose transform would be X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_components)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_components is the number of components.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_original array-like, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If whitening is enabled, inverse_transform will compute the\n",
      " |      exact inverse operation, which includes reversing whitening.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Apply dimensionality reduction to X.\n",
      " |      \n",
      " |      X is projected on the first principal components previously extracted\n",
      " |      from a training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array-like, shape (n_samples, n_components)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> from sklearn.decomposition import IncrementalPCA\n",
      " |      >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |      >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
      " |      >>> ipca.fit(X)\n",
      " |      IncrementalPCA(batch_size=3, n_components=2)\n",
      " |      >>> ipca.transform(X) # doctest: +SKIP\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PCA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
